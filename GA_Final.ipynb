{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing LSTM using Genetic Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\deap\\tools\\_hypervolume\\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.\n",
      "  \"module. Expect this to be very slow.\", ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.layers import LSTM, Input, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time,math\n",
    "\n",
    "from deap import base, creator, tools, algorithms\n",
    "from scipy.stats import bernoulli\n",
    "from bitstring import BitArray\n",
    "import operator\n",
    "import random\n",
    "\n",
    "\n",
    "np.random.seed(1120)\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157822, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# France\n",
    "#df = pd.read_excel('LSTM.xlsx', usecols=[1], nrows=50000)\n",
    "df = pd.read_excel('LSTM.xlsx', usecols=[1]) # Full Model\n",
    "#\n",
    "#df=df[0:50000]\n",
    "#df=df[0:2500]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 157822 entries, 0 to 157821\n",
      "Data columns (total 1 columns):\n",
      "Consumption    157822 non-null int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.479645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.479534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.478346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.451795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Consumption\n",
       "0     0.476205\n",
       "1     0.479645\n",
       "2     0.479534\n",
       "3     0.478346\n",
       "4     0.451795"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(df)\n",
    "df=pd.DataFrame(dataset)\n",
    "df.columns=['Consumption']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(ga_individual_solution):  \n",
    "    df1= df.copy(deep=True)\n",
    "    print('Dataset Shape',df1.shape)\n",
    "\n",
    "    # Decode GA solution to integer for window_size and num_units\n",
    "    \n",
    "    # Parameter 1\n",
    "    no_wins = ga_individual_solution[0:2]\n",
    "    no_wins = int(''.join(str(x) for x in no_wins), base=2)\n",
    "    \n",
    "    if no_wins==0:no_wins+=1\n",
    "    elif no_wins==1:no_wins+=1  \n",
    "    elif no_wins==2:no_wins+=1\n",
    "    elif no_wins==3:no_wins+=1 \n",
    "    \n",
    "    print('Number of Windows are:\\n',no_wins)\n",
    "    \n",
    "    # Parameter 2\n",
    "    # Windows starting points\n",
    "    s1=BitArray(ga_individual_solution[2:9]).uint #7 bits\n",
    "    s2=BitArray(ga_individual_solution[9:18]).uint # 9 bits\n",
    "    s3=BitArray(ga_individual_solution[18:28]).uint # 10 bits\n",
    "    s4=BitArray(ga_individual_solution[28:39]).uint # 11 bits\n",
    "    \n",
    "    #  Windows length, max=31\n",
    "    l1= BitArray(ga_individual_solution[39:44]).uint\n",
    "    l2= BitArray(ga_individual_solution[44:49]).uint\n",
    "    l3= BitArray(ga_individual_solution[49:54]).uint\n",
    "    l4= BitArray(ga_individual_solution[54:59]).uint\n",
    "    \n",
    "    # Batch Size\n",
    "    batch_size = BitArray(ga_individual_solution[59:66]).uint\n",
    "    batch_size+=10\n",
    "    print('Batch Size is:',batch_size)\n",
    "    \n",
    "    act = ga_individual_solution[66:68]\n",
    "    act = int(''.join(str(x) for x in act), base=2)\n",
    "    \n",
    "    opt = ga_individual_solution[68:70]\n",
    "    opt = int(''.join(str(x) for x in opt), base=2)\n",
    "    \n",
    "    # Activation Decode\n",
    "    if act==0:act='tanh'\n",
    "    elif act==1:act='relu'\n",
    "    elif act==2:act='selu'\n",
    "    else:act='elu'         \n",
    "        \n",
    "    #  Optimizer Decode\n",
    "    if opt==0:opt='sgd'\n",
    "    elif opt==1:opt='RMSprop'\n",
    "    elif opt==2:opt='Adamax'\n",
    "    else:opt='Adam'     \n",
    "        \n",
    "    length=[l1,l2,l3,l4]\n",
    "    new_list = [n+1 if n==0 else n for n in length]\n",
    "    l1,l2,l3,l4=new_list[0:4]\n",
    "    #window_length=max(new_list)\n",
    "\n",
    "    # Avoid Overlapping 1 & 2\n",
    "    \n",
    "    if s2-(s1+l1) <=-10:\n",
    "        print('Overlap 1 and 2')\n",
    "        return 0.80,\n",
    "\n",
    "        #1 & 3\n",
    "    elif s3-(s1+l1) <=-10:\n",
    "        print('Overlap 1 and 3')\n",
    "        return 0.80,\n",
    "\n",
    "        # 1 & 4\n",
    "    elif s4-(s1+l1)<=-10:\n",
    "        print('Overlap 1 and 4')\n",
    "        return 0.80,\n",
    "\n",
    "    elif s3-(s2+l2) <=-10:\n",
    "        print('Overlap 2 and 3')\n",
    "        return 0.80,\n",
    "              \n",
    "    elif s4-(s2+l2) <=-10:\n",
    "        print('Overlap 2 and 4')\n",
    "        return 0.80,\n",
    "\n",
    "    elif s4-(s3+l3) <=-10:\n",
    "        print('Overlap 3 and 4')\n",
    "        return 0.80,\n",
    "\n",
    "    else:\n",
    "        print('No Overlap')\n",
    "    \n",
    "    print('Windows length are:\\n',l1,l2,l3,l4)\n",
    "        \n",
    "    print('1st Window Start:',s1)\n",
    "    print('2nd Window Start:',s2)\n",
    "    print('3rd Window Start:',s3)\n",
    "    print('4th Window Start:',s4) \n",
    "    \n",
    "    print('Activation Function is:',act)\n",
    "    print('Optimizer is:',opt)\n",
    "     \n",
    "    # Return fitness score of 100 if window_size or num_unit is zero\n",
    "    if batch_size==0:\n",
    "        return 1, \n",
    "    \n",
    "    if no_wins==1:\n",
    "        lag=np.arange(s1,s1+l1,1)\n",
    "        window_length=l1\n",
    "        print('Only Lag is\\n',lag)\n",
    "        \n",
    "    elif no_wins==2:\n",
    "        lag1=np.arange(s1,s1+l1,1).tolist()\n",
    "        lag2=np.arange(s2,s2+l2,1).tolist()\n",
    "        lag=[lag1,lag2]\n",
    "        lag = pad_sequences(lag, padding='post',value=1)\n",
    "        lag = [item for sublist in lag for item in sublist]\n",
    "        window_length=max(l1,l2)\n",
    "        print('Two Lags are\\n',lag)\n",
    "\n",
    "    elif no_wins==3:\n",
    "        lag1=np.arange(s1,s1+l1,1).tolist()\n",
    "        lag2=np.arange(s2,s2+l2,1).tolist()\n",
    "        lag3=np.arange(s3,s3+l3,1).tolist()\n",
    "        lag=[lag1,lag2,lag3]\n",
    "        lag = pad_sequences(lag, padding='post',value=1)\n",
    "        lag = [item for sublist in lag for item in sublist]\n",
    "        print('Three Lags are\\n',lag)\n",
    "        window_length=max(l1,l2,l3)\n",
    "\n",
    "    elif no_wins==4:\n",
    "        lag1=np.arange(s1,s1+l1,1).tolist()\n",
    "        lag2=np.arange(s2,s2+l2,1).tolist()\n",
    "        lag3=np.arange(s3,s3+l3,1).tolist()\n",
    "        lag4=np.arange(s4,s4+l4,1).tolist()\n",
    "        lag=[lag1,lag2,lag3,lag4]\n",
    "        lag = pad_sequences(lag, padding='post',value=1)\n",
    "        lag = [item for sublist in lag for item in sublist]\n",
    "        window_length=max(l1,l2,l3,l4)\n",
    "        print('Four Lags are\\n',lag)\n",
    "      \n",
    "    print('Lags length:\\n', len(lag))\n",
    "    print('Length Used:\\n', window_length)\n",
    "    for col in df1.columns:\n",
    "        for idx, l in enumerate(lag):\n",
    "            df1.loc[:,col+\"_\"+str(idx)] = df1[col].shift(l)\n",
    "           \n",
    "    df1.dropna(how='any',inplace=True)\n",
    "    dataset = df1.values # Converted dataframe to numpy ndarray\n",
    "    dataset = dataset.astype('float32')\n",
    "    print('New Dataset shape after Lags:',dataset.shape)\n",
    "    \n",
    "    # split into train and test sets\n",
    "    train_size = int(len(dataset) * 0.80)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    print('Training and test data length',len(train), len(test))\n",
    "    \n",
    "    trainX=train[:,1:]\n",
    "    trainY=train[:,0]\n",
    "    testX=test[:,1:]\n",
    "    testY=test[:,0]\n",
    "    \n",
    "    print('trainX.shape[0]: ',trainX.shape[0])\n",
    "    print('window_length: ',window_length) \n",
    "    print('no_wins: ',no_wins)\n",
    "    \n",
    "    print('trainX shape:\\n',trainX.shape)\n",
    "    \n",
    "    #LSTM\n",
    "    # reshape input to be [samples, time steps, features]\n",
    "    trainX = np.reshape(trainX, (trainX.shape[0],window_length,(no_wins)))\n",
    "    testX = np.reshape(testX, (testX.shape[0],window_length,(no_wins)))\n",
    "    print('Train X shape after np.reshape',trainX.shape)\n",
    "    print('Test X shape after np.reshape',testX.shape)\n",
    "    print('Train Y Shape',trainY.shape)\n",
    "    print('Test Y Shape',testY.shape)\n",
    "      \n",
    "    # Train LSTM model and predict on validation set\n",
    "    #inputs = Input(shape=(window_size,1))\n",
    "    model = Sequential() # New Instance of Model Object\n",
    "    model.add(LSTM(50, input_shape=(window_length,(no_wins)))) \n",
    "    #model.add(Dense(10, activation=act))\n",
    "    model.add(Dense(50, activation=act))\n",
    "    model.add(Dense(40, activation=act))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=opt,loss='mean_squared_error')\n",
    "    #model.fit(trainX, trainY, epochs=1, batch_size=10,shuffle=True)\n",
    "    \n",
    "    start = time.time()\n",
    "    model.fit(trainX, trainY, epochs=70, shuffle=False,batch_size=batch_size,\n",
    "              validation_data=(testX, testY), \n",
    "              callbacks=[EarlyStopping(monitor='val_loss', patience=20)], verbose=1)\n",
    "    \n",
    "    y_pred = model.predict(testX)\n",
    "    end = time.time()\n",
    "    \n",
    "    # Calculate the RMSE score as fitness score for GA\n",
    "    rmse = np.sqrt(mean_squared_error(testY, y_pred))\n",
    "    print('Validation RMSE: ', rmse,'\\n')\n",
    "    print (\"Model took %0.2f seconds to train\"%(end - start))\n",
    "    return rmse,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are trying to minimize the RMSE score, that's why using -1.0. \n",
    "# In case, when you want to maximize accuracy for instance, use 1.0\n",
    "\n",
    "creator.create('FitnessMin', base.Fitness, weights = (-1.0,))\n",
    "creator.create('Individual', list , fitness = creator.FitnessMin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self\n",
    "#population_size = 20\n",
    "#num_generations = 30\n",
    "\n",
    "population_size = 10\n",
    "num_generations = 30\n",
    "\n",
    "#gene_length = 69\n",
    "gene_length = 70\n",
    "\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1) #Generates single RAND\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual,\n",
    "                 toolbox.attr_bool, n=gene_length)\n",
    "toolbox.register('population', tools.initRepeat, list , toolbox.individual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(toolbox.individual()))\n",
    "print(len(toolbox.population(population_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"population_size = 2\\nnum_generations = 3\\ngene_length = 24\\n\\n\\ntoolbox = base.Toolbox()\\ntoolbox.register('binary', bernoulli.rvs, 0.5) #Generates single bernoulli\\ntoolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n=gene_length)\\ntoolbox.register('population', tools.initRepeat, list , toolbox.individual)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''population_size = 2\n",
    "num_generations = 3\n",
    "gene_length = 24\n",
    "\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register('binary', bernoulli.rvs, 0.5) #Generates single bernoulli\n",
    "toolbox.register('individual', tools.initRepeat, creator.Individual, toolbox.binary, n=gene_length)\n",
    "toolbox.register('population', tools.initRepeat, list , toolbox.individual)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "*deap.algorithms.eaSimple(population, toolbox, cxpb, mutpb, ngen[, stats, halloffame, verbose])*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the crossover operator\n",
    "toolbox.register('mate', tools.cxUniform,indpb= 0.6)\n",
    "\n",
    "# Register a mutation operator\n",
    "toolbox.register('mutate', tools.mutShuffleIndexes, indpb = 0.1)\n",
    "\n",
    "# Operator for selecting individuals for breeding\n",
    "#toolbox.register('select', tools.selRoulette)\n",
    "toolbox.register('select',tools.selTournament,tournsize=4)\n",
    "\n",
    "# Register the evaluation function\n",
    "toolbox.register('evaluate', train_evaluate)\n",
    "\n",
    "population = toolbox.population(n = population_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 59\n",
      "Overlap 2 and 3\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 4\n",
      "Batch Size is: 112\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 17 30 20 1\n",
      "1st Window Start: 43\n",
      "2nd Window Start: 140\n",
      "3rd Window Start: 999\n",
      "4th Window Start: 1351\n",
      "Activation Function is: elu\n",
      "Optimizer is: Adamax\n",
      "Four Lags are\n",
      " [43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1351, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Lags length:\n",
      " 120\n",
      "Length Used:\n",
      " 30\n",
      "New Dataset shape after Lags: (48649, 121)\n",
      "Training and test data length 38919 9730\n",
      "trainX.shape[0]:  38919\n",
      "window_length:  30\n",
      "no_wins:  4\n",
      "trainX shape:\n",
      " (38919, 120)\n",
      "Train X shape after np.reshape (38919, 30, 4)\n",
      "Test X shape after np.reshape (9730, 30, 4)\n",
      "Train Y Shape (38919,)\n",
      "Test Y Shape (9730,)\n",
      "WARNING:tensorflow:From C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 38919 samples, validate on 9730 samples\n",
      "Epoch 1/70\n",
      "38919/38919 [==============================] - 17s 432us/step - loss: 0.0029 - val_loss: 5.5080e-04\n",
      "Epoch 2/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 5.7182e-04 - val_loss: 5.0458e-04\n",
      "Epoch 3/70\n",
      "38919/38919 [==============================] - 15s 397us/step - loss: 5.4479e-04 - val_loss: 5.4505e-04\n",
      "Epoch 4/70\n",
      "38919/38919 [==============================] - 15s 394us/step - loss: 4.9341e-04 - val_loss: 4.2511e-04\n",
      "Epoch 5/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 4.5474e-04 - val_loss: 4.5255e-04\n",
      "Epoch 6/70\n",
      "38919/38919 [==============================] - 15s 392us/step - loss: 4.3217e-04 - val_loss: 3.5683e-04\n",
      "Epoch 7/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 4.3315e-04 - val_loss: 3.8421e-04\n",
      "Epoch 8/70\n",
      "38919/38919 [==============================] - 16s 399us/step - loss: 4.3521e-04 - val_loss: 3.7171e-04\n",
      "Epoch 9/70\n",
      "38919/38919 [==============================] - 15s 397us/step - loss: 4.3419e-04 - val_loss: 4.7074e-04\n",
      "Epoch 10/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 4.6690e-04 - val_loss: 4.7242e-04\n",
      "Epoch 11/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 5.1463e-04 - val_loss: 0.0015\n",
      "Epoch 12/70\n",
      "38919/38919 [==============================] - 16s 401us/step - loss: 6.0567e-04 - val_loss: 0.0023\n",
      "Epoch 13/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 8.2837e-04 - val_loss: 7.3607e-04\n",
      "Epoch 14/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 6.3138e-04 - val_loss: 3.4359e-04\n",
      "Epoch 15/70\n",
      "38919/38919 [==============================] - 15s 397us/step - loss: 6.5520e-04 - val_loss: 4.5594e-04\n",
      "Epoch 16/70\n",
      "38919/38919 [==============================] - 15s 392us/step - loss: 6.2037e-04 - val_loss: 3.6525e-04\n",
      "Epoch 17/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 6.0812e-04 - val_loss: 4.0632e-04\n",
      "Epoch 18/70\n",
      "38919/38919 [==============================] - 15s 392us/step - loss: 5.9401e-04 - val_loss: 4.0685e-04\n",
      "Epoch 19/70\n",
      "38919/38919 [==============================] - 15s 392us/step - loss: 5.7857e-04 - val_loss: 4.0696e-04\n",
      "Epoch 20/70\n",
      "38919/38919 [==============================] - 15s 393us/step - loss: 5.6428e-04 - val_loss: 4.0848e-04\n",
      "Epoch 21/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 5.4900e-04 - val_loss: 4.0242e-04\n",
      "Epoch 22/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 5.3368e-04 - val_loss: 3.9288e-04\n",
      "Epoch 23/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 5.2008e-04 - val_loss: 3.5853e-04\n",
      "Epoch 24/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 5.0735e-04 - val_loss: 3.6788e-04\n",
      "Epoch 25/70\n",
      "38919/38919 [==============================] - 15s 393us/step - loss: 4.9781e-04 - val_loss: 3.5911e-04\n",
      "Epoch 26/70\n",
      "38919/38919 [==============================] - 15s 394us/step - loss: 4.8793e-04 - val_loss: 3.6985e-04\n",
      "Epoch 27/70\n",
      "38919/38919 [==============================] - 15s 397us/step - loss: 4.7950e-04 - val_loss: 3.4373e-04\n",
      "Epoch 28/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 4.7148e-04 - val_loss: 3.6965e-04\n",
      "Epoch 29/70\n",
      "38919/38919 [==============================] - 15s 397us/step - loss: 4.6390e-04 - val_loss: 3.4791e-04\n",
      "Epoch 30/70\n",
      "38919/38919 [==============================] - 15s 393us/step - loss: 4.5686e-04 - val_loss: 3.7254e-04\n",
      "Epoch 31/70\n",
      "38919/38919 [==============================] - 15s 393us/step - loss: 4.4994e-04 - val_loss: 3.5887e-04\n",
      "Epoch 32/70\n",
      "38919/38919 [==============================] - 15s 397us/step - loss: 4.4675e-04 - val_loss: 3.9017e-04\n",
      "Epoch 33/70\n",
      "38919/38919 [==============================] - 15s 394us/step - loss: 4.3647e-04 - val_loss: 3.3614e-04\n",
      "Epoch 34/70\n",
      "38919/38919 [==============================] - 15s 394us/step - loss: 4.3929e-04 - val_loss: 4.1482e-04\n",
      "Epoch 35/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 4.1602e-04 - val_loss: 3.2957e-04\n",
      "Epoch 36/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 4.3205e-04 - val_loss: 4.4854e-04\n",
      "Epoch 37/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 4.0402e-04 - val_loss: 3.6269e-04\n",
      "Epoch 38/70\n",
      "38919/38919 [==============================] - 15s 397us/step - loss: 4.1527e-04 - val_loss: 4.3390e-04\n",
      "Epoch 39/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.9954e-04 - val_loss: 3.7343e-04\n",
      "Epoch 40/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 4.0919e-04 - val_loss: 4.4768e-04\n",
      "Epoch 41/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 3.8610e-04 - val_loss: 3.4176e-04\n",
      "Epoch 42/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 4.0421e-04 - val_loss: 4.7320e-04\n",
      "Epoch 43/70\n",
      "38919/38919 [==============================] - 15s 392us/step - loss: 3.7679e-04 - val_loss: 3.6550e-04\n",
      "Epoch 44/70\n",
      "38919/38919 [==============================] - 15s 394us/step - loss: 3.9575e-04 - val_loss: 4.8271e-04\n",
      "Epoch 45/70\n",
      "38919/38919 [==============================] - 15s 392us/step - loss: 3.7054e-04 - val_loss: 3.4805e-04\n",
      "Epoch 46/70\n",
      "38919/38919 [==============================] - 16s 400us/step - loss: 3.9339e-04 - val_loss: 5.3082e-04\n",
      "Epoch 47/70\n",
      "38919/38919 [==============================] - 16s 399us/step - loss: 3.5943e-04 - val_loss: 3.3817e-04\n",
      "Epoch 48/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 3.8799e-04 - val_loss: 4.9360e-04\n",
      "Epoch 49/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.5755e-04 - val_loss: 3.2360e-04\n",
      "Epoch 50/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.8469e-04 - val_loss: 5.1561e-04\n",
      "Epoch 51/70\n",
      "38919/38919 [==============================] - 15s 392us/step - loss: 3.5552e-04 - val_loss: 3.1255e-04\n",
      "Epoch 52/70\n",
      "38919/38919 [==============================] - 15s 394us/step - loss: 3.8284e-04 - val_loss: 5.1280e-04\n",
      "Epoch 53/70\n",
      "38919/38919 [==============================] - 15s 393us/step - loss: 3.5456e-04 - val_loss: 3.0908e-04\n",
      "Epoch 54/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 3.8116e-04 - val_loss: 5.0792e-04\n",
      "Epoch 55/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.5773e-04 - val_loss: 3.3522e-04\n",
      "Epoch 56/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.7941e-04 - val_loss: 4.8154e-04\n",
      "Epoch 57/70\n",
      "38919/38919 [==============================] - 15s 397us/step - loss: 3.6387e-04 - val_loss: 3.9370e-04\n",
      "Epoch 58/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.6842e-04 - val_loss: 4.3095e-04\n",
      "Epoch 59/70\n",
      "38919/38919 [==============================] - 15s 394us/step - loss: 3.6938e-04 - val_loss: 4.7619e-04\n",
      "Epoch 60/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.5940e-04 - val_loss: 3.8445e-04\n",
      "Epoch 61/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 3.6576e-04 - val_loss: 4.6057e-04\n",
      "Epoch 62/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.6048e-04 - val_loss: 4.2625e-04\n",
      "Epoch 63/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 3.5635e-04 - val_loss: 3.7661e-04\n",
      "Epoch 64/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.6081e-04 - val_loss: 4.5287e-04\n",
      "Epoch 65/70\n",
      "38919/38919 [==============================] - 15s 395us/step - loss: 3.5206e-04 - val_loss: 3.7430e-04\n",
      "Epoch 66/70\n",
      "38919/38919 [==============================] - 16s 399us/step - loss: 3.5508e-04 - val_loss: 4.3112e-04\n",
      "Epoch 67/70\n",
      "38919/38919 [==============================] - 15s 396us/step - loss: 3.4796e-04 - val_loss: 3.5492e-04\n",
      "Epoch 68/70\n",
      "38919/38919 [==============================] - 15s 393us/step - loss: 3.5142e-04 - val_loss: 4.4515e-04\n",
      "Epoch 69/70\n",
      "38919/38919 [==============================] - 15s 398us/step - loss: 3.4380e-04 - val_loss: 3.4800e-04\n",
      "Epoch 70/70\n",
      "38919/38919 [==============================] - 15s 392us/step - loss: 3.4817e-04 - val_loss: 4.4554e-04\n",
      "Validation RMSE:  0.021107836 \n",
      "\n",
      "Model took 1085.41 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 129\n",
      "Overlap 3 and 4\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 2\n",
      "Batch Size is: 36\n",
      "Overlap 1 and 2\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 132\n",
      "Overlap 3 and 4\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 83\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 4 6 30 25\n",
      "1st Window Start: 99\n",
      "2nd Window Start: 157\n",
      "3rd Window Start: 899\n",
      "4th Window Start: 1763\n",
      "Activation Function is: relu\n",
      "Optimizer is: Adam\n",
      "Only Lag is\n",
      " [ 99 100 101 102]\n",
      "Lags length:\n",
      " 4\n",
      "Length Used:\n",
      " 4\n",
      "New Dataset shape after Lags: (49898, 5)\n",
      "Training and test data length 39918 9980\n",
      "trainX.shape[0]:  39918\n",
      "window_length:  4\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39918, 4)\n",
      "Train X shape after np.reshape (39918, 4, 1)\n",
      "Test X shape after np.reshape (9980, 4, 1)\n",
      "Train Y Shape (39918,)\n",
      "Test Y Shape (9980,)\n",
      "Train on 39918 samples, validate on 9980 samples\n",
      "Epoch 1/70\n",
      "39918/39918 [==============================] - 5s 124us/step - loss: 0.0200 - val_loss: 0.0153\n",
      "Epoch 2/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0139 - val_loss: 0.0174\n",
      "Epoch 3/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0141 - val_loss: 0.0189\n",
      "Epoch 4/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0142 - val_loss: 0.0204\n",
      "Epoch 5/70\n",
      "39918/39918 [==============================] - 4s 110us/step - loss: 0.0144 - val_loss: 0.0225\n",
      "Epoch 6/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0146 - val_loss: 0.0227\n",
      "Epoch 7/70\n",
      "39918/39918 [==============================] - 4s 110us/step - loss: 0.0145 - val_loss: 0.0237\n",
      "Epoch 8/70\n",
      "39918/39918 [==============================] - 4s 110us/step - loss: 0.0147 - val_loss: 0.0248\n",
      "Epoch 9/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0148 - val_loss: 0.0270\n",
      "Epoch 10/70\n",
      "39918/39918 [==============================] - 4s 110us/step - loss: 0.0153 - val_loss: 0.0201\n",
      "Epoch 11/70\n",
      "39918/39918 [==============================] - 4s 110us/step - loss: 0.0144 - val_loss: 0.0241\n",
      "Epoch 12/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0143 - val_loss: 0.0254\n",
      "Epoch 13/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0143 - val_loss: 0.0251\n",
      "Epoch 14/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0143 - val_loss: 0.0255\n",
      "Epoch 15/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0143 - val_loss: 0.0208\n",
      "Epoch 16/70\n",
      "39918/39918 [==============================] - 4s 110us/step - loss: 0.0137 - val_loss: 0.0230\n",
      "Epoch 17/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0136 - val_loss: 0.0236\n",
      "Epoch 18/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0136 - val_loss: 0.0238\n",
      "Epoch 19/70\n",
      "39918/39918 [==============================] - 4s 109us/step - loss: 0.0135 - val_loss: 0.0237\n",
      "Epoch 20/70\n",
      "39918/39918 [==============================] - 4s 110us/step - loss: 0.0133 - val_loss: 0.0229\n",
      "Epoch 21/70\n",
      "39918/39918 [==============================] - 4s 110us/step - loss: 0.0131 - val_loss: 0.0226\n",
      "Validation RMSE:  0.15040128 \n",
      "\n",
      "Model took 94.11 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 2\n",
      "Batch Size is: 76\n",
      "Overlap 1 and 2\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 84\n",
      "Overlap 1 and 2\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 2\n",
      "Batch Size is: 106\n",
      "Overlap 3 and 4\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 125\n",
      "Overlap 1 and 3\n",
      "gen\tnevals\tavg     \tstd     \tmin      \tmax\n",
      "0  \t10    \t0.657151\t0.287157\t0.0211078\t0.8\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 36\n",
      "Overlap 1 and 4\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 2\n",
      "Batch Size is: 59\n",
      "Overlap 1 and 2\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 124\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 29 22 23 9\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 44\n",
      "3rd Window Start: 844\n",
      "4th Window Start: 1093\n",
      "Activation Function is: elu\n",
      "Optimizer is: sgd\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36 37]\n",
      "Lags length:\n",
      " 29\n",
      "Length Used:\n",
      " 29\n",
      "New Dataset shape after Lags: (49963, 30)\n",
      "Training and test data length 39970 9993\n",
      "trainX.shape[0]:  39970\n",
      "window_length:  29\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39970, 29)\n",
      "Train X shape after np.reshape (39970, 29, 1)\n",
      "Test X shape after np.reshape (9993, 29, 1)\n",
      "Train Y Shape (39970,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39970 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39970/39970 [==============================] - 14s 363us/step - loss: 0.0161 - val_loss: 0.0250\n",
      "Epoch 2/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0133 - val_loss: 0.0239\n",
      "Epoch 3/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0133 - val_loss: 0.0232\n",
      "Epoch 4/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0132 - val_loss: 0.0227\n",
      "Epoch 5/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0132 - val_loss: 0.0223\n",
      "Epoch 6/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0131 - val_loss: 0.0220\n",
      "Epoch 7/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0131 - val_loss: 0.0218\n",
      "Epoch 8/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0131 - val_loss: 0.0217\n",
      "Epoch 9/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0131 - val_loss: 0.0216\n",
      "Epoch 10/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0131 - val_loss: 0.0215\n",
      "Epoch 11/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0131 - val_loss: 0.0214\n",
      "Epoch 12/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0130 - val_loss: 0.0214\n",
      "Epoch 13/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0130 - val_loss: 0.0213\n",
      "Epoch 14/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0130 - val_loss: 0.0213\n",
      "Epoch 15/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0130 - val_loss: 0.0212\n",
      "Epoch 16/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0130 - val_loss: 0.0212\n",
      "Epoch 17/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0130 - val_loss: 0.0211\n",
      "Epoch 18/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0130 - val_loss: 0.0211\n",
      "Epoch 19/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0130 - val_loss: 0.0210\n",
      "Epoch 20/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0130 - val_loss: 0.0210\n",
      "Epoch 21/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0129 - val_loss: 0.0209\n",
      "Epoch 22/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0129 - val_loss: 0.0209\n",
      "Epoch 23/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0129 - val_loss: 0.0208\n",
      "Epoch 24/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0129 - val_loss: 0.0208\n",
      "Epoch 25/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0129 - val_loss: 0.0207\n",
      "Epoch 26/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0129 - val_loss: 0.0207\n",
      "Epoch 27/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0129 - val_loss: 0.0206\n",
      "Epoch 28/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0128 - val_loss: 0.0206\n",
      "Epoch 29/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0128 - val_loss: 0.0205\n",
      "Epoch 30/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0128 - val_loss: 0.0205\n",
      "Epoch 31/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0128 - val_loss: 0.0204\n",
      "Epoch 32/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0128 - val_loss: 0.0204\n",
      "Epoch 33/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0128 - val_loss: 0.0203\n",
      "Epoch 34/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0128 - val_loss: 0.0203\n",
      "Epoch 35/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0127 - val_loss: 0.0202\n",
      "Epoch 36/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0127 - val_loss: 0.0202\n",
      "Epoch 37/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0127 - val_loss: 0.0201\n",
      "Epoch 38/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0127 - val_loss: 0.0200\n",
      "Epoch 39/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0127 - val_loss: 0.0200\n",
      "Epoch 40/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0126 - val_loss: 0.0199\n",
      "Epoch 41/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0126 - val_loss: 0.0199\n",
      "Epoch 42/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0126 - val_loss: 0.0198\n",
      "Epoch 43/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0126 - val_loss: 0.0197\n",
      "Epoch 44/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0126 - val_loss: 0.0197\n",
      "Epoch 45/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0125 - val_loss: 0.0196\n",
      "Epoch 46/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0125 - val_loss: 0.0195\n",
      "Epoch 47/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0125 - val_loss: 0.0195\n",
      "Epoch 48/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0125 - val_loss: 0.0194\n",
      "Epoch 49/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0125 - val_loss: 0.0193\n",
      "Epoch 50/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0124 - val_loss: 0.0193\n",
      "Epoch 51/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0124 - val_loss: 0.0192\n",
      "Epoch 52/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0124 - val_loss: 0.0191\n",
      "Epoch 53/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0124 - val_loss: 0.0191\n",
      "Epoch 54/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0124 - val_loss: 0.0190\n",
      "Epoch 55/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0123 - val_loss: 0.0189\n",
      "Epoch 56/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0123 - val_loss: 0.0189\n",
      "Epoch 57/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0123 - val_loss: 0.0188\n",
      "Epoch 58/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0123 - val_loss: 0.0187\n",
      "Epoch 59/70\n",
      "39970/39970 [==============================] - 14s 344us/step - loss: 0.0122 - val_loss: 0.0187\n",
      "Epoch 60/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0122 - val_loss: 0.0186\n",
      "Epoch 61/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0122 - val_loss: 0.0185\n",
      "Epoch 62/70\n",
      "39970/39970 [==============================] - 14s 344us/step - loss: 0.0122 - val_loss: 0.0185\n",
      "Epoch 63/70\n",
      "39970/39970 [==============================] - 14s 343us/step - loss: 0.0121 - val_loss: 0.0184\n",
      "Epoch 64/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0121 - val_loss: 0.0183\n",
      "Epoch 65/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0121 - val_loss: 0.0183\n",
      "Epoch 66/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0121 - val_loss: 0.0182\n",
      "Epoch 67/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0120 - val_loss: 0.0181\n",
      "Epoch 68/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0120 - val_loss: 0.0181\n",
      "Epoch 69/70\n",
      "39970/39970 [==============================] - 14s 346us/step - loss: 0.0120 - val_loss: 0.0180\n",
      "Epoch 70/70\n",
      "39970/39970 [==============================] - 14s 345us/step - loss: 0.0120 - val_loss: 0.0180\n",
      "Validation RMSE:  0.1340174 \n",
      "\n",
      "Model took 977.06 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 4\n",
      "Batch Size is: 120\n",
      "Overlap 3 and 4\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 125\n",
      "Overlap 2 and 3\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 59\n",
      "Overlap 1 and 2\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 76\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 1 20 17 3\n",
      "1st Window Start: 59\n",
      "2nd Window Start: 340\n",
      "3rd Window Start: 656\n",
      "4th Window Start: 1364\n",
      "Activation Function is: elu\n",
      "Optimizer is: sgd\n",
      "Only Lag is\n",
      " [59]\n",
      "Lags length:\n",
      " 1\n",
      "Length Used:\n",
      " 1\n",
      "New Dataset shape after Lags: (49941, 2)\n",
      "Training and test data length 39952 9989\n",
      "trainX.shape[0]:  39952\n",
      "window_length:  1\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39952, 1)\n",
      "Train X shape after np.reshape (39952, 1, 1)\n",
      "Test X shape after np.reshape (9989, 1, 1)\n",
      "Train Y Shape (39952,)\n",
      "Test Y Shape (9989,)\n",
      "Train on 39952 samples, validate on 9989 samples\n",
      "Epoch 1/70\n",
      "39952/39952 [==============================] - 3s 73us/step - loss: 0.0176 - val_loss: 0.0346\n",
      "Epoch 2/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0147 - val_loss: 0.0343\n",
      "Epoch 3/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0146 - val_loss: 0.0340\n",
      "Epoch 4/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0146 - val_loss: 0.0337\n",
      "Epoch 5/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0145 - val_loss: 0.0334\n",
      "Epoch 6/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0145 - val_loss: 0.0331\n",
      "Epoch 7/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0145 - val_loss: 0.0328\n",
      "Epoch 8/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0144 - val_loss: 0.0325\n",
      "Epoch 9/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0144 - val_loss: 0.0322\n",
      "Epoch 10/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0144 - val_loss: 0.0320\n",
      "Epoch 11/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0143 - val_loss: 0.0317\n",
      "Epoch 12/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0143 - val_loss: 0.0315\n",
      "Epoch 13/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0143 - val_loss: 0.0312\n",
      "Epoch 14/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0142 - val_loss: 0.0310\n",
      "Epoch 15/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0142 - val_loss: 0.0308\n",
      "Epoch 16/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0142 - val_loss: 0.0306\n",
      "Epoch 17/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0142 - val_loss: 0.0304\n",
      "Epoch 18/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0141 - val_loss: 0.0302\n",
      "Epoch 19/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0141 - val_loss: 0.0300\n",
      "Epoch 20/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0141 - val_loss: 0.0299\n",
      "Epoch 21/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0141 - val_loss: 0.0297\n",
      "Epoch 22/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0141 - val_loss: 0.0296\n",
      "Epoch 23/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0141 - val_loss: 0.0294\n",
      "Epoch 24/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0141 - val_loss: 0.0293\n",
      "Epoch 25/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0292\n",
      "Epoch 26/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0291\n",
      "Epoch 27/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0289\n",
      "Epoch 28/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0288\n",
      "Epoch 29/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0288\n",
      "Epoch 30/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0287\n",
      "Epoch 31/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0286\n",
      "Epoch 32/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0285\n",
      "Epoch 33/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0284\n",
      "Epoch 34/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0284\n",
      "Epoch 35/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0283\n",
      "Epoch 36/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0282\n",
      "Epoch 37/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0282\n",
      "Epoch 38/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0281\n",
      "Epoch 39/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0281\n",
      "Epoch 40/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0280\n",
      "Epoch 41/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0280\n",
      "Epoch 42/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0280\n",
      "Epoch 43/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0279\n",
      "Epoch 44/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0279\n",
      "Epoch 45/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0279\n",
      "Epoch 46/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0279\n",
      "Epoch 47/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0278\n",
      "Epoch 48/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0278\n",
      "Epoch 49/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0278\n",
      "Epoch 50/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0278\n",
      "Epoch 51/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0278\n",
      "Epoch 52/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0278\n",
      "Epoch 53/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 54/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 55/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 56/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 57/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 58/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 59/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 60/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 61/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 62/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 63/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 64/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 65/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 66/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 67/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 68/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 69/70\n",
      "39952/39952 [==============================] - 2s 55us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Epoch 70/70\n",
      "39952/39952 [==============================] - 2s 56us/step - loss: 0.0140 - val_loss: 0.0277\n",
      "Validation RMSE:  0.16647877 \n",
      "\n",
      "Model took 157.37 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 67\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 8 20 3 5\n",
      "1st Window Start: 125\n",
      "2nd Window Start: 188\n",
      "3rd Window Start: 309\n",
      "4th Window Start: 521\n",
      "Activation Function is: elu\n",
      "Optimizer is: sgd\n",
      "Only Lag is\n",
      " [125 126 127 128 129 130 131 132]\n",
      "Lags length:\n",
      " 8\n",
      "Length Used:\n",
      " 8\n",
      "New Dataset shape after Lags: (49868, 9)\n",
      "Training and test data length 39894 9974\n",
      "trainX.shape[0]:  39894\n",
      "window_length:  8\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39894, 8)\n",
      "Train X shape after np.reshape (39894, 8, 1)\n",
      "Test X shape after np.reshape (9974, 8, 1)\n",
      "Train Y Shape (39894,)\n",
      "Test Y Shape (9974,)\n",
      "Train on 39894 samples, validate on 9974 samples\n",
      "Epoch 1/70\n",
      "39894/39894 [==============================] - 9s 218us/step - loss: 0.0165 - val_loss: 0.0311\n",
      "Epoch 2/70\n",
      "39894/39894 [==============================] - 8s 200us/step - loss: 0.0143 - val_loss: 0.0334\n",
      "Epoch 3/70\n",
      "39894/39894 [==============================] - 8s 200us/step - loss: 0.0142 - val_loss: 0.0351\n",
      "Epoch 4/70\n",
      "39894/39894 [==============================] - 8s 198us/step - loss: 0.0142 - val_loss: 0.0364\n",
      "Epoch 5/70\n",
      "39894/39894 [==============================] - 8s 200us/step - loss: 0.0142 - val_loss: 0.0374\n",
      "Epoch 6/70\n",
      "39894/39894 [==============================] - 8s 198us/step - loss: 0.0143 - val_loss: 0.0381\n",
      "Epoch 7/70\n",
      "39894/39894 [==============================] - 8s 199us/step - loss: 0.0143 - val_loss: 0.0387\n",
      "Epoch 8/70\n",
      "39894/39894 [==============================] - 8s 200us/step - loss: 0.0143 - val_loss: 0.0391\n",
      "Epoch 9/70\n",
      "39894/39894 [==============================] - 8s 199us/step - loss: 0.0144 - val_loss: 0.0395\n",
      "Epoch 10/70\n",
      "39894/39894 [==============================] - 8s 199us/step - loss: 0.0144 - val_loss: 0.0397\n",
      "Epoch 11/70\n",
      "39894/39894 [==============================] - 8s 200us/step - loss: 0.0144 - val_loss: 0.0399\n",
      "Epoch 12/70\n",
      "39894/39894 [==============================] - 8s 199us/step - loss: 0.0145 - val_loss: 0.0401\n",
      "Epoch 13/70\n",
      "39894/39894 [==============================] - 8s 200us/step - loss: 0.0145 - val_loss: 0.0402\n",
      "Epoch 14/70\n",
      "39894/39894 [==============================] - 8s 198us/step - loss: 0.0145 - val_loss: 0.0404\n",
      "Epoch 15/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39894/39894 [==============================] - 8s 200us/step - loss: 0.0146 - val_loss: 0.0405\n",
      "Epoch 16/70\n",
      "39894/39894 [==============================] - 8s 199us/step - loss: 0.0146 - val_loss: 0.0406\n",
      "Epoch 17/70\n",
      "39894/39894 [==============================] - 8s 199us/step - loss: 0.0146 - val_loss: 0.0407\n",
      "Epoch 18/70\n",
      "39894/39894 [==============================] - 8s 199us/step - loss: 0.0147 - val_loss: 0.0408\n",
      "Epoch 19/70\n",
      "39894/39894 [==============================] - 8s 200us/step - loss: 0.0147 - val_loss: 0.0409\n",
      "Epoch 20/70\n",
      "39894/39894 [==============================] - 8s 200us/step - loss: 0.0147 - val_loss: 0.0409\n",
      "Epoch 21/70\n",
      "39894/39894 [==============================] - 8s 199us/step - loss: 0.0148 - val_loss: 0.0410\n",
      "Validation RMSE:  0.20251024 \n",
      "\n",
      "Model took 170.12 seconds to train\n",
      "1  \t8     \t0.610301\t0.290175\t0.134017 \t0.8\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 92\n",
      "Overlap 1 and 2\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 23 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 444\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 25s 616us/step - loss: 0.0139 - val_loss: 0.0180\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 24s 590us/step - loss: 0.0106 - val_loss: 0.0172\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0094 - val_loss: 0.0166\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0078 - val_loss: 0.0193\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0063 - val_loss: 0.0182\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0056 - val_loss: 0.0177\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0051 - val_loss: 0.0148\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0047 - val_loss: 0.0138\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0045 - val_loss: 0.0141\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0042 - val_loss: 0.0136\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0039 - val_loss: 0.0133\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0037 - val_loss: 0.0135\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0036 - val_loss: 0.0127\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0035 - val_loss: 0.0126\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0034 - val_loss: 0.0120\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0032 - val_loss: 0.0112\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0031 - val_loss: 0.0112\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0030 - val_loss: 0.0117\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0030 - val_loss: 0.0117\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0029 - val_loss: 0.0124\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0029 - val_loss: 0.0128\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0029 - val_loss: 0.0123\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0028 - val_loss: 0.0119\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0028 - val_loss: 0.0126\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0027 - val_loss: 0.0122\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0027 - val_loss: 0.0126\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0026 - val_loss: 0.0131\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0026 - val_loss: 0.0128\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0026 - val_loss: 0.0116\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0025 - val_loss: 0.0117\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0025 - val_loss: 0.0119\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0024 - val_loss: 0.0122\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0024 - val_loss: 0.0117\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0024 - val_loss: 0.0108\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0024 - val_loss: 0.0099\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0023 - val_loss: 0.0107\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0023 - val_loss: 0.0112\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0022 - val_loss: 0.0112\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0022 - val_loss: 0.0119\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0022 - val_loss: 0.0112\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0021 - val_loss: 0.0105\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0021 - val_loss: 0.0101\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0021 - val_loss: 0.0117\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0021 - val_loss: 0.0115\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0020 - val_loss: 0.0120\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0020 - val_loss: 0.0113\n",
      "Epoch 47/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0020 - val_loss: 0.0114\n",
      "Epoch 48/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0020 - val_loss: 0.0113\n",
      "Epoch 49/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0020 - val_loss: 0.0111\n",
      "Epoch 50/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0019 - val_loss: 0.0109\n",
      "Epoch 51/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0019 - val_loss: 0.0107\n",
      "Epoch 52/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0019 - val_loss: 0.0104\n",
      "Epoch 53/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0019 - val_loss: 0.0101\n",
      "Epoch 54/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0019 - val_loss: 0.0096\n",
      "Epoch 55/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0019 - val_loss: 0.0095\n",
      "Epoch 56/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0019 - val_loss: 0.0092\n",
      "Epoch 57/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0019 - val_loss: 0.0092\n",
      "Epoch 58/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0018 - val_loss: 0.0084\n",
      "Epoch 59/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0019 - val_loss: 0.0084\n",
      "Epoch 60/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0018 - val_loss: 0.0090\n",
      "Epoch 61/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0018 - val_loss: 0.0085\n",
      "Epoch 62/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0018 - val_loss: 0.0080\n",
      "Epoch 63/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0018 - val_loss: 0.0076\n",
      "Epoch 64/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0018 - val_loss: 0.0082\n",
      "Epoch 65/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0018 - val_loss: 0.0082\n",
      "Epoch 66/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0018 - val_loss: 0.0081\n",
      "Epoch 67/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0017 - val_loss: 0.0081\n",
      "Epoch 68/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0017 - val_loss: 0.0084\n",
      "Epoch 69/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0017 - val_loss: 0.0083\n",
      "Epoch 70/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0017 - val_loss: 0.0080\n",
      "Validation RMSE:  0.0891916 \n",
      "\n",
      "Model took 1672.94 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 132\n",
      "Overlap 3 and 4\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 124\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 7 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 108\n",
      "3rd Window Start: 860\n",
      "4th Window Start: 1605\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 15s 364us/step - loss: 0.0148 - val_loss: 0.0180\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0115 - val_loss: 0.0172\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0109 - val_loss: 0.0140\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0100 - val_loss: 0.0171\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0092 - val_loss: 0.0189\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0077 - val_loss: 0.0190\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0067 - val_loss: 0.0264\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0061 - val_loss: 0.0198\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0056 - val_loss: 0.0171\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0053 - val_loss: 0.0150\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0051 - val_loss: 0.0166\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0048 - val_loss: 0.0187\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0047 - val_loss: 0.0180\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0045 - val_loss: 0.0157\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0044 - val_loss: 0.0143\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0043 - val_loss: 0.0140\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0042 - val_loss: 0.0130\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0041 - val_loss: 0.0129\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0040 - val_loss: 0.0111\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0040 - val_loss: 0.0102\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0039 - val_loss: 0.0100\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0038 - val_loss: 0.0105\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 13s 338us/step - loss: 0.0038 - val_loss: 0.0096\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 13s 338us/step - loss: 0.0037 - val_loss: 0.0094\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0036 - val_loss: 0.0096\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0036 - val_loss: 0.0095\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0035 - val_loss: 0.0095\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0034 - val_loss: 0.0099\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0033 - val_loss: 0.0093\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0033 - val_loss: 0.0096\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0032 - val_loss: 0.0096\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 13s 338us/step - loss: 0.0031 - val_loss: 0.0093\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0031 - val_loss: 0.0091\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0030 - val_loss: 0.0086\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0030 - val_loss: 0.0083\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0029 - val_loss: 0.0085\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0029 - val_loss: 0.0081\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0028 - val_loss: 0.0079\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0028 - val_loss: 0.0078\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0028 - val_loss: 0.0078\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0028 - val_loss: 0.0078\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0027 - val_loss: 0.0076\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0027 - val_loss: 0.0078\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0027 - val_loss: 0.0080\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0026 - val_loss: 0.0069\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0026 - val_loss: 0.0074\n",
      "Epoch 47/70\n",
      "39971/39971 [==============================] - 13s 338us/step - loss: 0.0026 - val_loss: 0.0073\n",
      "Epoch 48/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0026 - val_loss: 0.0078\n",
      "Epoch 49/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0026 - val_loss: 0.0079\n",
      "Epoch 50/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0025 - val_loss: 0.0081\n",
      "Epoch 51/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0025 - val_loss: 0.0086\n",
      "Epoch 52/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0025 - val_loss: 0.0084\n",
      "Epoch 53/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0025 - val_loss: 0.0091\n",
      "Epoch 54/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0025 - val_loss: 0.0091\n",
      "Epoch 55/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0025 - val_loss: 0.0095\n",
      "Epoch 56/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0024 - val_loss: 0.0100\n",
      "Epoch 57/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0024 - val_loss: 0.0091\n",
      "Epoch 58/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0024 - val_loss: 0.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0024 - val_loss: 0.0112\n",
      "Epoch 60/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0024 - val_loss: 0.0108\n",
      "Epoch 61/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0024 - val_loss: 0.0122\n",
      "Epoch 62/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0024 - val_loss: 0.0112\n",
      "Epoch 63/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0023 - val_loss: 0.0125\n",
      "Epoch 64/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0023 - val_loss: 0.0113\n",
      "Epoch 65/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0023 - val_loss: 0.0111\n",
      "Validation RMSE:  0.10539708 \n",
      "\n",
      "Model took 887.95 seconds to train\n",
      "2  \t4     \t0.399558\t0.327712\t0.0891916\t0.8\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 124\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 23 9\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 44\n",
      "3rd Window Start: 860\n",
      "4th Window Start: 1605\n",
      "Activation Function is: relu\n",
      "Optimizer is: sgd\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 14s 358us/step - loss: 0.0260 - val_loss: 0.0395\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 13s 334us/step - loss: 0.0162 - val_loss: 0.0341\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0154 - val_loss: 0.0305\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0149 - val_loss: 0.0278\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0145 - val_loss: 0.0258\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0142 - val_loss: 0.0242\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0140 - val_loss: 0.0231\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0139 - val_loss: 0.0223\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0138 - val_loss: 0.0217\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 13s 338us/step - loss: 0.0137 - val_loss: 0.0212\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0137 - val_loss: 0.0208\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0136 - val_loss: 0.0204\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0136 - val_loss: 0.0200\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0135 - val_loss: 0.0198\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0135 - val_loss: 0.0196\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0134 - val_loss: 0.0194\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0134 - val_loss: 0.0192\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 13s 333us/step - loss: 0.0133 - val_loss: 0.0190\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0133 - val_loss: 0.0189\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0133 - val_loss: 0.0188\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0132 - val_loss: 0.0186\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0132 - val_loss: 0.0184\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0131 - val_loss: 0.0183\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0131 - val_loss: 0.0183\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0131 - val_loss: 0.0181\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0130 - val_loss: 0.0180\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0130 - val_loss: 0.0179\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0129 - val_loss: 0.0178\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0129 - val_loss: 0.0177\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 13s 338us/step - loss: 0.0129 - val_loss: 0.0176\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0128 - val_loss: 0.0176\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0128 - val_loss: 0.0175\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0127 - val_loss: 0.0174\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0127 - val_loss: 0.0173\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 13s 334us/step - loss: 0.0126 - val_loss: 0.0172\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0126 - val_loss: 0.0171\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0125 - val_loss: 0.0170\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0125 - val_loss: 0.0169\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0125 - val_loss: 0.0168\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0124 - val_loss: 0.0167\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0124 - val_loss: 0.0166\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 13s 334us/step - loss: 0.0124 - val_loss: 0.0165\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0123 - val_loss: 0.0165\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0123 - val_loss: 0.0163\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0122 - val_loss: 0.0163\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 13s 334us/step - loss: 0.0122 - val_loss: 0.0163\n",
      "Epoch 47/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0121 - val_loss: 0.0162\n",
      "Epoch 48/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0121 - val_loss: 0.0162\n",
      "Epoch 49/70\n",
      "39971/39971 [==============================] - 13s 334us/step - loss: 0.0121 - val_loss: 0.0161\n",
      "Epoch 50/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0120 - val_loss: 0.0160\n",
      "Epoch 51/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0120 - val_loss: 0.0159\n",
      "Epoch 52/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0120 - val_loss: 0.0159\n",
      "Epoch 53/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0119 - val_loss: 0.0158\n",
      "Epoch 54/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0119 - val_loss: 0.0157\n",
      "Epoch 55/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0119 - val_loss: 0.0156\n",
      "Epoch 56/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0118 - val_loss: 0.0156\n",
      "Epoch 57/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0118 - val_loss: 0.0155\n",
      "Epoch 58/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0118 - val_loss: 0.0154\n",
      "Epoch 59/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0118 - val_loss: 0.0154\n",
      "Epoch 60/70\n",
      "39971/39971 [==============================] - 13s 338us/step - loss: 0.0117 - val_loss: 0.0153\n",
      "Epoch 61/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 62/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 63/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 64/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 65/70\n",
      "39971/39971 [==============================] - 13s 335us/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 66/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 67/70\n",
      "39971/39971 [==============================] - 13s 334us/step - loss: 0.0115 - val_loss: 0.0149\n",
      "Epoch 68/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0115 - val_loss: 0.0149\n",
      "Epoch 69/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0115 - val_loss: 0.0149\n",
      "Epoch 70/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0115 - val_loss: 0.0148\n",
      "Validation RMSE:  0.12169493 \n",
      "\n",
      "Model took 947.62 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 128\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 29 22 7 1\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 100\n",
      "3rd Window Start: 844\n",
      "4th Window Start: 1221\n",
      "Activation Function is: elu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36 37]\n",
      "Lags length:\n",
      " 29\n",
      "Length Used:\n",
      " 29\n",
      "New Dataset shape after Lags: (49963, 30)\n",
      "Training and test data length 39970 9993\n",
      "trainX.shape[0]:  39970\n",
      "window_length:  29\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39970, 29)\n",
      "Train X shape after np.reshape (39970, 29, 1)\n",
      "Test X shape after np.reshape (9993, 29, 1)\n",
      "Train Y Shape (39970,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39970 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39970/39970 [==============================] - 15s 365us/step - loss: 0.0146 - val_loss: 0.0178\n",
      "Epoch 2/70\n",
      "39970/39970 [==============================] - 13s 335us/step - loss: 0.0118 - val_loss: 0.0155\n",
      "Epoch 3/70\n",
      "39970/39970 [==============================] - 13s 336us/step - loss: 0.0116 - val_loss: 0.0141\n",
      "Epoch 4/70\n",
      "39970/39970 [==============================] - 14s 339us/step - loss: 0.0113 - val_loss: 0.0136\n",
      "Epoch 5/70\n",
      "39970/39970 [==============================] - 13s 336us/step - loss: 0.0111 - val_loss: 0.0135\n",
      "Epoch 6/70\n",
      "39970/39970 [==============================] - 13s 336us/step - loss: 0.0109 - val_loss: 0.0134\n",
      "Epoch 7/70\n",
      "39970/39970 [==============================] - 14s 341us/step - loss: 0.0105 - val_loss: 0.0137\n",
      "Epoch 8/70\n",
      "39970/39970 [==============================] - 13s 337us/step - loss: 0.0097 - val_loss: 0.0139\n",
      "Epoch 9/70\n",
      "39970/39970 [==============================] - 13s 338us/step - loss: 0.0081 - val_loss: 0.0185\n",
      "Epoch 10/70\n",
      "39970/39970 [==============================] - 13s 334us/step - loss: 0.0070 - val_loss: 0.0220\n",
      "Epoch 11/70\n",
      "39970/39970 [==============================] - 13s 336us/step - loss: 0.0062 - val_loss: 0.0200\n",
      "Epoch 12/70\n",
      "39970/39970 [==============================] - 14s 338us/step - loss: 0.0057 - val_loss: 0.0179\n",
      "Epoch 13/70\n",
      "39970/39970 [==============================] - 13s 336us/step - loss: 0.0054 - val_loss: 0.0167\n",
      "Epoch 14/70\n",
      "39970/39970 [==============================] - 13s 337us/step - loss: 0.0052 - val_loss: 0.0162\n",
      "Epoch 15/70\n",
      "39970/39970 [==============================] - 13s 337us/step - loss: 0.0051 - val_loss: 0.0163\n",
      "Epoch 16/70\n",
      "39970/39970 [==============================] - 13s 335us/step - loss: 0.0050 - val_loss: 0.0162\n",
      "Epoch 17/70\n",
      "39970/39970 [==============================] - 13s 335us/step - loss: 0.0049 - val_loss: 0.0162\n",
      "Epoch 18/70\n",
      "39970/39970 [==============================] - 13s 337us/step - loss: 0.0048 - val_loss: 0.0163\n",
      "Epoch 19/70\n",
      "39970/39970 [==============================] - 13s 336us/step - loss: 0.0047 - val_loss: 0.0162\n",
      "Epoch 20/70\n",
      "39970/39970 [==============================] - 13s 338us/step - loss: 0.0046 - val_loss: 0.0154\n",
      "Epoch 21/70\n",
      "39970/39970 [==============================] - 13s 335us/step - loss: 0.0045 - val_loss: 0.0148\n",
      "Epoch 22/70\n",
      "39970/39970 [==============================] - 13s 335us/step - loss: 0.0044 - val_loss: 0.0151\n",
      "Epoch 23/70\n",
      "39970/39970 [==============================] - 13s 334us/step - loss: 0.0043 - val_loss: 0.0163\n",
      "Epoch 24/70\n",
      "39970/39970 [==============================] - 13s 336us/step - loss: 0.0042 - val_loss: 0.0155\n",
      "Epoch 25/70\n",
      "39970/39970 [==============================] - 13s 337us/step - loss: 0.0042 - val_loss: 0.0151\n",
      "Epoch 26/70\n",
      "39970/39970 [==============================] - 13s 337us/step - loss: 0.0041 - val_loss: 0.0152\n",
      "Validation RMSE:  0.123467535 \n",
      "\n",
      "Model took 356.75 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 124\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 7 9\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 108\n",
      "3rd Window Start: 860\n",
      "4th Window Start: 1605\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 15s 368us/step - loss: 0.0156 - val_loss: 0.0183\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 14s 343us/step - loss: 0.0115 - val_loss: 0.0184\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0121 - val_loss: 0.0184\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0115 - val_loss: 0.0172\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 14s 344us/step - loss: 0.0100 - val_loss: 0.0126\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0089 - val_loss: 0.0142\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0074 - val_loss: 0.0163\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0065 - val_loss: 0.0166\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0059 - val_loss: 0.0168\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 13s 336us/step - loss: 0.0055 - val_loss: 0.0155\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0051 - val_loss: 0.0148\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0048 - val_loss: 0.0148\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0046 - val_loss: 0.0150\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0044 - val_loss: 0.0147\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0043 - val_loss: 0.0147\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0042 - val_loss: 0.0136\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0041 - val_loss: 0.0129\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0040 - val_loss: 0.0126\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0040 - val_loss: 0.0123\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0039 - val_loss: 0.0124\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0038 - val_loss: 0.0125\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0038 - val_loss: 0.0119\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0037 - val_loss: 0.0113\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0037 - val_loss: 0.0104\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0036 - val_loss: 0.0098\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0036 - val_loss: 0.0101\n",
      "Epoch 27/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0036 - val_loss: 0.0090\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0036 - val_loss: 0.0087\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 13s 338us/step - loss: 0.0035 - val_loss: 0.0085\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0035 - val_loss: 0.0084\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0034 - val_loss: 0.0081\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0034 - val_loss: 0.0081\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0033 - val_loss: 0.0087\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0032 - val_loss: 0.0085\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0032 - val_loss: 0.0085\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0031 - val_loss: 0.0084\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0031 - val_loss: 0.0084\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0030 - val_loss: 0.0085\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0030 - val_loss: 0.0081\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0029 - val_loss: 0.0078\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0029 - val_loss: 0.0080\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0029 - val_loss: 0.0080\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 14s 345us/step - loss: 0.0028 - val_loss: 0.0080\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0028 - val_loss: 0.0078\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0027 - val_loss: 0.0078\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0027 - val_loss: 0.0077\n",
      "Epoch 47/70\n",
      "39971/39971 [==============================] - 14s 338us/step - loss: 0.0027 - val_loss: 0.0077\n",
      "Epoch 48/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0026 - val_loss: 0.0079\n",
      "Epoch 49/70\n",
      "39971/39971 [==============================] - 14s 343us/step - loss: 0.0026 - val_loss: 0.0079\n",
      "Epoch 50/70\n",
      "39971/39971 [==============================] - 14s 343us/step - loss: 0.0026 - val_loss: 0.0085\n",
      "Epoch 51/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0025 - val_loss: 0.0085\n",
      "Epoch 52/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0025 - val_loss: 0.0088\n",
      "Epoch 53/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0025 - val_loss: 0.0085\n",
      "Epoch 54/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 55/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0024 - val_loss: 0.0086\n",
      "Epoch 56/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0024 - val_loss: 0.0081\n",
      "Epoch 57/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0024 - val_loss: 0.0082\n",
      "Epoch 58/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0024 - val_loss: 0.0084\n",
      "Epoch 59/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0023 - val_loss: 0.0081\n",
      "Epoch 60/70\n",
      "39971/39971 [==============================] - 14s 341us/step - loss: 0.0023 - val_loss: 0.0078\n",
      "Epoch 61/70\n",
      "39971/39971 [==============================] - 13s 337us/step - loss: 0.0023 - val_loss: 0.0085\n",
      "Epoch 62/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0023 - val_loss: 0.0084\n",
      "Epoch 63/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0023 - val_loss: 0.0084\n",
      "Epoch 64/70\n",
      "39971/39971 [==============================] - 14s 340us/step - loss: 0.0023 - val_loss: 0.0089\n",
      "Epoch 65/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0023 - val_loss: 0.0092\n",
      "Epoch 66/70\n",
      "39971/39971 [==============================] - 14s 342us/step - loss: 0.0023 - val_loss: 0.0087\n",
      "Epoch 67/70\n",
      "39971/39971 [==============================] - 14s 339us/step - loss: 0.0023 - val_loss: 0.0097\n",
      "Validation RMSE:  0.09829551 \n",
      "\n",
      "Model took 917.38 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 124\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 29 22 23 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 44\n",
      "3rd Window Start: 844\n",
      "4th Window Start: 1093\n",
      "Activation Function is: elu\n",
      "Optimizer is: sgd\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36 37]\n",
      "Lags length:\n",
      " 29\n",
      "Length Used:\n",
      " 29\n",
      "New Dataset shape after Lags: (49963, 30)\n",
      "Training and test data length 39970 9993\n",
      "trainX.shape[0]:  39970\n",
      "window_length:  29\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39970, 29)\n",
      "Train X shape after np.reshape (39970, 29, 1)\n",
      "Test X shape after np.reshape (9993, 29, 1)\n",
      "Train Y Shape (39970,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39970 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39970/39970 [==============================] - 15s 377us/step - loss: 0.0190 - val_loss: 0.0337\n",
      "Epoch 2/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0145 - val_loss: 0.0308\n",
      "Epoch 3/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0142 - val_loss: 0.0287\n",
      "Epoch 4/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0139 - val_loss: 0.0271\n",
      "Epoch 5/70\n",
      "39970/39970 [==============================] - 14s 353us/step - loss: 0.0138 - val_loss: 0.0259\n",
      "Epoch 6/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0137 - val_loss: 0.0251\n",
      "Epoch 7/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0136 - val_loss: 0.0244\n",
      "Epoch 8/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0136 - val_loss: 0.0240\n",
      "Epoch 9/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0136 - val_loss: 0.0236\n",
      "Epoch 10/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0135 - val_loss: 0.0234\n",
      "Epoch 11/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0135 - val_loss: 0.0232\n",
      "Epoch 12/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0135 - val_loss: 0.0230\n",
      "Epoch 13/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0135 - val_loss: 0.0229\n",
      "Epoch 14/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0135 - val_loss: 0.0228\n",
      "Epoch 15/70\n",
      "39970/39970 [==============================] - 14s 352us/step - loss: 0.0135 - val_loss: 0.0227\n",
      "Epoch 16/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0135 - val_loss: 0.0226\n",
      "Epoch 17/70\n",
      "39970/39970 [==============================] - 14s 352us/step - loss: 0.0135 - val_loss: 0.0226\n",
      "Epoch 18/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0135 - val_loss: 0.0225\n",
      "Epoch 19/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0135 - val_loss: 0.0225\n",
      "Epoch 20/70\n",
      "39970/39970 [==============================] - 14s 352us/step - loss: 0.0134 - val_loss: 0.0225\n",
      "Epoch 21/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0134 - val_loss: 0.0224\n",
      "Epoch 22/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0134 - val_loss: 0.0224\n",
      "Epoch 23/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0134 - val_loss: 0.0224\n",
      "Epoch 24/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0134 - val_loss: 0.0223\n",
      "Epoch 25/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0134 - val_loss: 0.0223\n",
      "Epoch 26/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0134 - val_loss: 0.0223\n",
      "Epoch 27/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0134 - val_loss: 0.0222\n",
      "Epoch 28/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0134 - val_loss: 0.0222\n",
      "Epoch 29/70\n",
      "39970/39970 [==============================] - 14s 354us/step - loss: 0.0134 - val_loss: 0.0222\n",
      "Epoch 30/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0134 - val_loss: 0.0221\n",
      "Epoch 31/70\n",
      "39970/39970 [==============================] - 14s 352us/step - loss: 0.0134 - val_loss: 0.0221\n",
      "Epoch 32/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0134 - val_loss: 0.0221\n",
      "Epoch 33/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0133 - val_loss: 0.0220\n",
      "Epoch 34/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0133 - val_loss: 0.0220\n",
      "Epoch 35/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0133 - val_loss: 0.0220\n",
      "Epoch 36/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0133 - val_loss: 0.0219\n",
      "Epoch 37/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0133 - val_loss: 0.0219\n",
      "Epoch 38/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0133 - val_loss: 0.0219\n",
      "Epoch 39/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0133 - val_loss: 0.0218\n",
      "Epoch 40/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0133 - val_loss: 0.0218\n",
      "Epoch 41/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0133 - val_loss: 0.0217\n",
      "Epoch 42/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0133 - val_loss: 0.0217\n",
      "Epoch 43/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0132 - val_loss: 0.0217\n",
      "Epoch 44/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0132 - val_loss: 0.0216\n",
      "Epoch 45/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0132 - val_loss: 0.0216\n",
      "Epoch 46/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0132 - val_loss: 0.0215\n",
      "Epoch 47/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0132 - val_loss: 0.0215\n",
      "Epoch 48/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0132 - val_loss: 0.0214\n",
      "Epoch 49/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0132 - val_loss: 0.0214\n",
      "Epoch 50/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0132 - val_loss: 0.0213\n",
      "Epoch 51/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0132 - val_loss: 0.0213\n",
      "Epoch 52/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0131 - val_loss: 0.0212\n",
      "Epoch 53/70\n",
      "39970/39970 [==============================] - 14s 349us/step - loss: 0.0131 - val_loss: 0.0212\n",
      "Epoch 54/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0131 - val_loss: 0.0211\n",
      "Epoch 55/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0131 - val_loss: 0.0211\n",
      "Epoch 56/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0131 - val_loss: 0.0210\n",
      "Epoch 57/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0131 - val_loss: 0.0210\n",
      "Epoch 58/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0131 - val_loss: 0.0209\n",
      "Epoch 59/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0130 - val_loss: 0.0209\n",
      "Epoch 60/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0130 - val_loss: 0.0208\n",
      "Epoch 61/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0130 - val_loss: 0.0208\n",
      "Epoch 62/70\n",
      "39970/39970 [==============================] - 14s 352us/step - loss: 0.0130 - val_loss: 0.0207\n",
      "Epoch 63/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0130 - val_loss: 0.0206\n",
      "Epoch 64/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0130 - val_loss: 0.0206\n",
      "Epoch 65/70\n",
      "39970/39970 [==============================] - 14s 350us/step - loss: 0.0129 - val_loss: 0.0205\n",
      "Epoch 66/70\n",
      "39970/39970 [==============================] - 14s 347us/step - loss: 0.0129 - val_loss: 0.0205\n",
      "Epoch 67/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0129 - val_loss: 0.0204\n",
      "Epoch 68/70\n",
      "39970/39970 [==============================] - 14s 348us/step - loss: 0.0129 - val_loss: 0.0203\n",
      "Epoch 69/70\n",
      "39970/39970 [==============================] - 14s 351us/step - loss: 0.0129 - val_loss: 0.0203\n",
      "Epoch 70/70\n",
      "39970/39970 [==============================] - 14s 352us/step - loss: 0.0128 - val_loss: 0.0202\n",
      "Validation RMSE:  0.14208193 \n",
      "\n",
      "Model took 985.00 seconds to train\n",
      "3  \t4     \t0.179253\t0.207731\t0.0891916\t0.8\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 0.0036 - val_loss: 0.0020\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 9.5273e-04 - val_loss: 0.0010\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 6.7620e-04 - val_loss: 8.9389e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 5.8920e-04 - val_loss: 6.4001e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 5.3040e-04 - val_loss: 5.3148e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 4.8417e-04 - val_loss: 4.7564e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 4.5010e-04 - val_loss: 4.9266e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 434us/step - loss: 4.2195e-04 - val_loss: 4.7761e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 3.9755e-04 - val_loss: 4.7608e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 3.7482e-04 - val_loss: 4.8512e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 3.5399e-04 - val_loss: 5.5011e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 3.3730e-04 - val_loss: 4.7236e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 3.2407e-04 - val_loss: 3.5191e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 3.0812e-04 - val_loss: 4.6518e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 2.9920e-04 - val_loss: 2.7378e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 2.8795e-04 - val_loss: 2.6091e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 2.7660e-04 - val_loss: 2.6011e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 2.6831e-04 - val_loss: 2.4782e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.5828e-04 - val_loss: 2.4901e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 2.5018e-04 - val_loss: 2.7919e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 434us/step - loss: 2.4477e-04 - val_loss: 2.5824e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 2.3867e-04 - val_loss: 2.9292e-04\n",
      "Epoch 23/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 17s 437us/step - loss: 2.3507e-04 - val_loss: 2.3243e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 2.2908e-04 - val_loss: 2.4764e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.2555e-04 - val_loss: 2.6020e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.2174e-04 - val_loss: 2.4168e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.1752e-04 - val_loss: 2.0996e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 435us/step - loss: 2.1459e-04 - val_loss: 2.4056e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 2.1290e-04 - val_loss: 2.1816e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 435us/step - loss: 2.1026e-04 - val_loss: 1.8092e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 2.0484e-04 - val_loss: 2.1831e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 435us/step - loss: 2.0069e-04 - val_loss: 1.8826e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.9374e-04 - val_loss: 2.5066e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.9029e-04 - val_loss: 1.7680e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.8256e-04 - val_loss: 1.9509e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.7918e-04 - val_loss: 1.9242e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 1.7507e-04 - val_loss: 1.7684e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.7228e-04 - val_loss: 1.8212e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 1.7205e-04 - val_loss: 1.9000e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.6973e-04 - val_loss: 1.7285e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.6822e-04 - val_loss: 1.4186e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 1.6514e-04 - val_loss: 1.3129e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.6192e-04 - val_loss: 1.4299e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 1.6048e-04 - val_loss: 1.4643e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.5742e-04 - val_loss: 1.6350e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 1.5520e-04 - val_loss: 1.6538e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 435us/step - loss: 1.5357e-04 - val_loss: 1.7207e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.5129e-04 - val_loss: 1.4156e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.5037e-04 - val_loss: 1.5639e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 1.4868e-04 - val_loss: 1.7530e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 435us/step - loss: 1.4641e-04 - val_loss: 1.4262e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.4460e-04 - val_loss: 1.4123e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4526e-04 - val_loss: 1.4572e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 1.4326e-04 - val_loss: 1.7852e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 1.4386e-04 - val_loss: 1.4763e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 1.4343e-04 - val_loss: 1.4510e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 1.4132e-04 - val_loss: 1.5573e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 1.4130e-04 - val_loss: 1.4504e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 1.3832e-04 - val_loss: 2.0907e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.4019e-04 - val_loss: 1.4061e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.3719e-04 - val_loss: 1.3711e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 1.3610e-04 - val_loss: 1.4147e-04\n",
      "Validation RMSE:  0.011894033 \n",
      "\n",
      "Model took 1069.16 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 23 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 444\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 25s 633us/step - loss: 0.0135 - val_loss: 0.0173\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0099 - val_loss: 0.0190\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0097 - val_loss: 0.0143\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0079 - val_loss: 0.0161\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0067 - val_loss: 0.0179\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0059 - val_loss: 0.0182\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0053 - val_loss: 0.0173\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0049 - val_loss: 0.0156\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0048 - val_loss: 0.0161\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0046 - val_loss: 0.0158\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0044 - val_loss: 0.0160\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0042 - val_loss: 0.0155\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0040 - val_loss: 0.0154\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0039 - val_loss: 0.0146\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0038 - val_loss: 0.0149\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0037 - val_loss: 0.0141\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0035 - val_loss: 0.0140\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0034 - val_loss: 0.0138\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0033 - val_loss: 0.0128\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0032 - val_loss: 0.0121\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0032 - val_loss: 0.0120\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0031 - val_loss: 0.0129\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0030 - val_loss: 0.0128\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0030 - val_loss: 0.0122\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0029 - val_loss: 0.0114\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0028 - val_loss: 0.0118\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0028 - val_loss: 0.0112\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0027 - val_loss: 0.0115\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0027 - val_loss: 0.0116\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0026 - val_loss: 0.0120\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0025 - val_loss: 0.0128\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0025 - val_loss: 0.0130\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0024 - val_loss: 0.0127\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0024 - val_loss: 0.0130\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0023 - val_loss: 0.0135\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0023 - val_loss: 0.0135\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0022 - val_loss: 0.0132\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0022 - val_loss: 0.0129\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0022 - val_loss: 0.0123\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0021 - val_loss: 0.0126\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0021 - val_loss: 0.0126\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0021 - val_loss: 0.0127\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0021 - val_loss: 0.0130\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0021 - val_loss: 0.0127\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0020 - val_loss: 0.0124\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0020 - val_loss: 0.0125\n",
      "Epoch 47/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0020 - val_loss: 0.0128\n",
      "Validation RMSE:  0.11305236 \n",
      "\n",
      "Model took 1127.28 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 23 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 444\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 25s 630us/step - loss: 0.0146 - val_loss: 0.0177\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0105 - val_loss: 0.0177\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0100 - val_loss: 0.0179\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0094 - val_loss: 0.0162\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0077 - val_loss: 0.0179\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0068 - val_loss: 0.0177\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0062 - val_loss: 0.0192\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0055 - val_loss: 0.0168\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0050 - val_loss: 0.0150\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0048 - val_loss: 0.0147\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0045 - val_loss: 0.0140\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0043 - val_loss: 0.0145\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 24s 591us/step - loss: 0.0041 - val_loss: 0.0144\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0039 - val_loss: 0.0139\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0037 - val_loss: 0.0119\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0036 - val_loss: 0.0113\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0035 - val_loss: 0.0105\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0033 - val_loss: 0.0107\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0032 - val_loss: 0.0101\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 24s 591us/step - loss: 0.0031 - val_loss: 0.0096\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0030 - val_loss: 0.0095\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0029 - val_loss: 0.0103\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0028 - val_loss: 0.0104\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0027 - val_loss: 0.0100\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0027 - val_loss: 0.0109\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0026 - val_loss: 0.0107\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0025 - val_loss: 0.0103\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0025 - val_loss: 0.0101\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0024 - val_loss: 0.0110\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0024 - val_loss: 0.0116\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0024 - val_loss: 0.0110\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0023 - val_loss: 0.0106\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0023 - val_loss: 0.0112\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0023 - val_loss: 0.0101\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 24s 590us/step - loss: 0.0023 - val_loss: 0.0101\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0022 - val_loss: 0.0105\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0022 - val_loss: 0.0103\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0022 - val_loss: 0.0106\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0022 - val_loss: 0.0099\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0021 - val_loss: 0.0089\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0021 - val_loss: 0.0088\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0021 - val_loss: 0.0096\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0021 - val_loss: 0.0088\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 24s 591us/step - loss: 0.0020 - val_loss: 0.0100\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0020 - val_loss: 0.0100\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0020 - val_loss: 0.0097\n",
      "Epoch 47/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0020 - val_loss: 0.0103\n",
      "Epoch 48/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0020 - val_loss: 0.0098\n",
      "Epoch 49/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0020 - val_loss: 0.0093\n",
      "Epoch 50/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0019 - val_loss: 0.0098\n",
      "Epoch 51/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0019 - val_loss: 0.0098\n",
      "Epoch 52/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0019 - val_loss: 0.0097\n",
      "Epoch 53/70\n",
      "39971/39971 [==============================] - 24s 590us/step - loss: 0.0019 - val_loss: 0.0095\n",
      "Epoch 54/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0018 - val_loss: 0.0092\n",
      "Epoch 55/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0018 - val_loss: 0.0092\n",
      "Epoch 56/70\n",
      "39971/39971 [==============================] - 24s 591us/step - loss: 0.0018 - val_loss: 0.0095\n",
      "Epoch 57/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0018 - val_loss: 0.0089\n",
      "Epoch 58/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0017 - val_loss: 0.0093\n",
      "Epoch 59/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0017 - val_loss: 0.0095\n",
      "Epoch 60/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0017 - val_loss: 0.0093\n",
      "Epoch 61/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0017 - val_loss: 0.0094\n",
      "Epoch 62/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0017 - val_loss: 0.0096\n",
      "Epoch 63/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0017 - val_loss: 0.0098\n",
      "Validation RMSE:  0.098828174 \n",
      "\n",
      "Model took 1505.96 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 23 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 444\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 25s 631us/step - loss: 0.0141 - val_loss: 0.0176\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0106 - val_loss: 0.0161\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0095 - val_loss: 0.0165\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0085 - val_loss: 0.0157\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0069 - val_loss: 0.0181\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0061 - val_loss: 0.0193\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 24s 590us/step - loss: 0.0055 - val_loss: 0.0195\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0051 - val_loss: 0.0184\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0048 - val_loss: 0.0180\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0047 - val_loss: 0.0171\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0045 - val_loss: 0.0176\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0044 - val_loss: 0.0168\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0043 - val_loss: 0.0169\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0042 - val_loss: 0.0166\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0040 - val_loss: 0.0162\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0039 - val_loss: 0.0159\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0038 - val_loss: 0.0165\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0036 - val_loss: 0.0146\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 24s 591us/step - loss: 0.0035 - val_loss: 0.0140\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0034 - val_loss: 0.0136\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0033 - val_loss: 0.0124\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0032 - val_loss: 0.0121\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0031 - val_loss: 0.0110\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0030 - val_loss: 0.0107\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0029 - val_loss: 0.0107\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 24s 603us/step - loss: 0.0028 - val_loss: 0.0105\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0027 - val_loss: 0.0092\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0026 - val_loss: 0.0101\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0026 - val_loss: 0.0099\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0026 - val_loss: 0.0095\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0025 - val_loss: 0.0085\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0024 - val_loss: 0.0085\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0024 - val_loss: 0.0076\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 24s 603us/step - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0022 - val_loss: 0.0084\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 24s 591us/step - loss: 0.0022 - val_loss: 0.0082\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0022 - val_loss: 0.0080\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0022 - val_loss: 0.0087\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0021 - val_loss: 0.0085\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0021 - val_loss: 0.0080\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0021 - val_loss: 0.0088\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0021 - val_loss: 0.0087\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0020 - val_loss: 0.0087\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0020 - val_loss: 0.0092\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0020 - val_loss: 0.0094\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0020 - val_loss: 0.0100\n",
      "Epoch 47/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0020 - val_loss: 0.0095\n",
      "Epoch 48/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0019 - val_loss: 0.0100\n",
      "Epoch 49/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0019 - val_loss: 0.0103\n",
      "Epoch 50/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0019 - val_loss: 0.0102\n",
      "Epoch 51/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0019 - val_loss: 0.0111\n",
      "Epoch 52/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0018 - val_loss: 0.0119\n",
      "Epoch 53/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0018 - val_loss: 0.0126\n",
      "Validation RMSE:  0.11217341 \n",
      "\n",
      "Model took 1269.54 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 23 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 444\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 25s 637us/step - loss: 0.0150 - val_loss: 0.0169\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0105 - val_loss: 0.0175\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0106 - val_loss: 0.0198\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0099 - val_loss: 0.0245\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0102 - val_loss: 0.0187\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0083 - val_loss: 0.0205\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0065 - val_loss: 0.0190\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0056 - val_loss: 0.0189\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0051 - val_loss: 0.0164\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0048 - val_loss: 0.0147\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0047 - val_loss: 0.0137\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0045 - val_loss: 0.0126\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0043 - val_loss: 0.0125\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0040 - val_loss: 0.0127\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0039 - val_loss: 0.0120\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0037 - val_loss: 0.0125\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0036 - val_loss: 0.0126\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0035 - val_loss: 0.0124\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0034 - val_loss: 0.0120\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0033 - val_loss: 0.0118\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0031 - val_loss: 0.0118\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0030 - val_loss: 0.0113\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0029 - val_loss: 0.0110\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0029 - val_loss: 0.0109\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0028 - val_loss: 0.0099\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0027 - val_loss: 0.0094\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0027 - val_loss: 0.0088\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0026 - val_loss: 0.0087\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0026 - val_loss: 0.0087\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0025 - val_loss: 0.0080\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0025 - val_loss: 0.0072\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0024 - val_loss: 0.0075\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0024 - val_loss: 0.0070\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0023 - val_loss: 0.0072\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0023 - val_loss: 0.0069\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0022 - val_loss: 0.0067\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0022 - val_loss: 0.0074\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0022 - val_loss: 0.0062\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0022 - val_loss: 0.0071\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0021 - val_loss: 0.0066\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0021 - val_loss: 0.0067\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0021 - val_loss: 0.0065\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0021 - val_loss: 0.0069\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0020 - val_loss: 0.0069\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0020 - val_loss: 0.0072\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0020 - val_loss: 0.0070\n",
      "Epoch 47/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0020 - val_loss: 0.0070\n",
      "Epoch 48/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0019 - val_loss: 0.0070\n",
      "Epoch 49/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0019 - val_loss: 0.0073\n",
      "Epoch 50/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0019 - val_loss: 0.0073\n",
      "Epoch 51/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0019 - val_loss: 0.0074\n",
      "Epoch 52/70\n",
      "39971/39971 [==============================] - 24s 607us/step - loss: 0.0019 - val_loss: 0.0077\n",
      "Epoch 53/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0019 - val_loss: 0.0079\n",
      "Epoch 54/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0018 - val_loss: 0.0080\n",
      "Epoch 55/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0018 - val_loss: 0.0082\n",
      "Epoch 56/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0018 - val_loss: 0.0077\n",
      "Epoch 57/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0018 - val_loss: 0.0081\n",
      "Epoch 58/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0018 - val_loss: 0.0080\n",
      "Validation RMSE:  0.08924885 \n",
      "\n",
      "Model took 1390.94 seconds to train\n",
      "4  \t5     \t0.0880259\t0.0268972\t0.011894 \t0.113052\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 23 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 444\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 25s 637us/step - loss: 0.0145 - val_loss: 0.0178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0107 - val_loss: 0.0165\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0103 - val_loss: 0.0175\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0089 - val_loss: 0.0154\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0080 - val_loss: 0.0188\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0071 - val_loss: 0.0175\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0061 - val_loss: 0.0165\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0055 - val_loss: 0.0172\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0052 - val_loss: 0.0176\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0049 - val_loss: 0.0167\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0047 - val_loss: 0.0165\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0044 - val_loss: 0.0171\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0041 - val_loss: 0.0156\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0038 - val_loss: 0.0142\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0037 - val_loss: 0.0145\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0035 - val_loss: 0.0145\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0034 - val_loss: 0.0138\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0033 - val_loss: 0.0132\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0032 - val_loss: 0.0120\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0031 - val_loss: 0.0111\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0030 - val_loss: 0.0102\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0029 - val_loss: 0.0097\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0028 - val_loss: 0.0092\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0028 - val_loss: 0.0088\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0027 - val_loss: 0.0085\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0026 - val_loss: 0.0093\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0025 - val_loss: 0.0091\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0025 - val_loss: 0.0098\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0024 - val_loss: 0.0100\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 24s 591us/step - loss: 0.0024 - val_loss: 0.0101\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0023 - val_loss: 0.0101\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0023 - val_loss: 0.0101\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0023 - val_loss: 0.0102\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0022 - val_loss: 0.0109\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 24s 593us/step - loss: 0.0022 - val_loss: 0.0110\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 24s 592us/step - loss: 0.0022 - val_loss: 0.0119\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0022 - val_loss: 0.0112\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0021 - val_loss: 0.0111\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0021 - val_loss: 0.0099\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0021 - val_loss: 0.0103\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0021 - val_loss: 0.0104\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0020 - val_loss: 0.0104\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0020 - val_loss: 0.0102\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0020 - val_loss: 0.0108\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 24s 603us/step - loss: 0.0020 - val_loss: 0.0106\n",
      "Validation RMSE:  0.10296688 \n",
      "\n",
      "Model took 1104.88 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 23 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 444\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 26s 642us/step - loss: 0.0137 - val_loss: 0.0205\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0110 - val_loss: 0.0165\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0099 - val_loss: 0.0173\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0086 - val_loss: 0.0172\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0078 - val_loss: 0.0185\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0068 - val_loss: 0.0195\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 24s 594us/step - loss: 0.0061 - val_loss: 0.0197\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0054 - val_loss: 0.0185\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0050 - val_loss: 0.0166\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 24s 604us/step - loss: 0.0047 - val_loss: 0.0159\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0044 - val_loss: 0.0155\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0043 - val_loss: 0.0165\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0041 - val_loss: 0.0166\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0040 - val_loss: 0.0164\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0038 - val_loss: 0.0163\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0037 - val_loss: 0.0161\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0035 - val_loss: 0.0151\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0034 - val_loss: 0.0146\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0033 - val_loss: 0.0146\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0032 - val_loss: 0.0132\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0032 - val_loss: 0.0135\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0032 - val_loss: 0.0137\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0031 - val_loss: 0.0130\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0030 - val_loss: 0.0128\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0030 - val_loss: 0.0122\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 24s 604us/step - loss: 0.0030 - val_loss: 0.0109\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0029 - val_loss: 0.0104\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0028 - val_loss: 0.0103\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0028 - val_loss: 0.0100\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0027 - val_loss: 0.0103\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0027 - val_loss: 0.0099\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0026 - val_loss: 0.0111\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0026 - val_loss: 0.0097\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0025 - val_loss: 0.0095\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0025 - val_loss: 0.0099\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0024 - val_loss: 0.0100\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0024 - val_loss: 0.0101\n",
      "Epoch 38/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0024 - val_loss: 0.0096\n",
      "Epoch 39/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0024 - val_loss: 0.0092\n",
      "Epoch 40/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0023 - val_loss: 0.0084\n",
      "Epoch 41/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0023 - val_loss: 0.0091\n",
      "Epoch 42/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0023 - val_loss: 0.0089\n",
      "Epoch 43/70\n",
      "39971/39971 [==============================] - 24s 605us/step - loss: 0.0023 - val_loss: 0.0090\n",
      "Epoch 44/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0023 - val_loss: 0.0088\n",
      "Epoch 45/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0022 - val_loss: 0.0098\n",
      "Epoch 46/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0022 - val_loss: 0.0100\n",
      "Epoch 47/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0022 - val_loss: 0.0099\n",
      "Epoch 48/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0022 - val_loss: 0.0113\n",
      "Epoch 49/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0022 - val_loss: 0.0112\n",
      "Epoch 50/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0021 - val_loss: 0.0126\n",
      "Epoch 51/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0021 - val_loss: 0.0135\n",
      "Epoch 52/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0021 - val_loss: 0.0146\n",
      "Epoch 53/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0021 - val_loss: 0.0153\n",
      "Epoch 54/70\n",
      "39971/39971 [==============================] - 24s 596us/step - loss: 0.0021 - val_loss: 0.0159\n",
      "Epoch 55/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0021 - val_loss: 0.0154\n",
      "Epoch 56/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0020 - val_loss: 0.0158\n",
      "Epoch 57/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0020 - val_loss: 0.0150\n",
      "Epoch 58/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0020 - val_loss: 0.0145\n",
      "Epoch 59/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0019 - val_loss: 0.0151\n",
      "Epoch 60/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0020 - val_loss: 0.0157\n",
      "Validation RMSE:  0.12533803 \n",
      "\n",
      "Model took 1443.44 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 9.2471e-04 - val_loss: 0.0010\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 6.4755e-04 - val_loss: 6.7917e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 5.6835e-04 - val_loss: 5.9570e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 5.1614e-04 - val_loss: 5.7099e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 4.7878e-04 - val_loss: 4.2413e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 4.4189e-04 - val_loss: 4.6401e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 4.1542e-04 - val_loss: 4.8271e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 3.9209e-04 - val_loss: 4.6611e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 436us/step - loss: 3.7279e-04 - val_loss: 4.3551e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 3.5560e-04 - val_loss: 3.6224e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 3.4168e-04 - val_loss: 3.3521e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 3.2663e-04 - val_loss: 2.7745e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.1289e-04 - val_loss: 2.9536e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.9568e-04 - val_loss: 3.1374e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.8709e-04 - val_loss: 3.0174e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.7842e-04 - val_loss: 3.2430e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.7109e-04 - val_loss: 3.3517e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.6787e-04 - val_loss: 3.6429e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.6065e-04 - val_loss: 3.1043e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.5717e-04 - val_loss: 2.8022e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.5143e-04 - val_loss: 2.7231e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.4634e-04 - val_loss: 2.5152e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 2.4213e-04 - val_loss: 2.4816e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.3854e-04 - val_loss: 2.3781e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.3764e-04 - val_loss: 2.4127e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.3150e-04 - val_loss: 2.7631e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.3142e-04 - val_loss: 2.3233e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.2533e-04 - val_loss: 2.1192e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.2237e-04 - val_loss: 2.1262e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.1301e-04 - val_loss: 2.2890e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.0450e-04 - val_loss: 2.6602e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 2.0248e-04 - val_loss: 2.0115e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.9419e-04 - val_loss: 2.4987e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.9466e-04 - val_loss: 1.8601e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.8873e-04 - val_loss: 2.4679e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.8867e-04 - val_loss: 1.9343e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.8526e-04 - val_loss: 1.8903e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.8191e-04 - val_loss: 1.8864e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.7689e-04 - val_loss: 2.4125e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.7722e-04 - val_loss: 1.8581e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.7241e-04 - val_loss: 2.2015e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.7279e-04 - val_loss: 1.8630e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.6801e-04 - val_loss: 2.4838e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.6941e-04 - val_loss: 1.8597e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.6532e-04 - val_loss: 2.0045e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.6472e-04 - val_loss: 1.8969e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.6237e-04 - val_loss: 2.0100e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.6050e-04 - val_loss: 2.0609e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.5809e-04 - val_loss: 2.0509e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.5717e-04 - val_loss: 1.8908e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.5559e-04 - val_loss: 1.8769e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.5318e-04 - val_loss: 2.0214e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5232e-04 - val_loss: 2.0249e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.5093e-04 - val_loss: 2.0259e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 1.5084e-04 - val_loss: 2.0158e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4904e-04 - val_loss: 1.8240e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.4760e-04 - val_loss: 1.8402e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.4683e-04 - val_loss: 1.7915e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4617e-04 - val_loss: 1.8386e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.4538e-04 - val_loss: 2.0528e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.4415e-04 - val_loss: 1.9926e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4162e-04 - val_loss: 1.8920e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.4153e-04 - val_loss: 1.8575e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.3953e-04 - val_loss: 1.7929e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.3908e-04 - val_loss: 1.8147e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.3785e-04 - val_loss: 1.7901e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 1.3619e-04 - val_loss: 1.6572e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3528e-04 - val_loss: 1.6941e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.3367e-04 - val_loss: 1.6426e-04\n",
      "Validation RMSE:  0.012816381 \n",
      "\n",
      "Model took 1214.08 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 0.0033 - val_loss: 0.0016\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 437us/step - loss: 9.5881e-04 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 6.9036e-04 - val_loss: 8.1506e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 5.9197e-04 - val_loss: 9.0738e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 5.2902e-04 - val_loss: 6.7222e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 4.9196e-04 - val_loss: 4.8358e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 4.5451e-04 - val_loss: 5.5982e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 4.2747e-04 - val_loss: 5.5159e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 4.0217e-04 - val_loss: 3.5515e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.8419e-04 - val_loss: 3.8498e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 3.6572e-04 - val_loss: 3.7919e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 3.4789e-04 - val_loss: 4.2068e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 3.3003e-04 - val_loss: 6.1011e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.1635e-04 - val_loss: 4.1547e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.0253e-04 - val_loss: 3.1884e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.9030e-04 - val_loss: 3.0016e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.7480e-04 - val_loss: 2.7316e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.6654e-04 - val_loss: 2.6856e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.6167e-04 - val_loss: 2.9887e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.5559e-04 - val_loss: 3.1565e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.5010e-04 - val_loss: 2.8380e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.4538e-04 - val_loss: 2.7980e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.4006e-04 - val_loss: 2.8336e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.3674e-04 - val_loss: 2.7834e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.2986e-04 - val_loss: 2.7362e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.2419e-04 - val_loss: 2.4016e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 2.2067e-04 - val_loss: 2.5894e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.1558e-04 - val_loss: 2.3539e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.1097e-04 - val_loss: 2.2904e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.0685e-04 - val_loss: 2.1303e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.0177e-04 - val_loss: 2.0446e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.9590e-04 - val_loss: 1.5842e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.9313e-04 - val_loss: 1.6268e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.9016e-04 - val_loss: 1.5913e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.8453e-04 - val_loss: 1.6241e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.8325e-04 - val_loss: 1.8811e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.8042e-04 - val_loss: 1.8818e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.7659e-04 - val_loss: 1.6984e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.7527e-04 - val_loss: 1.6517e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.7342e-04 - val_loss: 1.6412e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.7049e-04 - val_loss: 1.4773e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.6724e-04 - val_loss: 1.3664e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6677e-04 - val_loss: 1.2178e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.6439e-04 - val_loss: 1.1699e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.6044e-04 - val_loss: 1.2746e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.5844e-04 - val_loss: 1.4802e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.5703e-04 - val_loss: 1.2383e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5509e-04 - val_loss: 1.3070e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.5361e-04 - val_loss: 1.4518e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.5288e-04 - val_loss: 1.5158e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.5093e-04 - val_loss: 1.3596e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.4928e-04 - val_loss: 1.2785e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4791e-04 - val_loss: 1.3284e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4753e-04 - val_loss: 1.4117e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.4516e-04 - val_loss: 1.3017e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4429e-04 - val_loss: 1.5967e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.4332e-04 - val_loss: 1.7494e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4220e-04 - val_loss: 1.6993e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4077e-04 - val_loss: 1.5080e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4029e-04 - val_loss: 1.4688e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4025e-04 - val_loss: 1.6428e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3808e-04 - val_loss: 1.5302e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3708e-04 - val_loss: 1.4226e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3666e-04 - val_loss: 1.3674e-04\n",
      "Validation RMSE:  0.011693786 \n",
      "\n",
      "Model took 1111.79 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 22 23 12\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 508\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n",
      " 35 36 37 38]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49962, 29)\n",
      "Training and test data length 39969 9993\n",
      "trainX.shape[0]:  39969\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39969, 28)\n",
      "Train X shape after np.reshape (39969, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39969,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39969 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39969/39969 [==============================] - 26s 654us/step - loss: 0.0136 - val_loss: 0.0182\n",
      "Epoch 2/70\n",
      "39969/39969 [==============================] - 24s 600us/step - loss: 0.0102 - val_loss: 0.0161\n",
      "Epoch 3/70\n",
      "39969/39969 [==============================] - 24s 600us/step - loss: 0.0087 - val_loss: 0.0174\n",
      "Epoch 4/70\n",
      "39969/39969 [==============================] - 24s 605us/step - loss: 0.0072 - val_loss: 0.0181\n",
      "Epoch 5/70\n",
      "39969/39969 [==============================] - 24s 602us/step - loss: 0.0070 - val_loss: 0.0178\n",
      "Epoch 6/70\n",
      "39969/39969 [==============================] - 24s 600us/step - loss: 0.0063 - val_loss: 0.0179\n",
      "Epoch 7/70\n",
      "39969/39969 [==============================] - 24s 601us/step - loss: 0.0059 - val_loss: 0.0203\n",
      "Epoch 8/70\n",
      "39969/39969 [==============================] - 24s 606us/step - loss: 0.0054 - val_loss: 0.0235\n",
      "Epoch 9/70\n",
      "39969/39969 [==============================] - 24s 596us/step - loss: 0.0052 - val_loss: 0.0255\n",
      "Epoch 10/70\n",
      "39969/39969 [==============================] - 24s 600us/step - loss: 0.0049 - val_loss: 0.0274\n",
      "Epoch 11/70\n",
      "39969/39969 [==============================] - 24s 600us/step - loss: 0.0047 - val_loss: 0.0275\n",
      "Epoch 12/70\n",
      "39969/39969 [==============================] - 24s 598us/step - loss: 0.0045 - val_loss: 0.0236\n",
      "Epoch 13/70\n",
      "39969/39969 [==============================] - 24s 602us/step - loss: 0.0044 - val_loss: 0.0232\n",
      "Epoch 14/70\n",
      "39969/39969 [==============================] - 24s 601us/step - loss: 0.0043 - val_loss: 0.0222\n",
      "Epoch 15/70\n",
      "39969/39969 [==============================] - 24s 604us/step - loss: 0.0043 - val_loss: 0.0207\n",
      "Epoch 16/70\n",
      "39969/39969 [==============================] - 24s 598us/step - loss: 0.0042 - val_loss: 0.0201\n",
      "Epoch 17/70\n",
      "39969/39969 [==============================] - 24s 599us/step - loss: 0.0041 - val_loss: 0.0189\n",
      "Epoch 18/70\n",
      "39969/39969 [==============================] - 24s 598us/step - loss: 0.0040 - val_loss: 0.0177\n",
      "Epoch 19/70\n",
      "39969/39969 [==============================] - 24s 599us/step - loss: 0.0039 - val_loss: 0.0164\n",
      "Epoch 20/70\n",
      "39969/39969 [==============================] - 24s 601us/step - loss: 0.0039 - val_loss: 0.0168\n",
      "Epoch 21/70\n",
      "39969/39969 [==============================] - 24s 600us/step - loss: 0.0038 - val_loss: 0.0170\n",
      "Epoch 22/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39969/39969 [==============================] - 24s 599us/step - loss: 0.0038 - val_loss: 0.0164\n",
      "Validation RMSE:  0.12814797 \n",
      "\n",
      "Model took 536.80 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 1\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 446, 447, 448, 449, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (49010, 61)\n",
      "Training and test data length 39208 9802\n",
      "trainX.shape[0]:  39208\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39208, 60)\n",
      "Train X shape after np.reshape (39208, 20, 3)\n",
      "Test X shape after np.reshape (9802, 20, 3)\n",
      "Train Y Shape (39208,)\n",
      "Test Y Shape (9802,)\n",
      "Train on 39208 samples, validate on 9802 samples\n",
      "Epoch 1/70\n",
      "39208/39208 [==============================] - 19s 495us/step - loss: 0.0044 - val_loss: 0.0013\n",
      "Epoch 2/70\n",
      "39208/39208 [==============================] - 17s 436us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 9.2709e-04 - val_loss: 9.4674e-04\n",
      "Epoch 4/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 8.0144e-04 - val_loss: 9.2227e-04\n",
      "Epoch 5/70\n",
      "39208/39208 [==============================] - 17s 443us/step - loss: 7.1199e-04 - val_loss: 6.8022e-04\n",
      "Epoch 6/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 6.5362e-04 - val_loss: 5.9313e-04\n",
      "Epoch 7/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 5.9827e-04 - val_loss: 4.7144e-04\n",
      "Epoch 8/70\n",
      "39208/39208 [==============================] - 17s 438us/step - loss: 5.4869e-04 - val_loss: 5.3495e-04\n",
      "Epoch 9/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 5.2600e-04 - val_loss: 3.7341e-04\n",
      "Epoch 10/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 4.9731e-04 - val_loss: 4.3642e-04\n",
      "Epoch 11/70\n",
      "39208/39208 [==============================] - 17s 446us/step - loss: 4.7643e-04 - val_loss: 4.7641e-04\n",
      "Epoch 12/70\n",
      "39208/39208 [==============================] - 17s 445us/step - loss: 4.5858e-04 - val_loss: 4.7494e-04\n",
      "Epoch 13/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 4.3971e-04 - val_loss: 4.3413e-04\n",
      "Epoch 14/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 4.2574e-04 - val_loss: 4.7111e-04\n",
      "Epoch 15/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 4.1457e-04 - val_loss: 4.2886e-04\n",
      "Epoch 16/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 4.0693e-04 - val_loss: 4.1069e-04\n",
      "Epoch 17/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 3.9863e-04 - val_loss: 3.8666e-04\n",
      "Epoch 18/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 3.8999e-04 - val_loss: 3.9010e-04\n",
      "Epoch 19/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 3.8439e-04 - val_loss: 4.0590e-04\n",
      "Epoch 20/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 3.8214e-04 - val_loss: 3.8248e-04\n",
      "Epoch 21/70\n",
      "39208/39208 [==============================] - 17s 445us/step - loss: 3.7444e-04 - val_loss: 4.2148e-04\n",
      "Epoch 22/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 3.6626e-04 - val_loss: 4.1230e-04\n",
      "Epoch 23/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 3.6437e-04 - val_loss: 3.9280e-04\n",
      "Epoch 24/70\n",
      "39208/39208 [==============================] - 17s 437us/step - loss: 3.6106e-04 - val_loss: 4.0906e-04\n",
      "Epoch 25/70\n",
      "39208/39208 [==============================] - 17s 438us/step - loss: 3.5552e-04 - val_loss: 4.3937e-04\n",
      "Epoch 26/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 3.4836e-04 - val_loss: 4.4497e-04\n",
      "Epoch 27/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 3.5007e-04 - val_loss: 4.5784e-04\n",
      "Epoch 28/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 3.4522e-04 - val_loss: 4.8318e-04\n",
      "Epoch 29/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 3.3735e-04 - val_loss: 4.9588e-04\n",
      "Validation RMSE:  0.022268439 \n",
      "\n",
      "Model took 508.42 seconds to train\n",
      "5  \t6     \t0.06827  \t0.0457488\t0.0116938\t0.128148\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (49010, 61)\n",
      "Training and test data length 39208 9802\n",
      "trainX.shape[0]:  39208\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39208, 60)\n",
      "Train X shape after np.reshape (39208, 20, 3)\n",
      "Test X shape after np.reshape (9802, 20, 3)\n",
      "Train Y Shape (39208,)\n",
      "Test Y Shape (9802,)\n",
      "Train on 39208 samples, validate on 9802 samples\n",
      "Epoch 1/70\n",
      "39208/39208 [==============================] - 19s 496us/step - loss: 0.0094 - val_loss: 0.0030\n",
      "Epoch 2/70\n",
      "39208/39208 [==============================] - 17s 438us/step - loss: 0.0013 - val_loss: 6.9219e-04\n",
      "Epoch 3/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 7.4670e-04 - val_loss: 5.9007e-04\n",
      "Epoch 4/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 5.9725e-04 - val_loss: 7.5180e-04\n",
      "Epoch 5/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 5.2358e-04 - val_loss: 5.9526e-04\n",
      "Epoch 6/70\n",
      "39208/39208 [==============================] - 17s 443us/step - loss: 4.7679e-04 - val_loss: 4.9846e-04\n",
      "Epoch 7/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 4.6443e-04 - val_loss: 4.5970e-04\n",
      "Epoch 8/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 4.4855e-04 - val_loss: 4.2663e-04\n",
      "Epoch 9/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 4.3083e-04 - val_loss: 4.1604e-04\n",
      "Epoch 10/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 4.1953e-04 - val_loss: 4.0516e-04\n",
      "Epoch 11/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 4.0607e-04 - val_loss: 3.9837e-04\n",
      "Epoch 12/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 3.9522e-04 - val_loss: 3.9890e-04\n",
      "Epoch 13/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 3.8436e-04 - val_loss: 4.1645e-04\n",
      "Epoch 14/70\n",
      "39208/39208 [==============================] - 17s 444us/step - loss: 3.7625e-04 - val_loss: 3.8600e-04\n",
      "Epoch 15/70\n",
      "39208/39208 [==============================] - 17s 445us/step - loss: 3.6706e-04 - val_loss: 3.5750e-04\n",
      "Epoch 16/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 3.5505e-04 - val_loss: 3.9632e-04\n",
      "Epoch 17/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 3.5019e-04 - val_loss: 4.1579e-04\n",
      "Epoch 18/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 3.3537e-04 - val_loss: 3.9450e-04\n",
      "Epoch 19/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 3.1983e-04 - val_loss: 3.6277e-04\n",
      "Epoch 20/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 3.0874e-04 - val_loss: 3.5305e-04\n",
      "Epoch 21/70\n",
      "39208/39208 [==============================] - 17s 444us/step - loss: 2.9645e-04 - val_loss: 3.3645e-04\n",
      "Epoch 22/70\n",
      "39208/39208 [==============================] - 17s 443us/step - loss: 2.9064e-04 - val_loss: 3.4023e-04\n",
      "Epoch 23/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 2.8016e-04 - val_loss: 3.3233e-04\n",
      "Epoch 24/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 2.7276e-04 - val_loss: 3.5649e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 2.6420e-04 - val_loss: 3.5305e-04\n",
      "Epoch 26/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 2.5990e-04 - val_loss: 3.4849e-04\n",
      "Epoch 27/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 2.5054e-04 - val_loss: 3.7409e-04\n",
      "Epoch 28/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 2.4525e-04 - val_loss: 3.5580e-04\n",
      "Epoch 29/70\n",
      "39208/39208 [==============================] - 17s 446us/step - loss: 2.4077e-04 - val_loss: 3.6478e-04\n",
      "Epoch 30/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 2.3798e-04 - val_loss: 3.9428e-04\n",
      "Epoch 31/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 2.3226e-04 - val_loss: 3.8251e-04\n",
      "Epoch 32/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 2.2789e-04 - val_loss: 3.9311e-04\n",
      "Epoch 33/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 2.2627e-04 - val_loss: 4.0747e-04\n",
      "Epoch 34/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 2.2266e-04 - val_loss: 3.7799e-04\n",
      "Epoch 35/70\n",
      "39208/39208 [==============================] - 17s 436us/step - loss: 2.1908e-04 - val_loss: 3.7343e-04\n",
      "Epoch 36/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 2.1740e-04 - val_loss: 3.5763e-04\n",
      "Epoch 37/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 2.1461e-04 - val_loss: 3.6110e-04\n",
      "Epoch 38/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 2.1246e-04 - val_loss: 3.3691e-04\n",
      "Epoch 39/70\n",
      "39208/39208 [==============================] - 17s 444us/step - loss: 2.0968e-04 - val_loss: 3.3244e-04\n",
      "Epoch 40/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 2.0797e-04 - val_loss: 3.2351e-04\n",
      "Epoch 41/70\n",
      "39208/39208 [==============================] - 17s 446us/step - loss: 2.0558e-04 - val_loss: 3.3110e-04\n",
      "Epoch 42/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 2.0272e-04 - val_loss: 2.9750e-04\n",
      "Epoch 43/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 1.9924e-04 - val_loss: 2.8449e-04\n",
      "Epoch 44/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 1.9709e-04 - val_loss: 3.1914e-04\n",
      "Epoch 45/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 1.9616e-04 - val_loss: 3.0816e-04\n",
      "Epoch 46/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 1.9223e-04 - val_loss: 3.0464e-04\n",
      "Epoch 47/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 1.8960e-04 - val_loss: 3.6104e-04\n",
      "Epoch 48/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 1.8963e-04 - val_loss: 3.1649e-04\n",
      "Epoch 49/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 1.8474e-04 - val_loss: 2.8606e-04\n",
      "Epoch 50/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 1.8710e-04 - val_loss: 3.1273e-04\n",
      "Epoch 51/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 1.8235e-04 - val_loss: 3.1863e-04\n",
      "Epoch 52/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 1.7959e-04 - val_loss: 2.9638e-04\n",
      "Epoch 53/70\n",
      "39208/39208 [==============================] - 17s 439us/step - loss: 1.8055e-04 - val_loss: 3.1773e-04\n",
      "Epoch 54/70\n",
      "39208/39208 [==============================] - 17s 443us/step - loss: 1.7426e-04 - val_loss: 3.3440e-04\n",
      "Epoch 55/70\n",
      "39208/39208 [==============================] - 17s 444us/step - loss: 1.7429e-04 - val_loss: 3.2724e-04\n",
      "Epoch 56/70\n",
      "39208/39208 [==============================] - 17s 443us/step - loss: 1.7222e-04 - val_loss: 3.0805e-04\n",
      "Epoch 57/70\n",
      "39208/39208 [==============================] - 17s 444us/step - loss: 1.7114e-04 - val_loss: 3.2064e-04\n",
      "Epoch 58/70\n",
      "39208/39208 [==============================] - 17s 441us/step - loss: 1.6947e-04 - val_loss: 3.3015e-04\n",
      "Epoch 59/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 1.6800e-04 - val_loss: 3.3799e-04\n",
      "Epoch 60/70\n",
      "39208/39208 [==============================] - 17s 443us/step - loss: 1.6808e-04 - val_loss: 3.0279e-04\n",
      "Epoch 61/70\n",
      "39208/39208 [==============================] - 17s 442us/step - loss: 1.6471e-04 - val_loss: 3.2696e-04\n",
      "Epoch 62/70\n",
      "39208/39208 [==============================] - 17s 440us/step - loss: 1.6494e-04 - val_loss: 3.2364e-04\n",
      "Epoch 63/70\n",
      "39208/39208 [==============================] - 17s 443us/step - loss: 1.6461e-04 - val_loss: 3.2059e-04\n",
      "Validation RMSE:  0.017904952 \n",
      "\n",
      "Model took 1097.10 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 6 23 4\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 508\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 26s 660us/step - loss: 0.0130 - val_loss: 0.0183\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0113 - val_loss: 0.0165\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0096 - val_loss: 0.0168\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0075 - val_loss: 0.0164\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0062 - val_loss: 0.0162\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 24s 605us/step - loss: 0.0055 - val_loss: 0.0157\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0050 - val_loss: 0.0160\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0047 - val_loss: 0.0156\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0044 - val_loss: 0.0156\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 24s 604us/step - loss: 0.0042 - val_loss: 0.0166\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 24s 604us/step - loss: 0.0040 - val_loss: 0.0161\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0039 - val_loss: 0.0135\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0038 - val_loss: 0.0124\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 24s 603us/step - loss: 0.0037 - val_loss: 0.0138\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0037 - val_loss: 0.0145\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0036 - val_loss: 0.0146\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0035 - val_loss: 0.0142\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0033 - val_loss: 0.0140\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 24s 610us/step - loss: 0.0032 - val_loss: 0.0134\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 24s 604us/step - loss: 0.0031 - val_loss: 0.0148\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0030 - val_loss: 0.0139\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 24s 607us/step - loss: 0.0029 - val_loss: 0.0150\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 24s 603us/step - loss: 0.0029 - val_loss: 0.0157\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0029 - val_loss: 0.0168\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0028 - val_loss: 0.0152\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0028 - val_loss: 0.0174\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0027 - val_loss: 0.0162\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 24s 603us/step - loss: 0.0027 - val_loss: 0.0159\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0027 - val_loss: 0.0145\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0026 - val_loss: 0.0136\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0026 - val_loss: 0.0128\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0026 - val_loss: 0.0135\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 24s 597us/step - loss: 0.0025 - val_loss: 0.0134\n",
      "Validation RMSE:  0.1157202 \n",
      "\n",
      "Model took 800.55 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 0.0049 - val_loss: 0.0025\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 0.0012 - val_loss: 9.7497e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 7.0296e-04 - val_loss: 0.0011\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 5.9982e-04 - val_loss: 8.9215e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 5.3603e-04 - val_loss: 7.1459e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 4.9452e-04 - val_loss: 5.1905e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 4.6189e-04 - val_loss: 5.3801e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 4.3310e-04 - val_loss: 5.3329e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 4.0784e-04 - val_loss: 5.3526e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 3.8537e-04 - val_loss: 5.7973e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.6418e-04 - val_loss: 6.0013e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.4303e-04 - val_loss: 4.8577e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 3.2564e-04 - val_loss: 4.6411e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 3.1284e-04 - val_loss: 3.5013e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.0041e-04 - val_loss: 3.0051e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.8921e-04 - val_loss: 3.3912e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.7992e-04 - val_loss: 3.3150e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.6926e-04 - val_loss: 3.2732e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.6219e-04 - val_loss: 3.1635e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.5655e-04 - val_loss: 3.1107e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.4996e-04 - val_loss: 3.1416e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.4273e-04 - val_loss: 2.9400e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.3568e-04 - val_loss: 2.6415e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.3236e-04 - val_loss: 2.4961e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.2765e-04 - val_loss: 2.2688e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.2401e-04 - val_loss: 2.1513e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.2114e-04 - val_loss: 2.0815e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.1735e-04 - val_loss: 2.3819e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.1412e-04 - val_loss: 2.0470e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.1285e-04 - val_loss: 2.1451e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.0976e-04 - val_loss: 2.0605e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.0578e-04 - val_loss: 2.6479e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.0435e-04 - val_loss: 2.5755e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.0099e-04 - val_loss: 2.4825e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.0034e-04 - val_loss: 1.9267e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.9563e-04 - val_loss: 1.9809e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.9332e-04 - val_loss: 2.0705e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.8818e-04 - val_loss: 2.1757e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.8251e-04 - val_loss: 2.4177e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.7645e-04 - val_loss: 2.6015e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.6943e-04 - val_loss: 2.3808e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.6667e-04 - val_loss: 3.4587e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.6459e-04 - val_loss: 2.9157e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6225e-04 - val_loss: 3.4098e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.6053e-04 - val_loss: 2.3755e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5722e-04 - val_loss: 2.2872e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 1.5511e-04 - val_loss: 2.5853e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.5365e-04 - val_loss: 2.3501e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.5039e-04 - val_loss: 3.0427e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.4873e-04 - val_loss: 4.0104e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.4659e-04 - val_loss: 3.3915e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4497e-04 - val_loss: 3.3218e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.4382e-04 - val_loss: 3.1040e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 1.4261e-04 - val_loss: 2.6355e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4168e-04 - val_loss: 3.2739e-04\n",
      "Validation RMSE:  0.018094013 \n",
      "\n",
      "Model took 960.73 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 0.0044 - val_loss: 0.0024\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 7.2246e-04 - val_loss: 0.0013\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 6.2371e-04 - val_loss: 0.0010\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 5.6477e-04 - val_loss: 4.8561e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 5.0973e-04 - val_loss: 3.4440e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 4.8110e-04 - val_loss: 4.1269e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 4.5567e-04 - val_loss: 4.0954e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 4.3291e-04 - val_loss: 4.2558e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 4.1385e-04 - val_loss: 4.3390e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.9451e-04 - val_loss: 4.3583e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.7077e-04 - val_loss: 5.7865e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.5471e-04 - val_loss: 5.8743e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.3668e-04 - val_loss: 5.7004e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 3.2639e-04 - val_loss: 3.8941e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.1588e-04 - val_loss: 4.0927e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.0607e-04 - val_loss: 3.8069e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.9729e-04 - val_loss: 3.3962e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.8849e-04 - val_loss: 3.2327e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.8207e-04 - val_loss: 2.9933e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.7619e-04 - val_loss: 2.9012e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.6569e-04 - val_loss: 2.8701e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.5996e-04 - val_loss: 2.8805e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.5543e-04 - val_loss: 2.8178e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.5065e-04 - val_loss: 2.9174e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.4620e-04 - val_loss: 3.0722e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.4228e-04 - val_loss: 3.2896e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.3696e-04 - val_loss: 3.0532e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.3713e-04 - val_loss: 2.0331e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.3233e-04 - val_loss: 2.0404e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.2984e-04 - val_loss: 2.0913e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.2533e-04 - val_loss: 2.4678e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.2206e-04 - val_loss: 2.8335e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.2080e-04 - val_loss: 2.5659e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.1685e-04 - val_loss: 2.5263e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.1275e-04 - val_loss: 2.3565e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.1072e-04 - val_loss: 1.9934e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.0688e-04 - val_loss: 1.5185e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.9754e-04 - val_loss: 2.0512e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.9342e-04 - val_loss: 1.4749e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.8417e-04 - val_loss: 1.3400e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.7803e-04 - val_loss: 1.5417e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.7605e-04 - val_loss: 1.4197e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.7185e-04 - val_loss: 1.5287e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.6914e-04 - val_loss: 1.3554e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6640e-04 - val_loss: 1.4102e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6524e-04 - val_loss: 1.4540e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.6231e-04 - val_loss: 1.5371e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.5917e-04 - val_loss: 1.5460e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5657e-04 - val_loss: 1.4151e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.5404e-04 - val_loss: 1.3590e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.5207e-04 - val_loss: 1.3366e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4898e-04 - val_loss: 1.3974e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.4843e-04 - val_loss: 1.2791e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4664e-04 - val_loss: 1.3089e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4449e-04 - val_loss: 1.4137e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4348e-04 - val_loss: 1.5617e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4352e-04 - val_loss: 1.2949e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4202e-04 - val_loss: 1.2359e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.3998e-04 - val_loss: 1.2973e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3808e-04 - val_loss: 1.2795e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.3697e-04 - val_loss: 1.3905e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.3740e-04 - val_loss: 1.3463e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.3593e-04 - val_loss: 1.3839e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3576e-04 - val_loss: 1.2967e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.3504e-04 - val_loss: 1.3094e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.3354e-04 - val_loss: 1.3575e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3367e-04 - val_loss: 1.3028e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.3236e-04 - val_loss: 1.3075e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3150e-04 - val_loss: 1.3273e-04\n",
      "Validation RMSE:  0.011520946 \n",
      "\n",
      "Model took 1223.23 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 0.0039 - val_loss: 0.0028\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 7.0072e-04 - val_loss: 0.0010\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 5.7952e-04 - val_loss: 7.7516e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 5.1257e-04 - val_loss: 5.7855e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 4.6853e-04 - val_loss: 4.3403e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 4.3698e-04 - val_loss: 3.6462e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 4.1011e-04 - val_loss: 3.4542e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.8702e-04 - val_loss: 3.3985e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.6205e-04 - val_loss: 3.4078e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.4901e-04 - val_loss: 3.4295e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.3475e-04 - val_loss: 3.6774e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.2198e-04 - val_loss: 4.0851e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.1247e-04 - val_loss: 3.6331e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.0194e-04 - val_loss: 3.4694e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.8858e-04 - val_loss: 3.4692e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.8334e-04 - val_loss: 3.2176e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.7160e-04 - val_loss: 3.1526e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.6233e-04 - val_loss: 3.2549e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.5343e-04 - val_loss: 4.5675e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.4843e-04 - val_loss: 3.2692e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.3859e-04 - val_loss: 3.9051e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.3809e-04 - val_loss: 2.8944e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.3246e-04 - val_loss: 2.7184e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.2877e-04 - val_loss: 2.8233e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.2342e-04 - val_loss: 3.8431e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.2433e-04 - val_loss: 2.6719e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.1865e-04 - val_loss: 2.8341e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.1924e-04 - val_loss: 2.4185e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 439us/step - loss: 2.1494e-04 - val_loss: 2.2455e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.1411e-04 - val_loss: 2.3823e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.1038e-04 - val_loss: 2.6378e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.0850e-04 - val_loss: 2.2710e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.0679e-04 - val_loss: 2.2723e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.0299e-04 - val_loss: 2.3728e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.0029e-04 - val_loss: 2.1964e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.9413e-04 - val_loss: 2.0632e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.8994e-04 - val_loss: 2.3339e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.8291e-04 - val_loss: 2.0148e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.8001e-04 - val_loss: 2.1243e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7670e-04 - val_loss: 1.8702e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.7492e-04 - val_loss: 1.8442e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.7202e-04 - val_loss: 1.8681e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6861e-04 - val_loss: 1.8956e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.6736e-04 - val_loss: 1.8819e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.6598e-04 - val_loss: 1.8914e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.6446e-04 - val_loss: 1.9394e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6122e-04 - val_loss: 2.2962e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.6161e-04 - val_loss: 2.1321e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.5993e-04 - val_loss: 1.9352e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.5840e-04 - val_loss: 1.8424e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5548e-04 - val_loss: 1.7247e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.5256e-04 - val_loss: 2.0934e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.5208e-04 - val_loss: 1.6904e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4959e-04 - val_loss: 1.5931e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.4609e-04 - val_loss: 1.4655e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4705e-04 - val_loss: 1.4557e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4645e-04 - val_loss: 1.4465e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.4464e-04 - val_loss: 1.3862e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4241e-04 - val_loss: 1.3503e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.4078e-04 - val_loss: 1.3136e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.3992e-04 - val_loss: 1.4251e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3785e-04 - val_loss: 1.4116e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.3399e-04 - val_loss: 1.2917e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.3299e-04 - val_loss: 1.3595e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3246e-04 - val_loss: 1.3762e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3152e-04 - val_loss: 1.5410e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.2991e-04 - val_loss: 1.6306e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.2988e-04 - val_loss: 1.5402e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.2722e-04 - val_loss: 1.4851e-04\n",
      "Validation RMSE:  0.012186638 \n",
      "\n",
      "Model took 1222.05 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 0.0031 - val_loss: 0.0020\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 9.5718e-04 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 6.9836e-04 - val_loss: 0.0011\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 6.0909e-04 - val_loss: 8.0312e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 5.4633e-04 - val_loss: 6.6770e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 4.9953e-04 - val_loss: 6.5574e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 4.6525e-04 - val_loss: 3.2332e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 4.3016e-04 - val_loss: 5.8221e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 4.0469e-04 - val_loss: 4.6686e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.8378e-04 - val_loss: 4.9136e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 3.6628e-04 - val_loss: 5.8421e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 3.5238e-04 - val_loss: 4.6536e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 438us/step - loss: 3.4143e-04 - val_loss: 2.9942e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.2821e-04 - val_loss: 3.0708e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.1749e-04 - val_loss: 3.0924e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.0354e-04 - val_loss: 2.9428e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.9265e-04 - val_loss: 2.5241e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.8268e-04 - val_loss: 3.8302e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.7776e-04 - val_loss: 3.2518e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.7297e-04 - val_loss: 2.9397e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.6553e-04 - val_loss: 2.6133e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.6204e-04 - val_loss: 2.5376e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.5749e-04 - val_loss: 2.4080e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.5241e-04 - val_loss: 2.3485e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.4740e-04 - val_loss: 2.2369e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.4539e-04 - val_loss: 2.2531e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.4205e-04 - val_loss: 2.2528e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.3875e-04 - val_loss: 2.1280e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.3629e-04 - val_loss: 2.2187e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.3404e-04 - val_loss: 2.2537e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.3042e-04 - val_loss: 2.6669e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.2860e-04 - val_loss: 2.7701e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.2561e-04 - val_loss: 2.4783e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 2.2286e-04 - val_loss: 2.0063e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.1956e-04 - val_loss: 1.9785e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.1710e-04 - val_loss: 2.0171e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.1509e-04 - val_loss: 2.1032e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 2.1118e-04 - val_loss: 2.0478e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.0797e-04 - val_loss: 1.9555e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.0332e-04 - val_loss: 1.9919e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.9823e-04 - val_loss: 2.0034e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.9271e-04 - val_loss: 2.0982e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.8946e-04 - val_loss: 1.7981e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.8588e-04 - val_loss: 1.7507e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.8318e-04 - val_loss: 1.7984e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.8018e-04 - val_loss: 1.7360e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7679e-04 - val_loss: 1.6132e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.7399e-04 - val_loss: 1.6224e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.7035e-04 - val_loss: 1.6052e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.6760e-04 - val_loss: 1.5539e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.6681e-04 - val_loss: 1.5835e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.6355e-04 - val_loss: 1.6381e-04\n",
      "Epoch 53/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6282e-04 - val_loss: 1.4278e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.6147e-04 - val_loss: 1.4436e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.5949e-04 - val_loss: 1.4188e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.5696e-04 - val_loss: 1.3182e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.5597e-04 - val_loss: 1.4111e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5471e-04 - val_loss: 1.3168e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5259e-04 - val_loss: 1.3262e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.5075e-04 - val_loss: 1.3333e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.4995e-04 - val_loss: 1.2711e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.4904e-04 - val_loss: 1.3213e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4730e-04 - val_loss: 1.4119e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4544e-04 - val_loss: 1.3649e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.4352e-04 - val_loss: 1.2456e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.4300e-04 - val_loss: 1.2163e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4189e-04 - val_loss: 1.2519e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4046e-04 - val_loss: 1.2647e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 17s 440us/step - loss: 1.3950e-04 - val_loss: 1.2706e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3737e-04 - val_loss: 1.3509e-04\n",
      "Validation RMSE:  0.011622864 \n",
      "\n",
      "Model took 1221.57 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 22 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 1, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1, 1, 1]\n",
      "Lags length:\n",
      " 66\n",
      "Length Used:\n",
      " 22\n",
      "New Dataset shape after Lags: (48978, 67)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  22\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 66)\n",
      "Train X shape after np.reshape (39182, 22, 3)\n",
      "Test X shape after np.reshape (9796, 22, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 0.0054 - val_loss: 8.5897e-04\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 7.1470e-04 - val_loss: 2.8845e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 5.3093e-04 - val_loss: 4.6201e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 4.6369e-04 - val_loss: 5.2039e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 4.0537e-04 - val_loss: 4.5509e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.6533e-04 - val_loss: 3.0821e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 3.3994e-04 - val_loss: 2.6048e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.2391e-04 - val_loss: 2.5179e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.1176e-04 - val_loss: 2.2824e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.9837e-04 - val_loss: 2.3635e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.9013e-04 - val_loss: 2.1735e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.8239e-04 - val_loss: 2.1020e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.7290e-04 - val_loss: 2.0368e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.6266e-04 - val_loss: 2.1357e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.5454e-04 - val_loss: 2.0235e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.4650e-04 - val_loss: 1.9722e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.3882e-04 - val_loss: 1.9817e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.3258e-04 - val_loss: 1.9046e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.2868e-04 - val_loss: 2.0256e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.2448e-04 - val_loss: 2.0511e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.1632e-04 - val_loss: 2.0168e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.1192e-04 - val_loss: 2.2265e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.0813e-04 - val_loss: 2.1851e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.0447e-04 - val_loss: 2.1801e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.0095e-04 - val_loss: 2.1010e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.9746e-04 - val_loss: 2.0405e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.9389e-04 - val_loss: 1.9775e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.9090e-04 - val_loss: 1.8264e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.8583e-04 - val_loss: 1.8730e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.8232e-04 - val_loss: 1.9130e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.7862e-04 - val_loss: 1.9558e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.7628e-04 - val_loss: 1.9605e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.7543e-04 - val_loss: 1.9102e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.7274e-04 - val_loss: 1.7619e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.7117e-04 - val_loss: 1.7910e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.6976e-04 - val_loss: 1.8197e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.6762e-04 - val_loss: 1.7818e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.6523e-04 - val_loss: 1.7108e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.6350e-04 - val_loss: 1.7299e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.6162e-04 - val_loss: 1.6592e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.6015e-04 - val_loss: 1.6418e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.5627e-04 - val_loss: 1.6038e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5518e-04 - val_loss: 1.5388e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.5255e-04 - val_loss: 1.6027e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.5080e-04 - val_loss: 1.5107e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.4869e-04 - val_loss: 1.4909e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.4574e-04 - val_loss: 1.7885e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.4467e-04 - val_loss: 1.4822e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4379e-04 - val_loss: 1.5477e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.4101e-04 - val_loss: 1.4918e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.4029e-04 - val_loss: 1.5231e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3965e-04 - val_loss: 1.3943e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3781e-04 - val_loss: 1.4339e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3570e-04 - val_loss: 1.4737e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3497e-04 - val_loss: 1.4605e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3392e-04 - val_loss: 1.4582e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3238e-04 - val_loss: 1.4641e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3233e-04 - val_loss: 1.4382e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2988e-04 - val_loss: 1.3977e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2734e-04 - val_loss: 1.3429e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2347e-04 - val_loss: 1.4212e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2471e-04 - val_loss: 1.3774e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.2269e-04 - val_loss: 1.3100e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2309e-04 - val_loss: 1.3378e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2113e-04 - val_loss: 1.3945e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2074e-04 - val_loss: 1.2808e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1954e-04 - val_loss: 1.2863e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1914e-04 - val_loss: 1.2252e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1601e-04 - val_loss: 1.3847e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1793e-04 - val_loss: 1.2942e-04\n",
      "Validation RMSE:  0.011376237 \n",
      "\n",
      "Model took 1339.06 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 1\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 4 23 4\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 508\n",
      "3rd Window Start: 972\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Only Lag is\n",
      " [ 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      " 33 34 35 36]\n",
      "Lags length:\n",
      " 28\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49964, 29)\n",
      "Training and test data length 39971 9993\n",
      "trainX.shape[0]:  39971\n",
      "window_length:  28\n",
      "no_wins:  1\n",
      "trainX shape:\n",
      " (39971, 28)\n",
      "Train X shape after np.reshape (39971, 28, 1)\n",
      "Test X shape after np.reshape (9993, 28, 1)\n",
      "Train Y Shape (39971,)\n",
      "Test Y Shape (9993,)\n",
      "Train on 39971 samples, validate on 9993 samples\n",
      "Epoch 1/70\n",
      "39971/39971 [==============================] - 27s 674us/step - loss: 0.0144 - val_loss: 0.0178\n",
      "Epoch 2/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0103 - val_loss: 0.0192\n",
      "Epoch 3/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0095 - val_loss: 0.0151\n",
      "Epoch 4/70\n",
      "39971/39971 [==============================] - 24s 603us/step - loss: 0.0076 - val_loss: 0.0165\n",
      "Epoch 5/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0064 - val_loss: 0.0173\n",
      "Epoch 6/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0056 - val_loss: 0.0191\n",
      "Epoch 7/70\n",
      "39971/39971 [==============================] - 24s 598us/step - loss: 0.0050 - val_loss: 0.0181\n",
      "Epoch 8/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0049 - val_loss: 0.0173\n",
      "Epoch 9/70\n",
      "39971/39971 [==============================] - 24s 604us/step - loss: 0.0046 - val_loss: 0.0139\n",
      "Epoch 10/70\n",
      "39971/39971 [==============================] - 24s 604us/step - loss: 0.0045 - val_loss: 0.0126\n",
      "Epoch 11/70\n",
      "39971/39971 [==============================] - 24s 605us/step - loss: 0.0043 - val_loss: 0.0132\n",
      "Epoch 12/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0041 - val_loss: 0.0135\n",
      "Epoch 13/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0039 - val_loss: 0.0125\n",
      "Epoch 14/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0038 - val_loss: 0.0117\n",
      "Epoch 15/70\n",
      "39971/39971 [==============================] - 24s 603us/step - loss: 0.0037 - val_loss: 0.0118\n",
      "Epoch 16/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0036 - val_loss: 0.0126\n",
      "Epoch 17/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0035 - val_loss: 0.0105\n",
      "Epoch 18/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0034 - val_loss: 0.0115\n",
      "Epoch 19/70\n",
      "39971/39971 [==============================] - 24s 608us/step - loss: 0.0033 - val_loss: 0.0116\n",
      "Epoch 20/70\n",
      "39971/39971 [==============================] - 24s 606us/step - loss: 0.0031 - val_loss: 0.0140\n",
      "Epoch 21/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0030 - val_loss: 0.0122\n",
      "Epoch 22/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0030 - val_loss: 0.0152\n",
      "Epoch 23/70\n",
      "39971/39971 [==============================] - 24s 605us/step - loss: 0.0029 - val_loss: 0.0156\n",
      "Epoch 24/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0028 - val_loss: 0.0161\n",
      "Epoch 25/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0028 - val_loss: 0.0164\n",
      "Epoch 26/70\n",
      "39971/39971 [==============================] - 24s 600us/step - loss: 0.0027 - val_loss: 0.0180\n",
      "Epoch 27/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0027 - val_loss: 0.0171\n",
      "Epoch 28/70\n",
      "39971/39971 [==============================] - 24s 603us/step - loss: 0.0027 - val_loss: 0.0166\n",
      "Epoch 29/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0026 - val_loss: 0.0172\n",
      "Epoch 30/70\n",
      "39971/39971 [==============================] - 24s 602us/step - loss: 0.0026 - val_loss: 0.0168\n",
      "Epoch 31/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0026 - val_loss: 0.0170\n",
      "Epoch 32/70\n",
      "39971/39971 [==============================] - 24s 595us/step - loss: 0.0025 - val_loss: 0.0166\n",
      "Epoch 33/70\n",
      "39971/39971 [==============================] - 24s 604us/step - loss: 0.0025 - val_loss: 0.0161\n",
      "Epoch 34/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0025 - val_loss: 0.0165\n",
      "Epoch 35/70\n",
      "39971/39971 [==============================] - 24s 601us/step - loss: 0.0024 - val_loss: 0.0168\n",
      "Epoch 36/70\n",
      "39971/39971 [==============================] - 24s 599us/step - loss: 0.0024 - val_loss: 0.0160\n",
      "Epoch 37/70\n",
      "39971/39971 [==============================] - 24s 605us/step - loss: 0.0024 - val_loss: 0.0139\n",
      "Validation RMSE:  0.1179241 \n",
      "\n",
      "Model took 899.02 seconds to train\n",
      "6  \t8     \t0.0339738\t0.0414998\t0.0113762\t0.117924\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 6 19 1\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 0.0036 - val_loss: 0.0020\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 9.8741e-04 - val_loss: 0.0010\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 6.8443e-04 - val_loss: 7.6882e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 5.7954e-04 - val_loss: 5.9811e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 5.2096e-04 - val_loss: 5.5922e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 4.7967e-04 - val_loss: 5.4118e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 4.5169e-04 - val_loss: 5.0508e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 4.2481e-04 - val_loss: 5.6116e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 4.0241e-04 - val_loss: 5.0633e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 3.7865e-04 - val_loss: 5.1589e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.6205e-04 - val_loss: 4.7587e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 3.4209e-04 - val_loss: 4.5292e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.2675e-04 - val_loss: 3.9461e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.0837e-04 - val_loss: 3.8517e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.9405e-04 - val_loss: 3.7051e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.8427e-04 - val_loss: 3.3295e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.7199e-04 - val_loss: 3.2742e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.6150e-04 - val_loss: 3.3543e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.5186e-04 - val_loss: 3.1687e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.4613e-04 - val_loss: 3.0674e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.4276e-04 - val_loss: 2.8275e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.3502e-04 - val_loss: 2.7698e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.2863e-04 - val_loss: 2.5419e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.2183e-04 - val_loss: 2.4134e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.1628e-04 - val_loss: 2.2526e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.1061e-04 - val_loss: 2.1512e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.0388e-04 - val_loss: 2.0069e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.9811e-04 - val_loss: 2.2096e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.9536e-04 - val_loss: 1.8940e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.8832e-04 - val_loss: 2.2549e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.8518e-04 - val_loss: 1.6521e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.7966e-04 - val_loss: 1.6378e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7715e-04 - val_loss: 1.7520e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7310e-04 - val_loss: 1.7060e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7113e-04 - val_loss: 1.7500e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6775e-04 - val_loss: 1.6850e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6442e-04 - val_loss: 1.6954e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.6042e-04 - val_loss: 1.7317e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.5660e-04 - val_loss: 1.7045e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.5278e-04 - val_loss: 1.7493e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4936e-04 - val_loss: 1.7661e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.4637e-04 - val_loss: 1.6557e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4529e-04 - val_loss: 1.7233e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.4361e-04 - val_loss: 1.7156e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.4066e-04 - val_loss: 2.4536e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4294e-04 - val_loss: 1.6747e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4012e-04 - val_loss: 1.7033e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3883e-04 - val_loss: 1.6285e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3752e-04 - val_loss: 1.7206e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3729e-04 - val_loss: 1.7201e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3777e-04 - val_loss: 1.6970e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3560e-04 - val_loss: 1.6703e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3515e-04 - val_loss: 1.6993e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3488e-04 - val_loss: 1.5945e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3532e-04 - val_loss: 1.6342e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3355e-04 - val_loss: 1.6518e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.3450e-04 - val_loss: 1.6402e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3331e-04 - val_loss: 1.7653e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3277e-04 - val_loss: 1.6462e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3199e-04 - val_loss: 1.6431e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3152e-04 - val_loss: 1.6869e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3163e-04 - val_loss: 1.6791e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3001e-04 - val_loss: 1.6638e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3043e-04 - val_loss: 1.7222e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3027e-04 - val_loss: 1.7696e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2927e-04 - val_loss: 1.7174e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2866e-04 - val_loss: 1.8406e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2889e-04 - val_loss: 1.7998e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2737e-04 - val_loss: 1.6860e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2736e-04 - val_loss: 1.8586e-04\n",
      "Validation RMSE:  0.013633227 \n",
      "\n",
      "Model took 1229.28 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 12\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 0.0058 - val_loss: 0.0028\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 7.0288e-04 - val_loss: 4.2138e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 5.9471e-04 - val_loss: 4.9654e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 5.3020e-04 - val_loss: 4.9980e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 4.8216e-04 - val_loss: 3.9755e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 4.2915e-04 - val_loss: 3.2928e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 3.9134e-04 - val_loss: 3.0840e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.5648e-04 - val_loss: 3.1963e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.2690e-04 - val_loss: 3.3660e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.1187e-04 - val_loss: 3.0551e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.9575e-04 - val_loss: 2.7944e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.7486e-04 - val_loss: 2.6906e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.5202e-04 - val_loss: 2.9634e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.4209e-04 - val_loss: 2.7480e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.3112e-04 - val_loss: 2.6127e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.2009e-04 - val_loss: 2.6739e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.1942e-04 - val_loss: 2.4663e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.1109e-04 - val_loss: 2.4719e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.1010e-04 - val_loss: 2.3161e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 2.0507e-04 - val_loss: 2.2092e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.0102e-04 - val_loss: 2.2028e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.9489e-04 - val_loss: 2.2503e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.8635e-04 - val_loss: 2.0046e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.7963e-04 - val_loss: 1.8265e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.7555e-04 - val_loss: 1.6936e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.7176e-04 - val_loss: 1.6077e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7048e-04 - val_loss: 1.5092e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6681e-04 - val_loss: 1.5694e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.6489e-04 - val_loss: 1.5074e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6174e-04 - val_loss: 1.4880e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.6021e-04 - val_loss: 1.4390e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.5763e-04 - val_loss: 1.4498e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.5615e-04 - val_loss: 1.5435e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5380e-04 - val_loss: 1.4251e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5308e-04 - val_loss: 1.4912e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.5169e-04 - val_loss: 1.4268e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4978e-04 - val_loss: 1.4117e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.4870e-04 - val_loss: 1.3924e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.4837e-04 - val_loss: 1.4552e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.4661e-04 - val_loss: 1.4595e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.4635e-04 - val_loss: 1.4966e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.4355e-04 - val_loss: 1.4197e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4226e-04 - val_loss: 1.4918e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3965e-04 - val_loss: 1.4384e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3642e-04 - val_loss: 1.4049e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3652e-04 - val_loss: 1.3347e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3501e-04 - val_loss: 1.3372e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3339e-04 - val_loss: 1.3090e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.3212e-04 - val_loss: 1.2919e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3061e-04 - val_loss: 1.3121e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.2943e-04 - val_loss: 1.3023e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.2833e-04 - val_loss: 1.2812e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.2639e-04 - val_loss: 1.2983e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.2502e-04 - val_loss: 1.2926e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.2436e-04 - val_loss: 1.3308e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2376e-04 - val_loss: 1.3751e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.2264e-04 - val_loss: 1.3797e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2183e-04 - val_loss: 1.6761e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2206e-04 - val_loss: 1.4593e-04\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.2105e-04 - val_loss: 1.5303e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.1962e-04 - val_loss: 1.4455e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.1894e-04 - val_loss: 1.3902e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.1860e-04 - val_loss: 1.4107e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.1780e-04 - val_loss: 1.4705e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.1697e-04 - val_loss: 1.4080e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.1652e-04 - val_loss: 1.3690e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.1635e-04 - val_loss: 1.3200e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.1556e-04 - val_loss: 1.3872e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.1451e-04 - val_loss: 1.4206e-04\n",
      "Validation RMSE:  0.011918742 \n",
      "\n",
      "Model took 1227.57 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 22 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 1, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1, 1, 1]\n",
      "Lags length:\n",
      " 66\n",
      "Length Used:\n",
      " 22\n",
      "New Dataset shape after Lags: (48978, 67)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  22\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 66)\n",
      "Train X shape after np.reshape (39182, 22, 3)\n",
      "Test X shape after np.reshape (9796, 22, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 22s 565us/step - loss: 0.0047 - val_loss: 8.8425e-04\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 7.6420e-04 - val_loss: 6.1554e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 5.4846e-04 - val_loss: 7.0492e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 4.7885e-04 - val_loss: 5.5109e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 4.3291e-04 - val_loss: 4.4822e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.9215e-04 - val_loss: 4.2532e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.6754e-04 - val_loss: 3.8962e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.4500e-04 - val_loss: 3.3176e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.3131e-04 - val_loss: 3.1278e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.2430e-04 - val_loss: 2.6162e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 3.1458e-04 - val_loss: 2.5033e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.0731e-04 - val_loss: 2.5119e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.9878e-04 - val_loss: 2.4804e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.9003e-04 - val_loss: 2.4898e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.8150e-04 - val_loss: 2.3499e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.7221e-04 - val_loss: 2.3764e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.6359e-04 - val_loss: 2.4387e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.5693e-04 - val_loss: 2.5690e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.4711e-04 - val_loss: 2.5927e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.4256e-04 - val_loss: 2.5640e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.3275e-04 - val_loss: 2.6755e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.2535e-04 - val_loss: 2.5817e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.1685e-04 - val_loss: 2.6202e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.0826e-04 - val_loss: 2.5835e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.0237e-04 - val_loss: 2.4368e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.9692e-04 - val_loss: 2.3127e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.9390e-04 - val_loss: 2.2720e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.9224e-04 - val_loss: 2.3057e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.8673e-04 - val_loss: 2.5982e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.8589e-04 - val_loss: 2.1231e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.8074e-04 - val_loss: 2.4128e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.7831e-04 - val_loss: 2.3609e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.7573e-04 - val_loss: 2.0617e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.7160e-04 - val_loss: 2.0661e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.7063e-04 - val_loss: 1.8886e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.6792e-04 - val_loss: 1.7621e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.6490e-04 - val_loss: 1.6785e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.6204e-04 - val_loss: 1.6367e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5858e-04 - val_loss: 1.7444e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.5564e-04 - val_loss: 1.9159e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.5736e-04 - val_loss: 1.7520e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5542e-04 - val_loss: 1.6098e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.5423e-04 - val_loss: 1.5784e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5296e-04 - val_loss: 1.4920e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4996e-04 - val_loss: 1.4149e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.4852e-04 - val_loss: 1.3496e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4584e-04 - val_loss: 1.3494e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.4410e-04 - val_loss: 1.3857e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4139e-04 - val_loss: 1.3196e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3836e-04 - val_loss: 1.3673e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3937e-04 - val_loss: 1.3274e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3651e-04 - val_loss: 1.4320e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3373e-04 - val_loss: 1.3881e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3153e-04 - val_loss: 1.4220e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3032e-04 - val_loss: 1.5436e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2566e-04 - val_loss: 1.6794e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2704e-04 - val_loss: 1.6700e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2422e-04 - val_loss: 1.5936e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2404e-04 - val_loss: 1.5845e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2090e-04 - val_loss: 1.6383e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2039e-04 - val_loss: 1.3620e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1872e-04 - val_loss: 1.6867e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1692e-04 - val_loss: 1.7452e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.1623e-04 - val_loss: 1.9160e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1671e-04 - val_loss: 1.7433e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1597e-04 - val_loss: 1.8061e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1398e-04 - val_loss: 1.7863e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1341e-04 - val_loss: 1.6211e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1302e-04 - val_loss: 1.5775e-04\n",
      "Validation RMSE:  0.012559793 \n",
      "\n",
      "Model took 1321.01 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 21 19 24\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 494\n",
      "4th Window Start: 1132\n",
      "Activation Function is: elu\n",
      "Optimizer is: sgd\n",
      "Three Lags are\n",
      " [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 1, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 1, 1]\n",
      "Lags length:\n",
      " 63\n",
      "Length Used:\n",
      " 21\n",
      "New Dataset shape after Lags: (49488, 64)\n",
      "Training and test data length 39590 9898\n",
      "trainX.shape[0]:  39590\n",
      "window_length:  21\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39590, 63)\n",
      "Train X shape after np.reshape (39590, 21, 3)\n",
      "Test X shape after np.reshape (9898, 21, 3)\n",
      "Train Y Shape (39590,)\n",
      "Test Y Shape (9898,)\n",
      "Train on 39590 samples, validate on 9898 samples\n",
      "Epoch 1/70\n",
      "39590/39590 [==============================] - 21s 540us/step - loss: 0.0149 - val_loss: 0.0333\n",
      "Epoch 2/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 0.0112 - val_loss: 0.0384\n",
      "Epoch 3/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 0.0098 - val_loss: 0.0413\n",
      "Epoch 4/70\n",
      "39590/39590 [==============================] - 18s 463us/step - loss: 0.0086 - val_loss: 0.0419\n",
      "Epoch 5/70\n",
      "39590/39590 [==============================] - 18s 455us/step - loss: 0.0075 - val_loss: 0.0406\n",
      "Epoch 6/70\n",
      "39590/39590 [==============================] - 18s 456us/step - loss: 0.0065 - val_loss: 0.0377\n",
      "Epoch 7/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 0.0055 - val_loss: 0.0338\n",
      "Epoch 8/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 0.0046 - val_loss: 0.0294\n",
      "Epoch 9/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 0.0038 - val_loss: 0.0250\n",
      "Epoch 10/70\n",
      "39590/39590 [==============================] - 18s 455us/step - loss: 0.0031 - val_loss: 0.0208\n",
      "Epoch 11/70\n",
      "39590/39590 [==============================] - 18s 460us/step - loss: 0.0025 - val_loss: 0.0170\n",
      "Epoch 12/70\n",
      "39590/39590 [==============================] - 18s 461us/step - loss: 0.0020 - val_loss: 0.0137\n",
      "Epoch 13/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 0.0016 - val_loss: 0.0111\n",
      "Epoch 14/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 0.0014 - val_loss: 0.0090\n",
      "Epoch 15/70\n",
      "39590/39590 [==============================] - 18s 460us/step - loss: 0.0012 - val_loss: 0.0073\n",
      "Epoch 16/70\n",
      "39590/39590 [==============================] - 18s 460us/step - loss: 0.0010 - val_loss: 0.0061\n",
      "Epoch 17/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 9.5171e-04 - val_loss: 0.0051\n",
      "Epoch 18/70\n",
      "39590/39590 [==============================] - 18s 463us/step - loss: 8.8574e-04 - val_loss: 0.0044\n",
      "Epoch 19/70\n",
      "39590/39590 [==============================] - 18s 461us/step - loss: 8.3881e-04 - val_loss: 0.0038\n",
      "Epoch 20/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 8.0488e-04 - val_loss: 0.0034\n",
      "Epoch 21/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 7.7991e-04 - val_loss: 0.0031\n",
      "Epoch 22/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 7.6123e-04 - val_loss: 0.0028\n",
      "Epoch 23/70\n",
      "39590/39590 [==============================] - 18s 461us/step - loss: 7.4702e-04 - val_loss: 0.0026\n",
      "Epoch 24/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 7.3607e-04 - val_loss: 0.0024\n",
      "Epoch 25/70\n",
      "39590/39590 [==============================] - 18s 462us/step - loss: 7.2753e-04 - val_loss: 0.0023\n",
      "Epoch 26/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 7.2079e-04 - val_loss: 0.0022\n",
      "Epoch 27/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 7.1544e-04 - val_loss: 0.0021\n",
      "Epoch 28/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 7.1112e-04 - val_loss: 0.0020\n",
      "Epoch 29/70\n",
      "39590/39590 [==============================] - 18s 462us/step - loss: 7.0762e-04 - val_loss: 0.0020\n",
      "Epoch 30/70\n",
      "39590/39590 [==============================] - 18s 465us/step - loss: 7.0474e-04 - val_loss: 0.0019\n",
      "Epoch 31/70\n",
      "39590/39590 [==============================] - 18s 462us/step - loss: 7.0233e-04 - val_loss: 0.0019\n",
      "Epoch 32/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 7.0030e-04 - val_loss: 0.0019\n",
      "Epoch 33/70\n",
      "39590/39590 [==============================] - 18s 461us/step - loss: 6.9856e-04 - val_loss: 0.0018\n",
      "Epoch 34/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 6.9703e-04 - val_loss: 0.0018\n",
      "Epoch 35/70\n",
      "39590/39590 [==============================] - 18s 455us/step - loss: 6.9569e-04 - val_loss: 0.0018\n",
      "Epoch 36/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 6.9447e-04 - val_loss: 0.0017\n",
      "Epoch 37/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 6.9336e-04 - val_loss: 0.0017\n",
      "Epoch 38/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 6.9234e-04 - val_loss: 0.0017\n",
      "Epoch 39/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 6.9137e-04 - val_loss: 0.0017\n",
      "Epoch 40/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 6.9046e-04 - val_loss: 0.0017\n",
      "Epoch 41/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 6.8958e-04 - val_loss: 0.0017\n",
      "Epoch 42/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 6.8873e-04 - val_loss: 0.0017\n",
      "Epoch 43/70\n",
      "39590/39590 [==============================] - 18s 462us/step - loss: 6.8791e-04 - val_loss: 0.0017\n",
      "Epoch 44/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 6.8711e-04 - val_loss: 0.0016\n",
      "Epoch 45/70\n",
      "39590/39590 [==============================] - 18s 456us/step - loss: 6.8633e-04 - val_loss: 0.0016\n",
      "Epoch 46/70\n",
      "39590/39590 [==============================] - 18s 456us/step - loss: 6.8556e-04 - val_loss: 0.0016\n",
      "Epoch 47/70\n",
      "39590/39590 [==============================] - 18s 460us/step - loss: 6.8480e-04 - val_loss: 0.0016\n",
      "Epoch 48/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 6.8405e-04 - val_loss: 0.0016\n",
      "Epoch 49/70\n",
      "39590/39590 [==============================] - 18s 456us/step - loss: 6.8330e-04 - val_loss: 0.0016\n",
      "Epoch 50/70\n",
      "39590/39590 [==============================] - 18s 464us/step - loss: 6.8257e-04 - val_loss: 0.0016\n",
      "Epoch 51/70\n",
      "39590/39590 [==============================] - 18s 460us/step - loss: 6.8184e-04 - val_loss: 0.0016\n",
      "Epoch 52/70\n",
      "39590/39590 [==============================] - 18s 456us/step - loss: 6.8112e-04 - val_loss: 0.0016\n",
      "Epoch 53/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 6.8040e-04 - val_loss: 0.0016\n",
      "Epoch 54/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 6.7969e-04 - val_loss: 0.0016\n",
      "Epoch 55/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 6.7898e-04 - val_loss: 0.0016\n",
      "Epoch 56/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 6.7828e-04 - val_loss: 0.0016\n",
      "Epoch 57/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 6.7758e-04 - val_loss: 0.0016\n",
      "Epoch 58/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 6.7688e-04 - val_loss: 0.0016\n",
      "Epoch 59/70\n",
      "39590/39590 [==============================] - 18s 460us/step - loss: 6.7619e-04 - val_loss: 0.0016\n",
      "Epoch 60/70\n",
      "39590/39590 [==============================] - 18s 460us/step - loss: 6.7551e-04 - val_loss: 0.0016\n",
      "Epoch 61/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 6.7482e-04 - val_loss: 0.0016\n",
      "Epoch 62/70\n",
      "39590/39590 [==============================] - 18s 456us/step - loss: 6.7414e-04 - val_loss: 0.0016\n",
      "Epoch 63/70\n",
      "39590/39590 [==============================] - 18s 456us/step - loss: 6.7347e-04 - val_loss: 0.0016\n",
      "Epoch 64/70\n",
      "39590/39590 [==============================] - 18s 459us/step - loss: 6.7280e-04 - val_loss: 0.0016\n",
      "Epoch 65/70\n",
      "39590/39590 [==============================] - 18s 457us/step - loss: 6.7213e-04 - val_loss: 0.0016\n",
      "Epoch 66/70\n",
      "39590/39590 [==============================] - 18s 460us/step - loss: 6.7147e-04 - val_loss: 0.0016\n",
      "Epoch 67/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 6.7081e-04 - val_loss: 0.0016\n",
      "Epoch 68/70\n",
      "39590/39590 [==============================] - 18s 455us/step - loss: 6.7015e-04 - val_loss: 0.0016\n",
      "Epoch 69/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 6.6950e-04 - val_loss: 0.0016\n",
      "Epoch 70/70\n",
      "39590/39590 [==============================] - 18s 458us/step - loss: 6.6885e-04 - val_loss: 0.0016\n",
      "Validation RMSE:  0.03937472 \n",
      "\n",
      "Model took 1280.47 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 7.1834e-04 - val_loss: 0.0010\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 6.0541e-04 - val_loss: 7.2165e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 5.3833e-04 - val_loss: 5.7911e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 4.9452e-04 - val_loss: 4.6780e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 4.5969e-04 - val_loss: 4.6145e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 4.2628e-04 - val_loss: 4.8869e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.9529e-04 - val_loss: 5.0564e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 3.7445e-04 - val_loss: 5.8036e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.5537e-04 - val_loss: 4.7886e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.3681e-04 - val_loss: 3.7918e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.2306e-04 - val_loss: 3.6705e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 3.0944e-04 - val_loss: 3.3481e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.9635e-04 - val_loss: 3.4723e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.8528e-04 - val_loss: 3.1163e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.7296e-04 - val_loss: 2.8524e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.6374e-04 - val_loss: 2.7505e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.5157e-04 - val_loss: 2.7478e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.4318e-04 - val_loss: 3.5547e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.3908e-04 - val_loss: 3.8741e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.3561e-04 - val_loss: 2.9977e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.3274e-04 - val_loss: 2.8828e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.2603e-04 - val_loss: 3.2633e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.2430e-04 - val_loss: 3.1013e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.1987e-04 - val_loss: 2.8709e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.1828e-04 - val_loss: 2.5871e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.1450e-04 - val_loss: 2.6067e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.1182e-04 - val_loss: 2.2548e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 2.0853e-04 - val_loss: 2.0593e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.0486e-04 - val_loss: 2.0792e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.0045e-04 - val_loss: 2.2387e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.9826e-04 - val_loss: 2.1573e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.9449e-04 - val_loss: 1.7728e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.8780e-04 - val_loss: 1.7399e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.8245e-04 - val_loss: 1.7631e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7733e-04 - val_loss: 1.9184e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7605e-04 - val_loss: 1.7496e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7272e-04 - val_loss: 1.8554e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.7102e-04 - val_loss: 1.7589e-04\n",
      "Epoch 41/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.6832e-04 - val_loss: 1.7631e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6764e-04 - val_loss: 1.5135e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.6589e-04 - val_loss: 1.4850e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6332e-04 - val_loss: 1.4231e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6134e-04 - val_loss: 1.4318e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.5802e-04 - val_loss: 1.5060e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.5862e-04 - val_loss: 1.3681e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.5514e-04 - val_loss: 1.4765e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.5478e-04 - val_loss: 1.3947e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.5121e-04 - val_loss: 1.3313e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.5013e-04 - val_loss: 1.1292e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4921e-04 - val_loss: 1.2319e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4760e-04 - val_loss: 1.2986e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4627e-04 - val_loss: 1.1547e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4259e-04 - val_loss: 1.2348e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4227e-04 - val_loss: 1.4928e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3987e-04 - val_loss: 1.8388e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4010e-04 - val_loss: 1.3443e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3744e-04 - val_loss: 1.1681e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3518e-04 - val_loss: 1.5064e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3502e-04 - val_loss: 1.5495e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3248e-04 - val_loss: 1.4936e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3206e-04 - val_loss: 1.4202e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.3070e-04 - val_loss: 1.4656e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2936e-04 - val_loss: 1.5510e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2823e-04 - val_loss: 1.6005e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2792e-04 - val_loss: 1.6199e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.2705e-04 - val_loss: 1.6027e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2589e-04 - val_loss: 1.6177e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.2435e-04 - val_loss: 1.8849e-04\n",
      "Validation RMSE:  0.013729105 \n",
      "\n",
      "Model took 1234.32 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 0.0039 - val_loss: 0.0025\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 7.4022e-04 - val_loss: 0.0012\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 6.3133e-04 - val_loss: 8.9971e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 5.6482e-04 - val_loss: 7.2129e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 5.0589e-04 - val_loss: 6.5765e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 4.6757e-04 - val_loss: 4.8729e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 4.3693e-04 - val_loss: 4.3785e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 4.0971e-04 - val_loss: 4.8332e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.8786e-04 - val_loss: 4.1912e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.7017e-04 - val_loss: 4.9650e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.5211e-04 - val_loss: 4.8477e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.3347e-04 - val_loss: 5.1560e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.2047e-04 - val_loss: 4.6588e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 3.0750e-04 - val_loss: 3.7739e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.9481e-04 - val_loss: 3.3299e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.8337e-04 - val_loss: 2.9105e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.7195e-04 - val_loss: 2.7107e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.6211e-04 - val_loss: 2.4200e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.5418e-04 - val_loss: 2.5340e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.4598e-04 - val_loss: 2.6067e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.4329e-04 - val_loss: 2.3796e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.3791e-04 - val_loss: 2.3220e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.3307e-04 - val_loss: 2.1810e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.2776e-04 - val_loss: 2.0015e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.2341e-04 - val_loss: 2.0219e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.1967e-04 - val_loss: 1.9770e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.1622e-04 - val_loss: 1.9634e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.1293e-04 - val_loss: 1.8985e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.0785e-04 - val_loss: 1.9352e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.0602e-04 - val_loss: 1.8679e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 2.0388e-04 - val_loss: 1.7429e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.9958e-04 - val_loss: 1.9122e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.9789e-04 - val_loss: 1.7509e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.9326e-04 - val_loss: 1.7913e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.8881e-04 - val_loss: 1.6812e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.8241e-04 - val_loss: 1.7731e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.7852e-04 - val_loss: 1.6067e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7378e-04 - val_loss: 1.6198e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.6899e-04 - val_loss: 1.8440e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6570e-04 - val_loss: 2.0922e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6225e-04 - val_loss: 1.8610e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 441us/step - loss: 1.5885e-04 - val_loss: 1.7540e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.5582e-04 - val_loss: 1.6353e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.5351e-04 - val_loss: 1.5758e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.5214e-04 - val_loss: 1.5570e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4906e-04 - val_loss: 1.4813e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.4696e-04 - val_loss: 1.4201e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4498e-04 - val_loss: 1.4019e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4363e-04 - val_loss: 1.5884e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.4430e-04 - val_loss: 1.4305e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4248e-04 - val_loss: 1.3490e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4061e-04 - val_loss: 1.2081e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3897e-04 - val_loss: 1.3804e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3711e-04 - val_loss: 1.3955e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3622e-04 - val_loss: 1.4721e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3471e-04 - val_loss: 1.4800e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3352e-04 - val_loss: 1.5306e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3136e-04 - val_loss: 1.5770e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3147e-04 - val_loss: 1.6276e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3040e-04 - val_loss: 1.6239e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3012e-04 - val_loss: 1.6124e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2838e-04 - val_loss: 1.6960e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.2909e-04 - val_loss: 1.7890e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.2663e-04 - val_loss: 1.6209e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.2648e-04 - val_loss: 1.8909e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2492e-04 - val_loss: 1.7777e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.2337e-04 - val_loss: 1.9119e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2217e-04 - val_loss: 1.8805e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2191e-04 - val_loss: 1.9404e-04\n",
      "Validation RMSE:  0.013929978 \n",
      "\n",
      "Model took 1232.11 seconds to train\n",
      "7  \t6     \t0.015175 \t0.00811882\t0.0113762\t0.0393747\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 0.0080 - val_loss: 0.0038\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 6.4934e-04 - val_loss: 5.8851e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 5.0502e-04 - val_loss: 3.8190e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 4.3706e-04 - val_loss: 3.7152e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.8694e-04 - val_loss: 3.9978e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.5575e-04 - val_loss: 3.6992e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.3791e-04 - val_loss: 3.2247e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 3.2306e-04 - val_loss: 3.0836e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.1067e-04 - val_loss: 2.8089e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.0065e-04 - val_loss: 2.6303e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.9254e-04 - val_loss: 2.4328e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.8343e-04 - val_loss: 2.3614e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.7466e-04 - val_loss: 2.4187e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.6472e-04 - val_loss: 2.3936e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.5649e-04 - val_loss: 2.3874e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.4852e-04 - val_loss: 2.3730e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.4224e-04 - val_loss: 2.6231e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.3614e-04 - val_loss: 2.6613e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.3022e-04 - val_loss: 2.7053e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.2233e-04 - val_loss: 2.4810e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.1347e-04 - val_loss: 2.4081e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.0785e-04 - val_loss: 1.8843e-04\n",
      "Epoch 24/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.0091e-04 - val_loss: 1.8533e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.9232e-04 - val_loss: 1.5299e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.8370e-04 - val_loss: 1.3764e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.7608e-04 - val_loss: 1.3138e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.6852e-04 - val_loss: 1.3821e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.6471e-04 - val_loss: 1.2235e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.5910e-04 - val_loss: 1.2768e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.5471e-04 - val_loss: 1.2265e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.5084e-04 - val_loss: 1.0780e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4793e-04 - val_loss: 1.0505e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4551e-04 - val_loss: 9.9304e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4304e-04 - val_loss: 9.8969e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4174e-04 - val_loss: 9.7425e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3997e-04 - val_loss: 9.5191e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3841e-04 - val_loss: 9.3695e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3672e-04 - val_loss: 9.1216e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3501e-04 - val_loss: 9.0596e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3300e-04 - val_loss: 9.0126e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3118e-04 - val_loss: 9.4536e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3061e-04 - val_loss: 9.2753e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2928e-04 - val_loss: 9.7544e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2792e-04 - val_loss: 9.3325e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.2658e-04 - val_loss: 9.2740e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.2576e-04 - val_loss: 9.2716e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2478e-04 - val_loss: 9.3655e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2278e-04 - val_loss: 9.1212e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.2162e-04 - val_loss: 9.1147e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.2025e-04 - val_loss: 9.1114e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.1929e-04 - val_loss: 9.2305e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.1750e-04 - val_loss: 1.0119e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.1684e-04 - val_loss: 9.8682e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.1650e-04 - val_loss: 9.8073e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.1605e-04 - val_loss: 9.5242e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.1515e-04 - val_loss: 9.4289e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.1434e-04 - val_loss: 9.3770e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.1331e-04 - val_loss: 8.9910e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.1238e-04 - val_loss: 8.8838e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.1245e-04 - val_loss: 8.8009e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.1189e-04 - val_loss: 8.7163e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.1106e-04 - val_loss: 8.8048e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.1085e-04 - val_loss: 8.5729e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.1018e-04 - val_loss: 8.5297e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1031e-04 - val_loss: 8.3337e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.0922e-04 - val_loss: 8.0428e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.0811e-04 - val_loss: 7.9984e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.0752e-04 - val_loss: 7.8959e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.0647e-04 - val_loss: 7.9218e-05\n",
      "Validation RMSE:  0.008900441 \n",
      "\n",
      "Model took 1233.53 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 6 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 446, 447, 448, 449, 450, 451, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 0.0071 - val_loss: 0.0028\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 7.2769e-04 - val_loss: 7.3631e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 6.0967e-04 - val_loss: 4.7859e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 5.3718e-04 - val_loss: 4.3459e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 4.9354e-04 - val_loss: 4.4536e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 4.6238e-04 - val_loss: 4.0336e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 4.3470e-04 - val_loss: 3.9953e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 4.1485e-04 - val_loss: 3.9648e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.9757e-04 - val_loss: 4.2720e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.8093e-04 - val_loss: 3.6635e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.6589e-04 - val_loss: 3.9828e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.5400e-04 - val_loss: 3.6645e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.3872e-04 - val_loss: 3.3938e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.2558e-04 - val_loss: 3.1821e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.0990e-04 - val_loss: 3.0978e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.9318e-04 - val_loss: 3.1061e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.7970e-04 - val_loss: 2.7748e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.7071e-04 - val_loss: 2.6017e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 2.5918e-04 - val_loss: 2.4347e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.4787e-04 - val_loss: 2.2460e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.3652e-04 - val_loss: 2.0709e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.2515e-04 - val_loss: 1.9569e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.1344e-04 - val_loss: 1.8529e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.0249e-04 - val_loss: 1.8556e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.9457e-04 - val_loss: 1.8412e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.8699e-04 - val_loss: 1.7932e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.8073e-04 - val_loss: 1.8379e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7681e-04 - val_loss: 1.7869e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7184e-04 - val_loss: 1.7554e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.6928e-04 - val_loss: 1.7757e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6715e-04 - val_loss: 1.5849e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.6495e-04 - val_loss: 1.4940e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6129e-04 - val_loss: 1.7646e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6042e-04 - val_loss: 1.7317e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 442us/step - loss: 1.5748e-04 - val_loss: 1.9280e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.5694e-04 - val_loss: 1.8003e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.5404e-04 - val_loss: 1.8093e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5453e-04 - val_loss: 1.7385e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5116e-04 - val_loss: 1.7832e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4938e-04 - val_loss: 1.6652e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.4718e-04 - val_loss: 1.6446e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4504e-04 - val_loss: 1.6306e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4406e-04 - val_loss: 1.5259e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4163e-04 - val_loss: 1.6114e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4014e-04 - val_loss: 1.4795e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3913e-04 - val_loss: 1.4592e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3834e-04 - val_loss: 1.4641e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3640e-04 - val_loss: 1.4570e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3418e-04 - val_loss: 1.5401e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3265e-04 - val_loss: 1.6098e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3134e-04 - val_loss: 1.5862e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3047e-04 - val_loss: 1.6080e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2975e-04 - val_loss: 1.6073e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.2880e-04 - val_loss: 1.6197e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.2804e-04 - val_loss: 1.5464e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.2665e-04 - val_loss: 1.6070e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2561e-04 - val_loss: 1.6337e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2473e-04 - val_loss: 1.6756e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.2355e-04 - val_loss: 1.7031e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.2281e-04 - val_loss: 1.7393e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2233e-04 - val_loss: 1.7784e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2109e-04 - val_loss: 1.8151e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2027e-04 - val_loss: 1.8042e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.1971e-04 - val_loss: 1.8543e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.1940e-04 - val_loss: 1.8894e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.1831e-04 - val_loss: 1.9575e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.1734e-04 - val_loss: 1.9264e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.1751e-04 - val_loss: 1.9808e-04\n",
      "Validation RMSE:  0.01407414 \n",
      "\n",
      "Model took 1217.02 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 446, 447, 448, 449, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 0.0037 - val_loss: 0.0019\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 9.6868e-04 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 6.7194e-04 - val_loss: 7.6499e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 5.6186e-04 - val_loss: 5.9118e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 4.9782e-04 - val_loss: 4.6793e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 4.5921e-04 - val_loss: 4.8772e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 4.3305e-04 - val_loss: 4.7696e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 4.1246e-04 - val_loss: 4.1957e-04\n",
      "Epoch 9/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 17s 445us/step - loss: 3.9292e-04 - val_loss: 4.0811e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.7662e-04 - val_loss: 4.0910e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.6377e-04 - val_loss: 4.0558e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 3.4995e-04 - val_loss: 4.0879e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.3990e-04 - val_loss: 4.1590e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.2372e-04 - val_loss: 4.0181e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.1029e-04 - val_loss: 3.7951e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 2.9364e-04 - val_loss: 3.3462e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.8140e-04 - val_loss: 3.1083e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.6909e-04 - val_loss: 2.7593e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.5692e-04 - val_loss: 2.5348e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.4861e-04 - val_loss: 2.2584e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 2.3763e-04 - val_loss: 2.1769e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.2896e-04 - val_loss: 2.1215e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.2225e-04 - val_loss: 2.1421e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.1887e-04 - val_loss: 2.2508e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 2.1646e-04 - val_loss: 2.2161e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.1291e-04 - val_loss: 2.1756e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.1023e-04 - val_loss: 2.1558e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.0655e-04 - val_loss: 2.1878e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.0258e-04 - val_loss: 2.1698e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.9892e-04 - val_loss: 1.9845e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.9339e-04 - val_loss: 1.9557e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.8713e-04 - val_loss: 1.8313e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.8266e-04 - val_loss: 1.8131e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.8006e-04 - val_loss: 1.5867e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.7622e-04 - val_loss: 1.5730e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7288e-04 - val_loss: 1.5430e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.6953e-04 - val_loss: 1.4638e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.6813e-04 - val_loss: 1.4109e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6930e-04 - val_loss: 1.4952e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6636e-04 - val_loss: 1.5073e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.6016e-04 - val_loss: 1.4066e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.5987e-04 - val_loss: 1.4816e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.5734e-04 - val_loss: 1.4587e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5325e-04 - val_loss: 1.5133e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.5282e-04 - val_loss: 1.3748e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.5221e-04 - val_loss: 1.3760e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4871e-04 - val_loss: 1.4680e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4771e-04 - val_loss: 1.4625e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4523e-04 - val_loss: 1.4454e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4481e-04 - val_loss: 1.4396e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4473e-04 - val_loss: 1.3994e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4332e-04 - val_loss: 1.3527e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4125e-04 - val_loss: 1.3948e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4074e-04 - val_loss: 1.3904e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3902e-04 - val_loss: 1.3697e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3709e-04 - val_loss: 1.4626e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3471e-04 - val_loss: 1.4435e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 443us/step - loss: 1.3401e-04 - val_loss: 1.3869e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3151e-04 - val_loss: 1.4758e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3125e-04 - val_loss: 1.4236e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3026e-04 - val_loss: 1.4510e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2840e-04 - val_loss: 1.3970e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.2672e-04 - val_loss: 1.3465e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2644e-04 - val_loss: 1.3741e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2533e-04 - val_loss: 1.3197e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.2352e-04 - val_loss: 1.3516e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2216e-04 - val_loss: 1.3432e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2251e-04 - val_loss: 1.4381e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2234e-04 - val_loss: 1.3837e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2130e-04 - val_loss: 1.4222e-04\n",
      "Validation RMSE:  0.011925728 \n",
      "\n",
      "Model took 1237.39 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 22 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 1, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1, 1, 1]\n",
      "Lags length:\n",
      " 66\n",
      "Length Used:\n",
      " 22\n",
      "New Dataset shape after Lags: (48978, 67)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  22\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 66)\n",
      "Train X shape after np.reshape (39182, 22, 3)\n",
      "Test X shape after np.reshape (9796, 22, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 23s 583us/step - loss: 0.0052 - val_loss: 0.0034\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 8.1225e-04 - val_loss: 7.5337e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 5.6343e-04 - val_loss: 7.7257e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 4.7133e-04 - val_loss: 5.9680e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 4.1804e-04 - val_loss: 3.9324e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.8416e-04 - val_loss: 3.9942e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 3.5829e-04 - val_loss: 3.8176e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.3899e-04 - val_loss: 3.8573e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.2708e-04 - val_loss: 3.6492e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.1493e-04 - val_loss: 3.7902e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.0612e-04 - val_loss: 3.7191e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.9883e-04 - val_loss: 3.6185e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.9084e-04 - val_loss: 3.4089e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.8374e-04 - val_loss: 3.2449e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.7547e-04 - val_loss: 3.0951e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.6981e-04 - val_loss: 2.9547e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.6257e-04 - val_loss: 2.8110e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.5711e-04 - val_loss: 2.7723e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.5187e-04 - val_loss: 2.5512e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.4749e-04 - val_loss: 2.6291e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.4154e-04 - val_loss: 2.4855e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.3710e-04 - val_loss: 2.4482e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.3194e-04 - val_loss: 2.3156e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.2566e-04 - val_loss: 2.3216e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.1831e-04 - val_loss: 2.2375e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.1022e-04 - val_loss: 2.1426e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.9993e-04 - val_loss: 2.1589e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.9288e-04 - val_loss: 2.1679e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.8979e-04 - val_loss: 1.9483e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.8714e-04 - val_loss: 1.9135e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.8513e-04 - val_loss: 1.7289e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.8265e-04 - val_loss: 1.7033e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.7849e-04 - val_loss: 1.6229e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.7603e-04 - val_loss: 1.8033e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.7412e-04 - val_loss: 1.6852e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.7190e-04 - val_loss: 1.5269e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.6534e-04 - val_loss: 1.5860e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.6144e-04 - val_loss: 1.5423e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.5921e-04 - val_loss: 1.9323e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.5660e-04 - val_loss: 1.8205e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.5470e-04 - val_loss: 1.3878e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.5021e-04 - val_loss: 1.5174e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.4861e-04 - val_loss: 1.3042e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.4399e-04 - val_loss: 1.3717e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.4100e-04 - val_loss: 1.5352e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.4085e-04 - val_loss: 1.3002e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3900e-04 - val_loss: 1.2917e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3564e-04 - val_loss: 1.3149e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3276e-04 - val_loss: 1.2156e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2988e-04 - val_loss: 1.2403e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.2929e-04 - val_loss: 1.3192e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2656e-04 - val_loss: 1.2111e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2451e-04 - val_loss: 1.1138e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2175e-04 - val_loss: 1.1225e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2092e-04 - val_loss: 1.0691e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.1927e-04 - val_loss: 1.0858e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1674e-04 - val_loss: 1.1005e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1455e-04 - val_loss: 1.1107e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1459e-04 - val_loss: 1.0975e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.1405e-04 - val_loss: 1.0919e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1215e-04 - val_loss: 1.0331e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.1039e-04 - val_loss: 1.0062e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.0938e-04 - val_loss: 1.0179e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.0831e-04 - val_loss: 1.0667e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.0716e-04 - val_loss: 1.0908e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.0488e-04 - val_loss: 1.0579e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.0452e-04 - val_loss: 9.8368e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.0384e-04 - val_loss: 1.0290e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.0263e-04 - val_loss: 9.9347e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.0123e-04 - val_loss: 9.6146e-05\n",
      "Validation RMSE:  0.009805397 \n",
      "\n",
      "Model took 1350.27 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 9.8887e-04 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 6.8264e-04 - val_loss: 7.2576e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 5.7867e-04 - val_loss: 0.0010\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 5.2922e-04 - val_loss: 6.0740e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 4.8828e-04 - val_loss: 5.6178e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 4.5496e-04 - val_loss: 6.5465e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 4.2617e-04 - val_loss: 3.8480e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 4.0572e-04 - val_loss: 3.0811e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.8349e-04 - val_loss: 2.9254e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.6628e-04 - val_loss: 3.4597e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.4751e-04 - val_loss: 4.6153e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.3798e-04 - val_loss: 4.8067e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.2242e-04 - val_loss: 5.0177e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.1238e-04 - val_loss: 4.5283e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.0443e-04 - val_loss: 3.5961e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 2.9442e-04 - val_loss: 3.4312e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.8242e-04 - val_loss: 3.4323e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.7002e-04 - val_loss: 3.5146e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.5811e-04 - val_loss: 3.4772e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.5008e-04 - val_loss: 2.7703e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.4397e-04 - val_loss: 2.4892e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.3987e-04 - val_loss: 2.2929e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.3314e-04 - val_loss: 2.2157e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.3068e-04 - val_loss: 2.1484e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.2604e-04 - val_loss: 2.1036e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.2280e-04 - val_loss: 2.0182e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.1870e-04 - val_loss: 2.0131e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 2.1394e-04 - val_loss: 2.5092e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.0992e-04 - val_loss: 3.1072e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 2.0785e-04 - val_loss: 2.9162e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.0551e-04 - val_loss: 2.3100e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.0279e-04 - val_loss: 2.4781e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.9884e-04 - val_loss: 2.2779e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.9787e-04 - val_loss: 2.1993e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.9463e-04 - val_loss: 2.1481e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.8757e-04 - val_loss: 2.2509e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.7730e-04 - val_loss: 2.2089e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.7332e-04 - val_loss: 1.6993e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6845e-04 - val_loss: 1.9485e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.6577e-04 - val_loss: 1.6101e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.6221e-04 - val_loss: 1.7443e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.6111e-04 - val_loss: 1.4323e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.5783e-04 - val_loss: 1.7492e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.5749e-04 - val_loss: 1.4597e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.5252e-04 - val_loss: 1.7566e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.5381e-04 - val_loss: 1.2927e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4912e-04 - val_loss: 1.8267e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4980e-04 - val_loss: 1.4123e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4784e-04 - val_loss: 1.1919e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4536e-04 - val_loss: 1.2493e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4482e-04 - val_loss: 1.1890e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4360e-04 - val_loss: 1.2393e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4227e-04 - val_loss: 1.2582e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4106e-04 - val_loss: 1.3723e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3971e-04 - val_loss: 1.4816e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3937e-04 - val_loss: 1.5714e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3678e-04 - val_loss: 1.9795e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3688e-04 - val_loss: 1.9528e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3636e-04 - val_loss: 1.9265e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 444us/step - loss: 1.3606e-04 - val_loss: 1.8923e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3430e-04 - val_loss: 2.0375e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.3369e-04 - val_loss: 2.0567e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.3266e-04 - val_loss: 1.9279e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3158e-04 - val_loss: 1.9173e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3110e-04 - val_loss: 1.7433e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2995e-04 - val_loss: 1.8719e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.2857e-04 - val_loss: 1.8758e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2846e-04 - val_loss: 1.8417e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.2641e-04 - val_loss: 1.9217e-04\n",
      "Validation RMSE:  0.013862452 \n",
      "\n",
      "Model took 1236.43 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 0.0037 - val_loss: 0.0025\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 6.9846e-04 - val_loss: 8.2773e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 5.9298e-04 - val_loss: 7.1346e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 5.3412e-04 - val_loss: 7.1630e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 4.9020e-04 - val_loss: 6.6189e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 4.5290e-04 - val_loss: 7.1135e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 4.2447e-04 - val_loss: 4.7848e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 4.0081e-04 - val_loss: 4.5718e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.8192e-04 - val_loss: 5.7776e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.6362e-04 - val_loss: 6.3676e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.4885e-04 - val_loss: 5.7477e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.3229e-04 - val_loss: 4.4540e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.1823e-04 - val_loss: 3.3173e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.0624e-04 - val_loss: 2.9922e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.9078e-04 - val_loss: 4.8975e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.8576e-04 - val_loss: 2.8417e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.7426e-04 - val_loss: 2.9663e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.6160e-04 - val_loss: 2.3140e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.5616e-04 - val_loss: 2.1138e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.5064e-04 - val_loss: 2.0497e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.4624e-04 - val_loss: 2.0425e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.3943e-04 - val_loss: 2.3087e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.3809e-04 - val_loss: 1.9928e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.3021e-04 - val_loss: 2.1971e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.2808e-04 - val_loss: 2.2088e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.2647e-04 - val_loss: 2.1758e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.2350e-04 - val_loss: 2.1001e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.2174e-04 - val_loss: 2.0406e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.1610e-04 - val_loss: 1.7511e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.1327e-04 - val_loss: 1.8803e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.0972e-04 - val_loss: 1.8468e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.0448e-04 - val_loss: 1.9025e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.0030e-04 - val_loss: 1.9016e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.9260e-04 - val_loss: 1.6578e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.8815e-04 - val_loss: 1.7941e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.8507e-04 - val_loss: 1.7455e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.8178e-04 - val_loss: 1.7930e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.7775e-04 - val_loss: 1.6955e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7670e-04 - val_loss: 1.6872e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.7530e-04 - val_loss: 1.6343e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7325e-04 - val_loss: 1.7365e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.7169e-04 - val_loss: 1.7254e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6942e-04 - val_loss: 1.5873e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.6734e-04 - val_loss: 1.4944e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6495e-04 - val_loss: 1.5589e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6243e-04 - val_loss: 1.3029e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.6098e-04 - val_loss: 1.3469e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.5893e-04 - val_loss: 1.3833e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.5704e-04 - val_loss: 1.5257e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5600e-04 - val_loss: 1.5610e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5351e-04 - val_loss: 1.3963e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5052e-04 - val_loss: 2.0052e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.5215e-04 - val_loss: 1.5411e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4994e-04 - val_loss: 1.5634e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.4875e-04 - val_loss: 1.5713e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4743e-04 - val_loss: 1.6760e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4692e-04 - val_loss: 1.5224e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4530e-04 - val_loss: 1.9640e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.4446e-04 - val_loss: 2.0306e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4429e-04 - val_loss: 2.0259e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4367e-04 - val_loss: 1.8514e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4055e-04 - val_loss: 2.2181e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4043e-04 - val_loss: 2.8345e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4116e-04 - val_loss: 1.9878e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4022e-04 - val_loss: 1.5966e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4042e-04 - val_loss: 1.9536e-04\n",
      "Validation RMSE:  0.013977145 \n",
      "\n",
      "Model took 1189.30 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 22s 551us/step - loss: 0.0052 - val_loss: 0.0036\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 0.0012 - val_loss: 0.0026\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 6.6673e-04 - val_loss: 7.1250e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 5.5778e-04 - val_loss: 5.2260e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 4.8814e-04 - val_loss: 4.3037e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 4.3627e-04 - val_loss: 3.7498e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.9933e-04 - val_loss: 2.8733e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.7271e-04 - val_loss: 2.6432e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.5681e-04 - val_loss: 2.5174e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.4208e-04 - val_loss: 2.6745e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.3180e-04 - val_loss: 2.5343e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 3.2045e-04 - val_loss: 2.3783e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.0704e-04 - val_loss: 2.2903e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.9657e-04 - val_loss: 2.3808e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.8430e-04 - val_loss: 2.3530e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.7288e-04 - val_loss: 2.3009e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.6505e-04 - val_loss: 2.2586e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.5936e-04 - val_loss: 2.2213e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.4812e-04 - val_loss: 3.3107e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.3825e-04 - val_loss: 2.4527e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.3379e-04 - val_loss: 2.1131e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.3364e-04 - val_loss: 1.7241e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 2.2620e-04 - val_loss: 1.8070e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.2227e-04 - val_loss: 1.6441e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.1797e-04 - val_loss: 1.4595e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.1042e-04 - val_loss: 1.3126e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.0268e-04 - val_loss: 1.2360e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.9348e-04 - val_loss: 1.1052e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.8765e-04 - val_loss: 1.0064e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.8186e-04 - val_loss: 1.0244e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.7760e-04 - val_loss: 1.0190e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.7301e-04 - val_loss: 9.8063e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.6931e-04 - val_loss: 9.8541e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.6631e-04 - val_loss: 9.3370e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6340e-04 - val_loss: 9.5411e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6091e-04 - val_loss: 9.4913e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5869e-04 - val_loss: 9.5226e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.5634e-04 - val_loss: 9.3019e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.5318e-04 - val_loss: 9.2460e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.5096e-04 - val_loss: 9.4371e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4940e-04 - val_loss: 9.4198e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4767e-04 - val_loss: 9.2622e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4382e-04 - val_loss: 8.9378e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4282e-04 - val_loss: 9.4373e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4076e-04 - val_loss: 9.2075e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3896e-04 - val_loss: 8.9462e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3804e-04 - val_loss: 9.0664e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3644e-04 - val_loss: 9.2388e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3490e-04 - val_loss: 9.1142e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3392e-04 - val_loss: 9.1844e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.3242e-04 - val_loss: 9.2007e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3202e-04 - val_loss: 9.7414e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3046e-04 - val_loss: 9.2203e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3027e-04 - val_loss: 9.6619e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2919e-04 - val_loss: 8.9676e-05\n",
      "Epoch 56/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2792e-04 - val_loss: 9.7328e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2658e-04 - val_loss: 9.5528e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.2599e-04 - val_loss: 9.2755e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.2531e-04 - val_loss: 8.9925e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2460e-04 - val_loss: 9.3458e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2329e-04 - val_loss: 9.1214e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2255e-04 - val_loss: 8.9946e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2151e-04 - val_loss: 8.7693e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1995e-04 - val_loss: 8.9292e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.1821e-04 - val_loss: 8.4110e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.1700e-04 - val_loss: 8.4063e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.1538e-04 - val_loss: 8.2916e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.1484e-04 - val_loss: 8.2264e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.1355e-04 - val_loss: 8.1780e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.1302e-04 - val_loss: 8.2252e-05\n",
      "Validation RMSE:  0.009069273 \n",
      "\n",
      "Model took 1241.76 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 6 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 446, 447, 448, 449, 450, 451, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 0.0034 - val_loss: 0.0021\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 9.4398e-04 - val_loss: 6.6950e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 6.7652e-04 - val_loss: 5.6087e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 5.7713e-04 - val_loss: 4.5494e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 5.2324e-04 - val_loss: 4.0225e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 4.7840e-04 - val_loss: 4.2307e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 4.4450e-04 - val_loss: 4.1209e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 4.1959e-04 - val_loss: 4.1906e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.9365e-04 - val_loss: 4.1620e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.8103e-04 - val_loss: 3.7203e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.6382e-04 - val_loss: 3.6828e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.4919e-04 - val_loss: 3.4651e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.3408e-04 - val_loss: 3.3961e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.2330e-04 - val_loss: 3.3289e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 3.1260e-04 - val_loss: 3.1900e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 3.0208e-04 - val_loss: 2.8807e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.9162e-04 - val_loss: 2.8693e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.8265e-04 - val_loss: 2.7270e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.7175e-04 - val_loss: 2.6539e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.6053e-04 - val_loss: 2.5565e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.4975e-04 - val_loss: 2.5889e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.4291e-04 - val_loss: 2.6009e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.3385e-04 - val_loss: 2.6540e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.2555e-04 - val_loss: 2.6033e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.1730e-04 - val_loss: 2.7137e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.0948e-04 - val_loss: 2.4494e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.0123e-04 - val_loss: 1.9618e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.9441e-04 - val_loss: 1.9913e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.8769e-04 - val_loss: 1.8439e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.8622e-04 - val_loss: 1.8943e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.8333e-04 - val_loss: 1.9332e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.8102e-04 - val_loss: 1.9343e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.7592e-04 - val_loss: 1.9710e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7449e-04 - val_loss: 1.8950e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.7330e-04 - val_loss: 1.8318e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.7196e-04 - val_loss: 1.7890e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7031e-04 - val_loss: 1.6661e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.6835e-04 - val_loss: 1.5854e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.6424e-04 - val_loss: 1.6592e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.6272e-04 - val_loss: 1.7564e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.6057e-04 - val_loss: 1.6067e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.5751e-04 - val_loss: 1.5688e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.5489e-04 - val_loss: 1.4540e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.5261e-04 - val_loss: 1.4164e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.5057e-04 - val_loss: 1.5112e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4906e-04 - val_loss: 1.5485e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4783e-04 - val_loss: 1.5661e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4562e-04 - val_loss: 1.3599e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4309e-04 - val_loss: 1.4432e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4324e-04 - val_loss: 1.3988e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4206e-04 - val_loss: 1.4390e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4099e-04 - val_loss: 1.4760e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4083e-04 - val_loss: 1.4696e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3935e-04 - val_loss: 1.4647e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3860e-04 - val_loss: 1.4328e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3713e-04 - val_loss: 1.4594e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3624e-04 - val_loss: 1.4683e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3501e-04 - val_loss: 1.4699e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3393e-04 - val_loss: 1.4644e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3271e-04 - val_loss: 1.4995e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3143e-04 - val_loss: 1.4367e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3033e-04 - val_loss: 1.4511e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2921e-04 - val_loss: 1.4578e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.2765e-04 - val_loss: 1.5188e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2676e-04 - val_loss: 1.4765e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2466e-04 - val_loss: 1.4619e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2279e-04 - val_loss: 1.4746e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2084e-04 - val_loss: 1.5929e-04\n",
      "Validation RMSE:  0.012621046 \n",
      "\n",
      "Model took 1203.78 seconds to train\n",
      "8  \t8     \t0.0117133\t0.00186763\t0.00890044\t0.0140741\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 22 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 1, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1, 1, 1]\n",
      "Lags length:\n",
      " 66\n",
      "Length Used:\n",
      " 22\n",
      "New Dataset shape after Lags: (48978, 67)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  22\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 66)\n",
      "Train X shape after np.reshape (39182, 22, 3)\n",
      "Test X shape after np.reshape (9796, 22, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 24s 603us/step - loss: 0.0052 - val_loss: 0.0025\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 7.9868e-04 - val_loss: 5.0811e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 5.5952e-04 - val_loss: 3.9219e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 4.8970e-04 - val_loss: 3.0001e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 4.4026e-04 - val_loss: 2.7160e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 4.0466e-04 - val_loss: 2.3028e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.8374e-04 - val_loss: 2.2712e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 3.6737e-04 - val_loss: 2.2511e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.5008e-04 - val_loss: 2.2598e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 3.3864e-04 - val_loss: 2.2197e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.2730e-04 - val_loss: 2.3500e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 3.1681e-04 - val_loss: 2.4448e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.0997e-04 - val_loss: 2.3921e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 3.0359e-04 - val_loss: 2.5761e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.9478e-04 - val_loss: 2.7518e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.8763e-04 - val_loss: 2.8375e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.8295e-04 - val_loss: 2.8988e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.7717e-04 - val_loss: 2.9226e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.7621e-04 - val_loss: 3.1318e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.7168e-04 - val_loss: 3.0447e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.6865e-04 - val_loss: 3.2767e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.6379e-04 - val_loss: 2.6425e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.5911e-04 - val_loss: 2.9269e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.5339e-04 - val_loss: 2.9951e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.4147e-04 - val_loss: 2.7482e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.3541e-04 - val_loss: 2.7243e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.2389e-04 - val_loss: 2.6024e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.2175e-04 - val_loss: 2.5072e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.1911e-04 - val_loss: 2.3838e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.1557e-04 - val_loss: 2.5304e-04\n",
      "Validation RMSE:  0.015907144 \n",
      "\n",
      "Model took 588.44 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 22s 562us/step - loss: 0.0053 - val_loss: 0.0033\n",
      "Epoch 2/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 448us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 6.5382e-04 - val_loss: 5.5844e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 5.3788e-04 - val_loss: 3.6753e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 4.6240e-04 - val_loss: 5.3043e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 4.1121e-04 - val_loss: 3.8310e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.8283e-04 - val_loss: 2.9977e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.6249e-04 - val_loss: 3.3106e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.4629e-04 - val_loss: 3.1903e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.3283e-04 - val_loss: 2.7536e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.2119e-04 - val_loss: 2.5268e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.1054e-04 - val_loss: 2.4778e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.9874e-04 - val_loss: 2.2945e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.8786e-04 - val_loss: 2.2352e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.7936e-04 - val_loss: 2.3476e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.7039e-04 - val_loss: 2.4390e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 2.6164e-04 - val_loss: 2.5934e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.4458e-04 - val_loss: 2.5522e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.3719e-04 - val_loss: 1.7330e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.2814e-04 - val_loss: 1.5900e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.1774e-04 - val_loss: 1.9311e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.0906e-04 - val_loss: 1.5762e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.0071e-04 - val_loss: 1.3783e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.9186e-04 - val_loss: 1.1252e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.8659e-04 - val_loss: 1.0911e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.8318e-04 - val_loss: 1.1909e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.7998e-04 - val_loss: 1.1253e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.7595e-04 - val_loss: 1.1079e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.7481e-04 - val_loss: 1.1184e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7250e-04 - val_loss: 1.0631e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.6907e-04 - val_loss: 1.0209e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6799e-04 - val_loss: 1.0280e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.6541e-04 - val_loss: 9.5318e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6214e-04 - val_loss: 9.7712e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.6040e-04 - val_loss: 1.0002e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.5868e-04 - val_loss: 9.4595e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5658e-04 - val_loss: 9.5218e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5584e-04 - val_loss: 9.1111e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.5217e-04 - val_loss: 9.2775e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.5159e-04 - val_loss: 9.2579e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4920e-04 - val_loss: 9.3273e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4728e-04 - val_loss: 9.2127e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4595e-04 - val_loss: 9.3226e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.4474e-04 - val_loss: 9.4527e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4322e-04 - val_loss: 9.1970e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4210e-04 - val_loss: 9.3663e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4062e-04 - val_loss: 9.1226e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3921e-04 - val_loss: 9.3442e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3755e-04 - val_loss: 9.1949e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3645e-04 - val_loss: 9.1109e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3493e-04 - val_loss: 9.2016e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3304e-04 - val_loss: 9.2176e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3125e-04 - val_loss: 9.1986e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.3024e-04 - val_loss: 9.0871e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.2920e-04 - val_loss: 8.9961e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2773e-04 - val_loss: 9.3334e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2669e-04 - val_loss: 9.0766e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2576e-04 - val_loss: 9.3711e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.2508e-04 - val_loss: 9.1159e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2437e-04 - val_loss: 9.2295e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2384e-04 - val_loss: 9.5403e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.2218e-04 - val_loss: 8.9246e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2218e-04 - val_loss: 8.9378e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2156e-04 - val_loss: 9.1296e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.2051e-04 - val_loss: 8.8319e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.1970e-04 - val_loss: 9.0725e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.1883e-04 - val_loss: 9.3656e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.1778e-04 - val_loss: 9.3623e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1858e-04 - val_loss: 8.9578e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.1689e-04 - val_loss: 9.3339e-05\n",
      "Validation RMSE:  0.009661207 \n",
      "\n",
      "Model took 1241.81 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 60\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 21 4 19 10\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 350\n",
      "3rd Window Start: 876\n",
      "4th Window Start: 1645\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 350, 351, 352, 353, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 1, 1]\n",
      "Lags length:\n",
      " 63\n",
      "Length Used:\n",
      " 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (49106, 64)\n",
      "Training and test data length 39284 9822\n",
      "trainX.shape[0]:  39284\n",
      "window_length:  21\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39284, 63)\n",
      "Train X shape after np.reshape (39284, 21, 3)\n",
      "Test X shape after np.reshape (9822, 21, 3)\n",
      "Train Y Shape (39284,)\n",
      "Test Y Shape (9822,)\n",
      "Train on 39284 samples, validate on 9822 samples\n",
      "Epoch 1/70\n",
      "39284/39284 [==============================] - 25s 642us/step - loss: 0.0037 - val_loss: 0.0012\n",
      "Epoch 2/70\n",
      "39284/39284 [==============================] - 21s 529us/step - loss: 0.0011 - val_loss: 7.5442e-04\n",
      "Epoch 3/70\n",
      "39284/39284 [==============================] - 21s 531us/step - loss: 8.2870e-04 - val_loss: 6.3800e-04\n",
      "Epoch 4/70\n",
      "39284/39284 [==============================] - 21s 529us/step - loss: 6.9833e-04 - val_loss: 5.9496e-04\n",
      "Epoch 5/70\n",
      "39284/39284 [==============================] - 21s 534us/step - loss: 6.3406e-04 - val_loss: 5.9215e-04\n",
      "Epoch 6/70\n",
      "39284/39284 [==============================] - 21s 531us/step - loss: 5.9064e-04 - val_loss: 6.3975e-04\n",
      "Epoch 7/70\n",
      "39284/39284 [==============================] - 21s 532us/step - loss: 5.4690e-04 - val_loss: 6.8922e-04\n",
      "Epoch 8/70\n",
      "39284/39284 [==============================] - 21s 533us/step - loss: 5.0429e-04 - val_loss: 6.2548e-04\n",
      "Epoch 9/70\n",
      "39284/39284 [==============================] - 21s 530us/step - loss: 4.6923e-04 - val_loss: 8.1312e-04\n",
      "Epoch 10/70\n",
      "39284/39284 [==============================] - 21s 530us/step - loss: 4.5445e-04 - val_loss: 9.7073e-04\n",
      "Epoch 11/70\n",
      "39284/39284 [==============================] - 21s 529us/step - loss: 4.4021e-04 - val_loss: 0.0012\n",
      "Epoch 12/70\n",
      "39284/39284 [==============================] - 21s 534us/step - loss: 4.2299e-04 - val_loss: 0.0012\n",
      "Epoch 13/70\n",
      "39284/39284 [==============================] - 21s 532us/step - loss: 4.0952e-04 - val_loss: 0.0016\n",
      "Epoch 14/70\n",
      "39284/39284 [==============================] - 21s 527us/step - loss: 3.9438e-04 - val_loss: 0.0012\n",
      "Epoch 15/70\n",
      "39284/39284 [==============================] - 21s 526us/step - loss: 3.7766e-04 - val_loss: 0.0010\n",
      "Epoch 16/70\n",
      "39284/39284 [==============================] - 21s 528us/step - loss: 3.5988e-04 - val_loss: 8.3498e-04\n",
      "Epoch 17/70\n",
      "39284/39284 [==============================] - 21s 527us/step - loss: 3.4454e-04 - val_loss: 7.7820e-04\n",
      "Epoch 18/70\n",
      "39284/39284 [==============================] - 21s 532us/step - loss: 3.3086e-04 - val_loss: 7.7301e-04\n",
      "Epoch 19/70\n",
      "39284/39284 [==============================] - 21s 527us/step - loss: 3.2179e-04 - val_loss: 7.8148e-04\n",
      "Epoch 20/70\n",
      "39284/39284 [==============================] - 21s 527us/step - loss: 3.1569e-04 - val_loss: 7.8397e-04\n",
      "Epoch 21/70\n",
      "39284/39284 [==============================] - 21s 528us/step - loss: 3.0682e-04 - val_loss: 6.9118e-04\n",
      "Epoch 22/70\n",
      "39284/39284 [==============================] - 21s 525us/step - loss: 2.9838e-04 - val_loss: 6.4851e-04\n",
      "Epoch 23/70\n",
      "39284/39284 [==============================] - 21s 531us/step - loss: 2.9107e-04 - val_loss: 6.1426e-04\n",
      "Epoch 24/70\n",
      "39284/39284 [==============================] - 21s 531us/step - loss: 2.8686e-04 - val_loss: 6.0779e-04\n",
      "Epoch 25/70\n",
      "39284/39284 [==============================] - 21s 532us/step - loss: 2.8336e-04 - val_loss: 5.9987e-04\n",
      "Validation RMSE:  0.024492146 \n",
      "\n",
      "Model took 530.97 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 22s 569us/step - loss: 0.0034 - val_loss: 0.0020\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 9.2835e-04 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 6.7697e-04 - val_loss: 9.9200e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 5.8727e-04 - val_loss: 7.8889e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 5.1122e-04 - val_loss: 5.5898e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 4.6451e-04 - val_loss: 4.2280e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 4.2670e-04 - val_loss: 3.9183e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.9718e-04 - val_loss: 4.1550e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.7245e-04 - val_loss: 4.2254e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.5155e-04 - val_loss: 4.0199e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.3538e-04 - val_loss: 3.9596e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.1767e-04 - val_loss: 3.6278e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.0495e-04 - val_loss: 3.0403e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.9158e-04 - val_loss: 2.9279e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.7636e-04 - val_loss: 3.0535e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.6797e-04 - val_loss: 2.4430e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.5747e-04 - val_loss: 2.2705e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.5090e-04 - val_loss: 2.2237e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 2.4563e-04 - val_loss: 2.1036e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.4080e-04 - val_loss: 2.5005e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.3744e-04 - val_loss: 2.1031e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.3305e-04 - val_loss: 2.0341e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.2847e-04 - val_loss: 2.5253e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.2721e-04 - val_loss: 2.4897e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.2162e-04 - val_loss: 2.0269e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.1959e-04 - val_loss: 2.0289e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.1497e-04 - val_loss: 1.9086e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.1272e-04 - val_loss: 1.8300e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.1115e-04 - val_loss: 1.8631e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.0766e-04 - val_loss: 1.8854e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.0456e-04 - val_loss: 1.8572e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.0086e-04 - val_loss: 1.9597e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.9465e-04 - val_loss: 2.0658e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.8800e-04 - val_loss: 2.0533e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8480e-04 - val_loss: 1.7055e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.7868e-04 - val_loss: 1.6545e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.7442e-04 - val_loss: 1.6263e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6969e-04 - val_loss: 1.9098e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6741e-04 - val_loss: 1.8871e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.6387e-04 - val_loss: 1.9807e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6283e-04 - val_loss: 1.5591e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.6101e-04 - val_loss: 1.6580e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5758e-04 - val_loss: 1.5654e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.5568e-04 - val_loss: 1.5155e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.5352e-04 - val_loss: 1.4465e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.5157e-04 - val_loss: 1.4132e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4993e-04 - val_loss: 1.3541e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4847e-04 - val_loss: 1.3678e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4669e-04 - val_loss: 1.2849e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4598e-04 - val_loss: 1.2577e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4425e-04 - val_loss: 1.2710e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4333e-04 - val_loss: 1.2655e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4171e-04 - val_loss: 1.2160e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4133e-04 - val_loss: 1.2471e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4103e-04 - val_loss: 1.1827e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3911e-04 - val_loss: 1.1711e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3816e-04 - val_loss: 1.1644e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3795e-04 - val_loss: 1.1044e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3661e-04 - val_loss: 1.1474e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3680e-04 - val_loss: 1.1293e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.3554e-04 - val_loss: 1.1169e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3496e-04 - val_loss: 1.1141e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3440e-04 - val_loss: 1.1259e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3245e-04 - val_loss: 1.1111e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3092e-04 - val_loss: 1.3143e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3130e-04 - val_loss: 1.3701e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3017e-04 - val_loss: 1.2505e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2999e-04 - val_loss: 1.2970e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2933e-04 - val_loss: 1.3584e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2865e-04 - val_loss: 1.3340e-04\n",
      "Validation RMSE:  0.011549866 \n",
      "\n",
      "Model took 1244.99 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 4\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 22s 572us/step - loss: 0.0068 - val_loss: 0.0045\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 7.4690e-04 - val_loss: 6.9559e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 5.4878e-04 - val_loss: 4.0969e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 4.8457e-04 - val_loss: 3.4910e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 4.3431e-04 - val_loss: 3.5415e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 4.0622e-04 - val_loss: 3.3302e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.8279e-04 - val_loss: 2.9101e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.6154e-04 - val_loss: 2.7928e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.4903e-04 - val_loss: 2.4573e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.3611e-04 - val_loss: 2.5513e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.2377e-04 - val_loss: 2.5977e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.1031e-04 - val_loss: 2.4664e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.9963e-04 - val_loss: 2.4218e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.9196e-04 - val_loss: 2.3057e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.8352e-04 - val_loss: 2.2095e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.7514e-04 - val_loss: 2.1555e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.6563e-04 - val_loss: 2.0581e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.5696e-04 - val_loss: 1.9579e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.4469e-04 - val_loss: 1.8360e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.3953e-04 - val_loss: 1.5313e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.3320e-04 - val_loss: 1.4108e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.1403e-04 - val_loss: 1.4525e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 2.0379e-04 - val_loss: 1.3938e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.9522e-04 - val_loss: 1.1438e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8731e-04 - val_loss: 1.0819e-04\n",
      "Epoch 27/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8092e-04 - val_loss: 1.0880e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7609e-04 - val_loss: 1.0638e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.7170e-04 - val_loss: 1.0291e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.6671e-04 - val_loss: 9.7864e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.6311e-04 - val_loss: 9.5890e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5975e-04 - val_loss: 9.6970e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5602e-04 - val_loss: 9.6311e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5382e-04 - val_loss: 9.6924e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5134e-04 - val_loss: 1.0285e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4848e-04 - val_loss: 9.6988e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.4711e-04 - val_loss: 1.0531e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4688e-04 - val_loss: 1.0056e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.4573e-04 - val_loss: 9.4775e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4427e-04 - val_loss: 9.4199e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4273e-04 - val_loss: 9.8340e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4124e-04 - val_loss: 9.2349e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3994e-04 - val_loss: 9.2243e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.3861e-04 - val_loss: 8.9393e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 17s 447us/step - loss: 1.3664e-04 - val_loss: 8.7170e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3567e-04 - val_loss: 8.4882e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3366e-04 - val_loss: 8.2959e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3271e-04 - val_loss: 8.3257e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3061e-04 - val_loss: 8.6015e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2945e-04 - val_loss: 8.3446e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2839e-04 - val_loss: 8.0056e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2501e-04 - val_loss: 8.1625e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2573e-04 - val_loss: 8.1474e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2419e-04 - val_loss: 8.1005e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2315e-04 - val_loss: 8.0341e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2247e-04 - val_loss: 7.8921e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.2200e-04 - val_loss: 7.8824e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.2112e-04 - val_loss: 7.8005e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1987e-04 - val_loss: 7.8527e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1959e-04 - val_loss: 7.9387e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.1870e-04 - val_loss: 8.0670e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.1753e-04 - val_loss: 8.0233e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.1596e-04 - val_loss: 8.1752e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.1518e-04 - val_loss: 8.0237e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.1441e-04 - val_loss: 8.5694e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.1365e-04 - val_loss: 8.5657e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1231e-04 - val_loss: 8.7021e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.1162e-04 - val_loss: 8.7788e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.0990e-04 - val_loss: 8.9099e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.0927e-04 - val_loss: 8.5070e-05\n",
      "Validation RMSE:  0.00922334 \n",
      "\n",
      "Model took 1246.04 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 23s 576us/step - loss: 0.0058 - val_loss: 0.0031\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 6.5371e-04 - val_loss: 7.1098e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 5.3998e-04 - val_loss: 4.9419e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 4.7707e-04 - val_loss: 4.8253e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 4.3042e-04 - val_loss: 3.8695e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.9730e-04 - val_loss: 3.6231e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.7000e-04 - val_loss: 3.6068e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.5369e-04 - val_loss: 3.4011e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.3749e-04 - val_loss: 3.2863e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.2366e-04 - val_loss: 3.3065e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.1254e-04 - val_loss: 3.4216e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.0526e-04 - val_loss: 3.3435e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.9709e-04 - val_loss: 3.2681e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.8713e-04 - val_loss: 3.3337e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.7844e-04 - val_loss: 3.2397e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.7100e-04 - val_loss: 3.1197e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.6460e-04 - val_loss: 3.0584e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.5620e-04 - val_loss: 2.9241e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.4959e-04 - val_loss: 3.0375e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.4435e-04 - val_loss: 2.7970e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.3914e-04 - val_loss: 2.7062e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.3113e-04 - val_loss: 2.5241e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.2067e-04 - val_loss: 2.2887e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.1333e-04 - val_loss: 1.6268e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.0168e-04 - val_loss: 1.3891e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.9145e-04 - val_loss: 1.2316e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.8530e-04 - val_loss: 1.2362e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.8085e-04 - val_loss: 1.1432e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.7530e-04 - val_loss: 1.0752e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.6810e-04 - val_loss: 1.0591e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.6444e-04 - val_loss: 1.0886e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6058e-04 - val_loss: 1.0160e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5708e-04 - val_loss: 9.9078e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5471e-04 - val_loss: 9.9279e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5099e-04 - val_loss: 1.0010e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4797e-04 - val_loss: 1.0087e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4551e-04 - val_loss: 1.0205e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.4295e-04 - val_loss: 1.0594e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4063e-04 - val_loss: 1.1017e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3840e-04 - val_loss: 1.0914e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3653e-04 - val_loss: 1.0981e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3488e-04 - val_loss: 1.1806e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3456e-04 - val_loss: 1.2659e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3304e-04 - val_loss: 1.2773e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3166e-04 - val_loss: 1.2242e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3048e-04 - val_loss: 1.1804e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2890e-04 - val_loss: 1.1768e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2799e-04 - val_loss: 1.1336e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2728e-04 - val_loss: 1.1052e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2622e-04 - val_loss: 1.1432e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2502e-04 - val_loss: 1.1633e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2433e-04 - val_loss: 1.1204e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2330e-04 - val_loss: 1.0976e-04\n",
      "Validation RMSE:  0.010476879 \n",
      "\n",
      "Model took 966.42 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 55\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 16 22 27 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1620\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030]\n",
      "Lags length:\n",
      " 81\n",
      "Length Used:\n",
      " 27\n",
      "New Dataset shape after Lags: (48970, 82)\n",
      "Training and test data length 39176 9794\n",
      "trainX.shape[0]:  39176\n",
      "window_length:  27\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39176, 81)\n",
      "Train X shape after np.reshape (39176, 27, 3)\n",
      "Test X shape after np.reshape (9794, 27, 3)\n",
      "Train Y Shape (39176,)\n",
      "Test Y Shape (9794,)\n",
      "Train on 39176 samples, validate on 9794 samples\n",
      "Epoch 1/70\n",
      "39176/39176 [==============================] - 33s 852us/step - loss: 0.0048 - val_loss: 0.0053\n",
      "Epoch 2/70\n",
      "39176/39176 [==============================] - 28s 725us/step - loss: 0.0017 - val_loss: 0.0045\n",
      "Epoch 3/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 0.0012 - val_loss: 0.0030\n",
      "Epoch 4/70\n",
      "39176/39176 [==============================] - 29s 729us/step - loss: 9.7563e-04 - val_loss: 0.0017\n",
      "Epoch 5/70\n",
      "39176/39176 [==============================] - 29s 728us/step - loss: 8.5723e-04 - val_loss: 0.0014\n",
      "Epoch 6/70\n",
      "39176/39176 [==============================] - 29s 728us/step - loss: 7.9876e-04 - val_loss: 0.0021\n",
      "Epoch 7/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 7.4865e-04 - val_loss: 0.0026\n",
      "Epoch 8/70\n",
      "39176/39176 [==============================] - 29s 728us/step - loss: 7.1971e-04 - val_loss: 0.0025\n",
      "Epoch 9/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 6.9410e-04 - val_loss: 0.0025\n",
      "Epoch 10/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 6.7135e-04 - val_loss: 0.0026\n",
      "Epoch 11/70\n",
      "39176/39176 [==============================] - 28s 722us/step - loss: 6.4780e-04 - val_loss: 0.0020\n",
      "Epoch 12/70\n",
      "39176/39176 [==============================] - 29s 731us/step - loss: 6.2670e-04 - val_loss: 0.0020\n",
      "Epoch 13/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 6.1350e-04 - val_loss: 0.0020\n",
      "Epoch 14/70\n",
      "39176/39176 [==============================] - 29s 730us/step - loss: 5.9749e-04 - val_loss: 0.0021\n",
      "Epoch 15/70\n",
      "39176/39176 [==============================] - 29s 729us/step - loss: 5.8765e-04 - val_loss: 0.0018\n",
      "Epoch 16/70\n",
      "39176/39176 [==============================] - 28s 727us/step - loss: 5.8088e-04 - val_loss: 0.0017\n",
      "Epoch 17/70\n",
      "39176/39176 [==============================] - 28s 727us/step - loss: 5.7389e-04 - val_loss: 0.0016\n",
      "Epoch 18/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 5.6651e-04 - val_loss: 0.0015\n",
      "Epoch 19/70\n",
      "39176/39176 [==============================] - 28s 725us/step - loss: 5.5295e-04 - val_loss: 0.0016\n",
      "Epoch 20/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 5.4668e-04 - val_loss: 0.0014\n",
      "Epoch 21/70\n",
      "39176/39176 [==============================] - 28s 722us/step - loss: 5.4046e-04 - val_loss: 0.0015\n",
      "Epoch 22/70\n",
      "39176/39176 [==============================] - 29s 731us/step - loss: 5.2656e-04 - val_loss: 0.0014\n",
      "Epoch 23/70\n",
      "39176/39176 [==============================] - 29s 729us/step - loss: 5.2600e-04 - val_loss: 0.0014\n",
      "Epoch 24/70\n",
      "39176/39176 [==============================] - 28s 725us/step - loss: 5.2137e-04 - val_loss: 0.0014\n",
      "Epoch 25/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 5.1655e-04 - val_loss: 0.0013\n",
      "Epoch 26/70\n",
      "39176/39176 [==============================] - 28s 727us/step - loss: 5.0393e-04 - val_loss: 0.0012\n",
      "Epoch 27/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39176/39176 [==============================] - 28s 725us/step - loss: 5.0130e-04 - val_loss: 0.0012\n",
      "Epoch 28/70\n",
      "39176/39176 [==============================] - 28s 727us/step - loss: 4.9527e-04 - val_loss: 0.0011\n",
      "Epoch 29/70\n",
      "39176/39176 [==============================] - 29s 732us/step - loss: 4.9024e-04 - val_loss: 0.0010\n",
      "Epoch 30/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 4.8519e-04 - val_loss: 0.0010\n",
      "Epoch 31/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 4.7799e-04 - val_loss: 0.0010\n",
      "Epoch 32/70\n",
      "39176/39176 [==============================] - 28s 724us/step - loss: 4.6753e-04 - val_loss: 9.9693e-04\n",
      "Epoch 33/70\n",
      "39176/39176 [==============================] - 28s 725us/step - loss: 4.6112e-04 - val_loss: 9.6184e-04\n",
      "Epoch 34/70\n",
      "39176/39176 [==============================] - 29s 732us/step - loss: 4.5440e-04 - val_loss: 9.4775e-04\n",
      "Epoch 35/70\n",
      "39176/39176 [==============================] - 29s 728us/step - loss: 4.4773e-04 - val_loss: 9.2318e-04\n",
      "Epoch 36/70\n",
      "39176/39176 [==============================] - 29s 731us/step - loss: 4.4119e-04 - val_loss: 8.6663e-04\n",
      "Epoch 37/70\n",
      "39176/39176 [==============================] - 28s 725us/step - loss: 4.3782e-04 - val_loss: 8.2165e-04\n",
      "Epoch 38/70\n",
      "39176/39176 [==============================] - 28s 725us/step - loss: 4.3325e-04 - val_loss: 7.9760e-04\n",
      "Epoch 39/70\n",
      "39176/39176 [==============================] - 28s 724us/step - loss: 4.2605e-04 - val_loss: 6.9812e-04\n",
      "Epoch 40/70\n",
      "39176/39176 [==============================] - 28s 724us/step - loss: 4.1990e-04 - val_loss: 6.9959e-04\n",
      "Epoch 41/70\n",
      "39176/39176 [==============================] - 28s 727us/step - loss: 4.1010e-04 - val_loss: 6.5997e-04\n",
      "Epoch 42/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 4.0555e-04 - val_loss: 6.1998e-04\n",
      "Epoch 43/70\n",
      "39176/39176 [==============================] - 28s 720us/step - loss: 4.0125e-04 - val_loss: 6.1252e-04\n",
      "Epoch 44/70\n",
      "39176/39176 [==============================] - 29s 729us/step - loss: 3.9766e-04 - val_loss: 5.9071e-04\n",
      "Epoch 45/70\n",
      "39176/39176 [==============================] - 29s 728us/step - loss: 3.9139e-04 - val_loss: 5.7809e-04\n",
      "Epoch 46/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 3.8870e-04 - val_loss: 5.5345e-04\n",
      "Epoch 47/70\n",
      "39176/39176 [==============================] - 28s 724us/step - loss: 3.8513e-04 - val_loss: 5.8333e-04\n",
      "Epoch 48/70\n",
      "39176/39176 [==============================] - 28s 724us/step - loss: 3.7968e-04 - val_loss: 5.5840e-04\n",
      "Epoch 49/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 3.8237e-04 - val_loss: 5.0308e-04\n",
      "Epoch 50/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 3.7246e-04 - val_loss: 5.1963e-04\n",
      "Epoch 51/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 3.7432e-04 - val_loss: 4.9563e-04\n",
      "Epoch 52/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 3.6676e-04 - val_loss: 5.1381e-04\n",
      "Epoch 53/70\n",
      "39176/39176 [==============================] - 28s 725us/step - loss: 3.6155e-04 - val_loss: 4.8303e-04\n",
      "Epoch 54/70\n",
      "39176/39176 [==============================] - 28s 727us/step - loss: 3.6233e-04 - val_loss: 4.8475e-04\n",
      "Epoch 55/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 3.5637e-04 - val_loss: 5.0235e-04\n",
      "Epoch 56/70\n",
      "39176/39176 [==============================] - 28s 724us/step - loss: 3.5409e-04 - val_loss: 4.5056e-04\n",
      "Epoch 57/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 3.5226e-04 - val_loss: 4.2900e-04\n",
      "Epoch 58/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 3.4717e-04 - val_loss: 4.2102e-04\n",
      "Epoch 59/70\n",
      "39176/39176 [==============================] - 28s 720us/step - loss: 3.4714e-04 - val_loss: 4.4140e-04\n",
      "Epoch 60/70\n",
      "39176/39176 [==============================] - 28s 727us/step - loss: 3.4382e-04 - val_loss: 4.0422e-04\n",
      "Epoch 61/70\n",
      "39176/39176 [==============================] - 28s 723us/step - loss: 3.3763e-04 - val_loss: 3.8587e-04\n",
      "Epoch 62/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 3.3578e-04 - val_loss: 4.3691e-04\n",
      "Epoch 63/70\n",
      "39176/39176 [==============================] - 28s 724us/step - loss: 3.3109e-04 - val_loss: 4.1022e-04\n",
      "Epoch 64/70\n",
      "39176/39176 [==============================] - 28s 721us/step - loss: 3.2699e-04 - val_loss: 4.9425e-04\n",
      "Epoch 65/70\n",
      "39176/39176 [==============================] - 28s 725us/step - loss: 3.2885e-04 - val_loss: 4.2562e-04\n",
      "Epoch 66/70\n",
      "39176/39176 [==============================] - 28s 725us/step - loss: 3.2461e-04 - val_loss: 4.4607e-04\n",
      "Epoch 67/70\n",
      "39176/39176 [==============================] - 28s 724us/step - loss: 3.2166e-04 - val_loss: 3.2944e-04\n",
      "Epoch 68/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 3.1637e-04 - val_loss: 4.0079e-04\n",
      "Epoch 69/70\n",
      "39176/39176 [==============================] - 28s 726us/step - loss: 3.1850e-04 - val_loss: 3.8010e-04\n",
      "Epoch 70/70\n",
      "39176/39176 [==============================] - 29s 732us/step - loss: 3.1062e-04 - val_loss: 3.3956e-04\n",
      "Validation RMSE:  0.018427283 \n",
      "\n",
      "Model took 2002.71 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 12\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 446\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 446, 447, 448, 449, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 23s 582us/step - loss: 0.0041 - val_loss: 0.0026\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 6.9361e-04 - val_loss: 8.1970e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 5.7030e-04 - val_loss: 6.0552e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 5.1240e-04 - val_loss: 4.8238e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 4.7569e-04 - val_loss: 4.1778e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 4.4937e-04 - val_loss: 4.0586e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 4.2620e-04 - val_loss: 4.0366e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 4.0517e-04 - val_loss: 4.0935e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.8531e-04 - val_loss: 4.1454e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.6731e-04 - val_loss: 4.1483e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.4904e-04 - val_loss: 4.3172e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 3.3267e-04 - val_loss: 4.2296e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.1592e-04 - val_loss: 3.7628e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.9877e-04 - val_loss: 3.4241e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.8783e-04 - val_loss: 3.2260e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.7689e-04 - val_loss: 2.7517e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.6474e-04 - val_loss: 2.4806e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.5604e-04 - val_loss: 2.3976e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.4932e-04 - val_loss: 2.3402e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.4223e-04 - val_loss: 2.3262e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.3802e-04 - val_loss: 2.1666e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.3118e-04 - val_loss: 2.2323e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.2473e-04 - val_loss: 2.2362e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.2266e-04 - val_loss: 2.0838e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.1898e-04 - val_loss: 2.1196e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.1490e-04 - val_loss: 2.1711e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.1051e-04 - val_loss: 2.1136e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.0600e-04 - val_loss: 2.4045e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.0200e-04 - val_loss: 2.3989e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.9772e-04 - val_loss: 2.0786e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.9354e-04 - val_loss: 1.9914e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.8728e-04 - val_loss: 1.8895e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8163e-04 - val_loss: 1.9699e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.7795e-04 - val_loss: 1.9004e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.7544e-04 - val_loss: 1.8874e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.7221e-04 - val_loss: 1.8559e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.6897e-04 - val_loss: 1.7856e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.6629e-04 - val_loss: 1.6663e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.6418e-04 - val_loss: 1.6922e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.6244e-04 - val_loss: 1.5840e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5963e-04 - val_loss: 1.6167e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5801e-04 - val_loss: 1.5566e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.5609e-04 - val_loss: 1.5260e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5566e-04 - val_loss: 1.4936e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5253e-04 - val_loss: 1.4880e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.5150e-04 - val_loss: 1.4104e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.5020e-04 - val_loss: 1.3887e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 17s 446us/step - loss: 1.4768e-04 - val_loss: 1.3764e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4588e-04 - val_loss: 1.3230e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.4298e-04 - val_loss: 1.4245e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4084e-04 - val_loss: 1.3580e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3950e-04 - val_loss: 1.4294e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3754e-04 - val_loss: 1.4774e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.3551e-04 - val_loss: 1.4769e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3393e-04 - val_loss: 1.5728e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3385e-04 - val_loss: 1.6169e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3415e-04 - val_loss: 1.5839e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3269e-04 - val_loss: 1.5063e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3157e-04 - val_loss: 1.5898e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3197e-04 - val_loss: 1.6374e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3180e-04 - val_loss: 1.5704e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3113e-04 - val_loss: 1.7089e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.3043e-04 - val_loss: 1.4569e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3019e-04 - val_loss: 1.4340e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2907e-04 - val_loss: 1.5826e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2887e-04 - val_loss: 1.5080e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2807e-04 - val_loss: 1.4510e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2824e-04 - val_loss: 1.5123e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2908e-04 - val_loss: 1.4864e-04\n",
      "Validation RMSE:  0.012191638 \n",
      "\n",
      "Model took 1247.62 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 22 19 1\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 1, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1, 1, 1]\n",
      "Lags length:\n",
      " 66\n",
      "Length Used:\n",
      " 22\n",
      "New Dataset shape after Lags: (48978, 67)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  22\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 66)\n",
      "Train X shape after np.reshape (39182, 22, 3)\n",
      "Test X shape after np.reshape (9796, 22, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 25s 630us/step - loss: 0.0054 - val_loss: 0.0034\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 7.8644e-04 - val_loss: 9.5343e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 5.4117e-04 - val_loss: 9.3644e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 4.4967e-04 - val_loss: 4.6465e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 4.0090e-04 - val_loss: 3.8028e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 3.6881e-04 - val_loss: 3.9005e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 3.5045e-04 - val_loss: 4.0745e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 3.3905e-04 - val_loss: 4.1393e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 3.2769e-04 - val_loss: 3.9727e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 3.2432e-04 - val_loss: 3.3702e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 3.1586e-04 - val_loss: 3.3366e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 3.0743e-04 - val_loss: 3.2179e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 3.0235e-04 - val_loss: 3.1626e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 2.9690e-04 - val_loss: 3.2133e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.9166e-04 - val_loss: 3.1220e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.8530e-04 - val_loss: 2.9202e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.7930e-04 - val_loss: 2.7946e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.7274e-04 - val_loss: 2.6860e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.6714e-04 - val_loss: 2.7207e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.6409e-04 - val_loss: 2.5994e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.5769e-04 - val_loss: 2.4715e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.5049e-04 - val_loss: 2.2909e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.3390e-04 - val_loss: 2.1203e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.1889e-04 - val_loss: 2.1218e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.0950e-04 - val_loss: 2.0682e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.0132e-04 - val_loss: 1.9907e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.9366e-04 - val_loss: 1.9039e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.8843e-04 - val_loss: 1.7229e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.8244e-04 - val_loss: 1.6468e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.7581e-04 - val_loss: 1.5456e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.7106e-04 - val_loss: 1.4816e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.6547e-04 - val_loss: 1.5094e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.6066e-04 - val_loss: 1.3416e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.5774e-04 - val_loss: 1.2705e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.5322e-04 - val_loss: 1.1424e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.5069e-04 - val_loss: 1.1023e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.4707e-04 - val_loss: 1.1454e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.4595e-04 - val_loss: 1.0353e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.4445e-04 - val_loss: 9.9860e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4149e-04 - val_loss: 1.0133e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.4079e-04 - val_loss: 9.6596e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.3836e-04 - val_loss: 9.6227e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.3601e-04 - val_loss: 9.5410e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.3436e-04 - val_loss: 9.5929e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.3335e-04 - val_loss: 9.4948e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.3185e-04 - val_loss: 9.4705e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.3024e-04 - val_loss: 9.5253e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.2778e-04 - val_loss: 9.8923e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.2677e-04 - val_loss: 9.9027e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2545e-04 - val_loss: 1.0331e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2405e-04 - val_loss: 9.5903e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.2387e-04 - val_loss: 1.0346e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2302e-04 - val_loss: 1.0552e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.2222e-04 - val_loss: 1.0445e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2016e-04 - val_loss: 1.0907e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.2023e-04 - val_loss: 1.2577e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2026e-04 - val_loss: 1.1429e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1959e-04 - val_loss: 1.1183e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.1947e-04 - val_loss: 1.2455e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.1774e-04 - val_loss: 1.2022e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1617e-04 - val_loss: 1.2372e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.1647e-04 - val_loss: 1.2743e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.1505e-04 - val_loss: 1.3208e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1492e-04 - val_loss: 1.3260e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.1446e-04 - val_loss: 1.2909e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.1410e-04 - val_loss: 1.2806e-04\n",
      "Validation RMSE:  0.011316544 \n",
      "\n",
      "Model took 1287.70 seconds to train\n",
      "9  \t9     \t0.0132146\t0.00472958\t0.00890044\t0.0244921\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 23s 586us/step - loss: 0.0046 - val_loss: 0.0038\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 6.6730e-04 - val_loss: 6.6199e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 5.2712e-04 - val_loss: 6.6060e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 4.6985e-04 - val_loss: 4.1402e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 4.2668e-04 - val_loss: 3.0459e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.9113e-04 - val_loss: 2.8263e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 3.7392e-04 - val_loss: 2.6143e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.5838e-04 - val_loss: 2.6435e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.4846e-04 - val_loss: 2.8142e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.3759e-04 - val_loss: 2.6754e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.2783e-04 - val_loss: 2.6216e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.1769e-04 - val_loss: 2.6749e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.0766e-04 - val_loss: 2.5421e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.9568e-04 - val_loss: 2.6325e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.8395e-04 - val_loss: 2.4728e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.7127e-04 - val_loss: 2.3699e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.5072e-04 - val_loss: 1.9070e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.3968e-04 - val_loss: 1.9780e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.3187e-04 - val_loss: 2.5026e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.2046e-04 - val_loss: 2.6522e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.1235e-04 - val_loss: 1.5852e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.0548e-04 - val_loss: 1.3864e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.9754e-04 - val_loss: 1.3817e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.9041e-04 - val_loss: 1.2805e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.8370e-04 - val_loss: 1.2226e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.7883e-04 - val_loss: 1.1998e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.7535e-04 - val_loss: 1.2332e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.7146e-04 - val_loss: 1.2245e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.6782e-04 - val_loss: 1.2409e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.6482e-04 - val_loss: 1.2284e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.6238e-04 - val_loss: 1.1627e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.5994e-04 - val_loss: 1.0958e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.5778e-04 - val_loss: 1.1026e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5635e-04 - val_loss: 1.1175e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5445e-04 - val_loss: 1.0934e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5249e-04 - val_loss: 1.0680e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5100e-04 - val_loss: 1.0494e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.4826e-04 - val_loss: 1.0121e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4601e-04 - val_loss: 1.0332e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4367e-04 - val_loss: 1.0745e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.4071e-04 - val_loss: 1.0736e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3861e-04 - val_loss: 1.1146e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3748e-04 - val_loss: 1.0812e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3520e-04 - val_loss: 1.0428e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3403e-04 - val_loss: 9.7659e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3183e-04 - val_loss: 9.6297e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3209e-04 - val_loss: 9.6836e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.3085e-04 - val_loss: 9.6473e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2804e-04 - val_loss: 9.7808e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2771e-04 - val_loss: 9.6817e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2546e-04 - val_loss: 8.9250e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2503e-04 - val_loss: 9.1785e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2432e-04 - val_loss: 8.5687e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2342e-04 - val_loss: 8.4915e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2263e-04 - val_loss: 8.4077e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2160e-04 - val_loss: 8.1896e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2119e-04 - val_loss: 8.1958e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2047e-04 - val_loss: 7.9926e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1799e-04 - val_loss: 7.8860e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1810e-04 - val_loss: 8.1789e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1709e-04 - val_loss: 7.7693e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1689e-04 - val_loss: 8.3410e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1608e-04 - val_loss: 8.4882e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1579e-04 - val_loss: 8.0809e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1523e-04 - val_loss: 8.2034e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1446e-04 - val_loss: 8.2437e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1331e-04 - val_loss: 8.5234e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1259e-04 - val_loss: 8.7677e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1179e-04 - val_loss: 8.5121e-05\n",
      "Validation RMSE:  0.009226122 \n",
      "\n",
      "Model took 1252.78 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 23s 592us/step - loss: 0.0062 - val_loss: 0.0048\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 8.3125e-04 - val_loss: 0.0012\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 5.4557e-04 - val_loss: 5.0624e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 4.5881e-04 - val_loss: 5.4023e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 4.1049e-04 - val_loss: 4.5733e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.7229e-04 - val_loss: 3.6370e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.5138e-04 - val_loss: 3.1048e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.2971e-04 - val_loss: 2.8860e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.1508e-04 - val_loss: 2.9049e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 3.0348e-04 - val_loss: 2.8577e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.8904e-04 - val_loss: 2.9073e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.7714e-04 - val_loss: 3.0503e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.6829e-04 - val_loss: 2.9772e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 2.5866e-04 - val_loss: 2.8906e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.5378e-04 - val_loss: 2.9170e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.4784e-04 - val_loss: 2.7839e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.4036e-04 - val_loss: 2.5462e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.3300e-04 - val_loss: 2.5094e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.2475e-04 - val_loss: 2.1860e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 2.1577e-04 - val_loss: 2.2194e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.0964e-04 - val_loss: 2.0221e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.0358e-04 - val_loss: 1.7256e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.9727e-04 - val_loss: 1.6634e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.8993e-04 - val_loss: 1.7502e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.8435e-04 - val_loss: 1.5658e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.7794e-04 - val_loss: 1.4162e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 17s 445us/step - loss: 1.7261e-04 - val_loss: 1.3527e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.6837e-04 - val_loss: 1.2718e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.6598e-04 - val_loss: 1.2600e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.6183e-04 - val_loss: 1.2651e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.5854e-04 - val_loss: 1.2399e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.5583e-04 - val_loss: 1.2504e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 447us/step - loss: 1.5368e-04 - val_loss: 1.2378e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.5106e-04 - val_loss: 1.2791e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4824e-04 - val_loss: 1.2297e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4688e-04 - val_loss: 1.1660e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 448us/step - loss: 1.4393e-04 - val_loss: 1.1668e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.4212e-04 - val_loss: 1.1423e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.4082e-04 - val_loss: 1.1317e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3869e-04 - val_loss: 1.1480e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3708e-04 - val_loss: 1.1816e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3545e-04 - val_loss: 1.2333e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3337e-04 - val_loss: 1.2555e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3046e-04 - val_loss: 1.2457e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2835e-04 - val_loss: 1.4114e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2753e-04 - val_loss: 1.3728e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2607e-04 - val_loss: 1.2745e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2487e-04 - val_loss: 1.3143e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2427e-04 - val_loss: 1.2367e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2280e-04 - val_loss: 1.2213e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2169e-04 - val_loss: 1.2139e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2157e-04 - val_loss: 1.1575e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2045e-04 - val_loss: 1.0993e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2047e-04 - val_loss: 1.0811e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1938e-04 - val_loss: 1.0709e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1845e-04 - val_loss: 1.0553e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1639e-04 - val_loss: 1.3326e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.1743e-04 - val_loss: 8.6814e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1560e-04 - val_loss: 1.0101e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.1634e-04 - val_loss: 9.0873e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1483e-04 - val_loss: 9.1795e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.1462e-04 - val_loss: 9.0485e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1371e-04 - val_loss: 8.8339e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.1206e-04 - val_loss: 8.7697e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.1104e-04 - val_loss: 8.4678e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1017e-04 - val_loss: 8.4127e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.1071e-04 - val_loss: 8.2445e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1029e-04 - val_loss: 8.3318e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1019e-04 - val_loss: 8.3347e-05\n",
      "Validation RMSE:  0.009129452 \n",
      "\n",
      "Model took 1249.02 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 12\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 23s 592us/step - loss: 0.0049 - val_loss: 0.0033\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 6.6508e-04 - val_loss: 5.5565e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 5.4557e-04 - val_loss: 3.8393e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 4.8255e-04 - val_loss: 3.1937e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 4.3866e-04 - val_loss: 2.9228e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 4.1003e-04 - val_loss: 2.9737e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.8869e-04 - val_loss: 2.7229e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.6971e-04 - val_loss: 2.8365e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.5738e-04 - val_loss: 2.9235e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.4190e-04 - val_loss: 2.8914e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.3369e-04 - val_loss: 2.8342e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.2212e-04 - val_loss: 3.3738e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.1151e-04 - val_loss: 3.2489e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.0276e-04 - val_loss: 3.2236e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.9157e-04 - val_loss: 3.2919e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.8109e-04 - val_loss: 3.1027e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.7474e-04 - val_loss: 2.8221e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.6271e-04 - val_loss: 2.6507e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.5085e-04 - val_loss: 2.0642e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.4499e-04 - val_loss: 1.5949e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.3079e-04 - val_loss: 1.2647e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.2253e-04 - val_loss: 1.2044e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.1347e-04 - val_loss: 1.2760e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.0779e-04 - val_loss: 1.0965e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.9937e-04 - val_loss: 1.0376e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.9213e-04 - val_loss: 9.9553e-05\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8673e-04 - val_loss: 1.0511e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8259e-04 - val_loss: 1.0405e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.7901e-04 - val_loss: 1.0363e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.7650e-04 - val_loss: 9.9528e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.7437e-04 - val_loss: 1.0410e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.7161e-04 - val_loss: 1.0382e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.6885e-04 - val_loss: 1.0510e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.6613e-04 - val_loss: 1.0319e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.6181e-04 - val_loss: 1.0301e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.6187e-04 - val_loss: 1.1055e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5841e-04 - val_loss: 1.0474e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5614e-04 - val_loss: 1.0968e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.5274e-04 - val_loss: 1.0313e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.5017e-04 - val_loss: 9.6771e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.4736e-04 - val_loss: 9.9186e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4499e-04 - val_loss: 1.0266e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.4298e-04 - val_loss: 1.0326e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4138e-04 - val_loss: 1.0626e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3748e-04 - val_loss: 9.8890e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3777e-04 - val_loss: 1.0069e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3441e-04 - val_loss: 9.4443e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3653e-04 - val_loss: 9.3692e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3495e-04 - val_loss: 8.8517e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3266e-04 - val_loss: 9.4029e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2906e-04 - val_loss: 8.9339e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2967e-04 - val_loss: 9.3423e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2781e-04 - val_loss: 8.5475e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2619e-04 - val_loss: 8.7794e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2503e-04 - val_loss: 8.5674e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2370e-04 - val_loss: 8.6756e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2273e-04 - val_loss: 8.9990e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2284e-04 - val_loss: 8.7949e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2005e-04 - val_loss: 8.6882e-05\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1898e-04 - val_loss: 8.5822e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1735e-04 - val_loss: 8.8833e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1537e-04 - val_loss: 8.6795e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1311e-04 - val_loss: 9.0106e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1204e-04 - val_loss: 8.9723e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1233e-04 - val_loss: 8.4405e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1097e-04 - val_loss: 8.4943e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1003e-04 - val_loss: 8.5299e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.0944e-04 - val_loss: 8.7854e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.0863e-04 - val_loss: 8.6346e-05\n",
      "Validation RMSE:  0.009292238 \n",
      "\n",
      "Model took 1256.40 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 1\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 23s 598us/step - loss: 0.0063 - val_loss: 0.0038\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 6.5509e-04 - val_loss: 4.3019e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 5.1988e-04 - val_loss: 3.8734e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 4.5919e-04 - val_loss: 4.0284e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 4.1296e-04 - val_loss: 3.5803e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.8099e-04 - val_loss: 3.0797e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.5720e-04 - val_loss: 2.9399e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.4110e-04 - val_loss: 2.7755e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.2377e-04 - val_loss: 2.8945e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.1424e-04 - val_loss: 2.8709e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.0106e-04 - val_loss: 2.7171e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.8966e-04 - val_loss: 2.6677e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.8145e-04 - val_loss: 2.4817e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.7472e-04 - val_loss: 2.3719e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.6692e-04 - val_loss: 2.3224e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.5605e-04 - val_loss: 2.4470e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.4700e-04 - val_loss: 2.2763e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.4172e-04 - val_loss: 2.1858e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.3430e-04 - val_loss: 2.0232e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.2500e-04 - val_loss: 2.0701e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.1501e-04 - val_loss: 1.8642e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.0765e-04 - val_loss: 1.6462e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.0030e-04 - val_loss: 1.6030e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.9053e-04 - val_loss: 1.6296e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8591e-04 - val_loss: 1.8344e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.7983e-04 - val_loss: 1.4936e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.7350e-04 - val_loss: 1.3776e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.6830e-04 - val_loss: 1.4770e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.6402e-04 - val_loss: 1.5267e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.6177e-04 - val_loss: 1.3805e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5870e-04 - val_loss: 1.3621e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5536e-04 - val_loss: 1.1911e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.5304e-04 - val_loss: 1.2396e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4949e-04 - val_loss: 1.1494e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.4696e-04 - val_loss: 1.2577e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.4481e-04 - val_loss: 1.1769e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4260e-04 - val_loss: 1.1617e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.4028e-04 - val_loss: 1.2083e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3855e-04 - val_loss: 1.1545e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3616e-04 - val_loss: 1.1304e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3471e-04 - val_loss: 1.1089e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3349e-04 - val_loss: 1.1145e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3204e-04 - val_loss: 1.0703e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3025e-04 - val_loss: 1.0512e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2924e-04 - val_loss: 1.0451e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2730e-04 - val_loss: 1.0373e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2656e-04 - val_loss: 1.0336e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2575e-04 - val_loss: 1.0221e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2500e-04 - val_loss: 1.0093e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2364e-04 - val_loss: 9.6891e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2319e-04 - val_loss: 1.0084e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2238e-04 - val_loss: 1.0159e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2172e-04 - val_loss: 1.0112e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2175e-04 - val_loss: 9.9052e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.2133e-04 - val_loss: 9.6989e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2056e-04 - val_loss: 9.4837e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2037e-04 - val_loss: 9.7075e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1961e-04 - val_loss: 9.4552e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1896e-04 - val_loss: 9.4772e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1825e-04 - val_loss: 9.7089e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1744e-04 - val_loss: 1.0030e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1742e-04 - val_loss: 1.0398e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1733e-04 - val_loss: 1.0145e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1727e-04 - val_loss: 1.0418e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1656e-04 - val_loss: 1.0298e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1636e-04 - val_loss: 1.0535e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1507e-04 - val_loss: 1.0064e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1485e-04 - val_loss: 1.0488e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1418e-04 - val_loss: 1.0745e-04\n",
      "Validation RMSE:  0.010365574 \n",
      "\n",
      "Model took 1255.48 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 23s 597us/step - loss: 0.0052 - val_loss: 0.0025\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 0.0011 - val_loss: 0.0010\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 6.6569e-04 - val_loss: 5.1866e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 5.5842e-04 - val_loss: 3.3857e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 4.9042e-04 - val_loss: 3.4297e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 4.4324e-04 - val_loss: 3.1501e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 4.0672e-04 - val_loss: 2.9516e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.8273e-04 - val_loss: 2.9611e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.6383e-04 - val_loss: 2.9855e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.4832e-04 - val_loss: 2.9158e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.3733e-04 - val_loss: 3.0140e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.2693e-04 - val_loss: 2.8655e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.1477e-04 - val_loss: 2.6750e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 3.0462e-04 - val_loss: 2.6456e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.9425e-04 - val_loss: 2.3919e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.8385e-04 - val_loss: 2.4666e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.7413e-04 - val_loss: 2.2271e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.6316e-04 - val_loss: 1.9330e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.4926e-04 - val_loss: 1.7429e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.3467e-04 - val_loss: 2.3270e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.2566e-04 - val_loss: 1.8561e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.1218e-04 - val_loss: 1.7504e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.0688e-04 - val_loss: 1.3424e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.9660e-04 - val_loss: 1.2305e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.8814e-04 - val_loss: 1.1851e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.8157e-04 - val_loss: 1.2515e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.7670e-04 - val_loss: 1.1264e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.7293e-04 - val_loss: 1.0338e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6906e-04 - val_loss: 1.0065e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6585e-04 - val_loss: 9.8341e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.6257e-04 - val_loss: 9.7559e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5948e-04 - val_loss: 9.8576e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5654e-04 - val_loss: 9.8344e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5383e-04 - val_loss: 9.7918e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5144e-04 - val_loss: 1.0031e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.4899e-04 - val_loss: 9.8184e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4569e-04 - val_loss: 9.9095e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4345e-04 - val_loss: 9.8029e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4219e-04 - val_loss: 9.6071e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4070e-04 - val_loss: 9.7478e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3920e-04 - val_loss: 1.0018e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3755e-04 - val_loss: 9.9560e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3604e-04 - val_loss: 9.7156e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3417e-04 - val_loss: 9.7823e-05\n",
      "Epoch 45/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3322e-04 - val_loss: 9.5878e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3126e-04 - val_loss: 9.6873e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2930e-04 - val_loss: 9.6346e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2925e-04 - val_loss: 1.0014e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2812e-04 - val_loss: 9.7768e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2727e-04 - val_loss: 1.0443e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2708e-04 - val_loss: 1.0719e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2609e-04 - val_loss: 1.0737e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2515e-04 - val_loss: 1.0374e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2409e-04 - val_loss: 1.0217e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2354e-04 - val_loss: 9.9941e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2247e-04 - val_loss: 9.8910e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2147e-04 - val_loss: 9.4967e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2059e-04 - val_loss: 9.5554e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1981e-04 - val_loss: 9.3903e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1870e-04 - val_loss: 9.3918e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1815e-04 - val_loss: 9.6155e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1704e-04 - val_loss: 9.4595e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1613e-04 - val_loss: 9.5158e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1527e-04 - val_loss: 9.4664e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1449e-04 - val_loss: 9.6557e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1313e-04 - val_loss: 9.4269e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1262e-04 - val_loss: 9.4197e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1202e-04 - val_loss: 9.4533e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1039e-04 - val_loss: 8.7915e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1156e-04 - val_loss: 9.4736e-05\n",
      "Validation RMSE:  0.009733244 \n",
      "\n",
      "Model took 1260.10 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 24s 603us/step - loss: 0.0059 - val_loss: 0.0036\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 6.9900e-04 - val_loss: 5.3935e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 5.4317e-04 - val_loss: 3.0400e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 4.7967e-04 - val_loss: 3.1413e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 4.2592e-04 - val_loss: 2.7235e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.9635e-04 - val_loss: 2.7833e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.6903e-04 - val_loss: 2.6509e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.5070e-04 - val_loss: 2.6392e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.3956e-04 - val_loss: 2.5187e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.2824e-04 - val_loss: 2.4509e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.1710e-04 - val_loss: 2.3086e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.0559e-04 - val_loss: 2.2483e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.9724e-04 - val_loss: 2.3032e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.8999e-04 - val_loss: 2.2063e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.8151e-04 - val_loss: 2.0861e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.7339e-04 - val_loss: 2.0902e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 2.6384e-04 - val_loss: 2.1325e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.5536e-04 - val_loss: 2.0367e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 2.4773e-04 - val_loss: 2.1150e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.3486e-04 - val_loss: 2.0750e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.2637e-04 - val_loss: 1.9736e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.1847e-04 - val_loss: 1.6368e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.0920e-04 - val_loss: 1.7388e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.0345e-04 - val_loss: 1.5750e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.9354e-04 - val_loss: 1.2475e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.8715e-04 - val_loss: 1.1506e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.8239e-04 - val_loss: 1.1503e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7850e-04 - val_loss: 1.0941e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7465e-04 - val_loss: 1.0003e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7079e-04 - val_loss: 9.4259e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.6788e-04 - val_loss: 9.2679e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6563e-04 - val_loss: 9.8783e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.6302e-04 - val_loss: 1.0030e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6054e-04 - val_loss: 1.0275e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.5767e-04 - val_loss: 9.8597e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.5489e-04 - val_loss: 9.8467e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5227e-04 - val_loss: 9.5028e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4969e-04 - val_loss: 9.6542e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.4696e-04 - val_loss: 9.3902e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.4566e-04 - val_loss: 9.5786e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4455e-04 - val_loss: 9.2832e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4280e-04 - val_loss: 9.3493e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4120e-04 - val_loss: 9.4494e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3989e-04 - val_loss: 9.4578e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3851e-04 - val_loss: 9.2226e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3665e-04 - val_loss: 8.6167e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.3548e-04 - val_loss: 8.9309e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3475e-04 - val_loss: 8.7345e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3295e-04 - val_loss: 8.8214e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3122e-04 - val_loss: 8.9838e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3014e-04 - val_loss: 8.9076e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2799e-04 - val_loss: 8.4145e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.2846e-04 - val_loss: 9.2685e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 450us/step - loss: 1.2768e-04 - val_loss: 1.0022e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2627e-04 - val_loss: 9.7980e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 449us/step - loss: 1.2503e-04 - val_loss: 9.5039e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2376e-04 - val_loss: 9.2431e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2247e-04 - val_loss: 9.0805e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2176e-04 - val_loss: 8.8657e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2120e-04 - val_loss: 8.8489e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2035e-04 - val_loss: 8.7192e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1935e-04 - val_loss: 8.4713e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.1892e-04 - val_loss: 8.4405e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1809e-04 - val_loss: 8.3389e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1781e-04 - val_loss: 8.1174e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1692e-04 - val_loss: 8.3710e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1645e-04 - val_loss: 8.6189e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1562e-04 - val_loss: 8.6977e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1370e-04 - val_loss: 8.5049e-05\n",
      "Validation RMSE:  0.0092221815 \n",
      "\n",
      "Model took 1256.78 seconds to train\n",
      "10 \t6     \t0.00933313\t0.000442972\t0.00890044\t0.0103656\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 24s 606us/step - loss: 0.0061 - val_loss: 0.0043\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 7.9741e-04 - val_loss: 7.0343e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 5.5224e-04 - val_loss: 4.1643e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 4.6216e-04 - val_loss: 4.3078e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 4.2629e-04 - val_loss: 3.1146e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.9913e-04 - val_loss: 2.9616e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.7830e-04 - val_loss: 2.8779e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.6321e-04 - val_loss: 2.8059e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.4723e-04 - val_loss: 2.7812e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.3562e-04 - val_loss: 2.7526e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.2424e-04 - val_loss: 2.6643e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.1163e-04 - val_loss: 2.5164e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.0295e-04 - val_loss: 2.4142e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.9525e-04 - val_loss: 2.2942e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.8691e-04 - val_loss: 2.2515e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.7474e-04 - val_loss: 2.3343e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.6569e-04 - val_loss: 2.1004e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.4855e-04 - val_loss: 1.9546e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.4380e-04 - val_loss: 1.6907e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.3526e-04 - val_loss: 1.6351e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.3078e-04 - val_loss: 1.9822e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.2586e-04 - val_loss: 1.6284e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.1411e-04 - val_loss: 1.8468e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.0597e-04 - val_loss: 1.8216e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.9967e-04 - val_loss: 1.3474e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.9053e-04 - val_loss: 1.1074e-04\n",
      "Epoch 28/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8340e-04 - val_loss: 1.0749e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.7781e-04 - val_loss: 1.0800e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7394e-04 - val_loss: 1.0679e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.7059e-04 - val_loss: 1.0595e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.6648e-04 - val_loss: 1.0457e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.6529e-04 - val_loss: 9.9956e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.6160e-04 - val_loss: 9.9695e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.5890e-04 - val_loss: 1.0423e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5790e-04 - val_loss: 1.0075e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.5601e-04 - val_loss: 9.8624e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5114e-04 - val_loss: 9.5311e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4951e-04 - val_loss: 9.7292e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.4847e-04 - val_loss: 9.4102e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4760e-04 - val_loss: 9.4451e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.4576e-04 - val_loss: 8.6498e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4141e-04 - val_loss: 8.6154e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.4116e-04 - val_loss: 8.5973e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3812e-04 - val_loss: 8.7834e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3788e-04 - val_loss: 8.7866e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3588e-04 - val_loss: 9.3055e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3453e-04 - val_loss: 9.5686e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3307e-04 - val_loss: 9.9561e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3247e-04 - val_loss: 9.4935e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3185e-04 - val_loss: 8.6200e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2842e-04 - val_loss: 8.7598e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2829e-04 - val_loss: 8.5811e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2594e-04 - val_loss: 8.9776e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2629e-04 - val_loss: 8.7974e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2538e-04 - val_loss: 8.5812e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2310e-04 - val_loss: 8.6282e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2246e-04 - val_loss: 8.8713e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2140e-04 - val_loss: 9.6976e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2258e-04 - val_loss: 8.8746e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2101e-04 - val_loss: 9.0725e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1990e-04 - val_loss: 9.2748e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1948e-04 - val_loss: 9.3421e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1966e-04 - val_loss: 9.1056e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1861e-04 - val_loss: 9.1701e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1897e-04 - val_loss: 9.1504e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1731e-04 - val_loss: 8.6616e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1644e-04 - val_loss: 8.5066e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1686e-04 - val_loss: 8.6698e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1610e-04 - val_loss: 8.3849e-05\n",
      "Validation RMSE:  0.009156924 \n",
      "\n",
      "Model took 1260.07 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 24s 607us/step - loss: 0.0056 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 6.3126e-04 - val_loss: 4.7121e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 4.9837e-04 - val_loss: 3.7925e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 4.3459e-04 - val_loss: 3.8987e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 3.9475e-04 - val_loss: 3.3580e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.6204e-04 - val_loss: 2.8268e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.3882e-04 - val_loss: 2.5108e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.2130e-04 - val_loss: 2.5099e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.0619e-04 - val_loss: 2.5860e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.9455e-04 - val_loss: 2.4833e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.8412e-04 - val_loss: 2.4857e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.7501e-04 - val_loss: 2.5168e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.6416e-04 - val_loss: 2.6021e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.5406e-04 - val_loss: 2.6749e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.4591e-04 - val_loss: 2.4890e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.4051e-04 - val_loss: 2.5831e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.3549e-04 - val_loss: 2.5474e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.3012e-04 - val_loss: 2.5987e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.2621e-04 - val_loss: 2.3095e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.1956e-04 - val_loss: 2.3363e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.1196e-04 - val_loss: 1.4797e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.0104e-04 - val_loss: 1.9511e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.9782e-04 - val_loss: 1.8022e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.8565e-04 - val_loss: 1.5492e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7793e-04 - val_loss: 1.3576e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.7311e-04 - val_loss: 1.1690e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6826e-04 - val_loss: 1.1647e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6297e-04 - val_loss: 1.0657e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5737e-04 - val_loss: 1.0450e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5234e-04 - val_loss: 1.0386e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.4977e-04 - val_loss: 9.9056e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4747e-04 - val_loss: 9.5468e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4535e-04 - val_loss: 9.3562e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4313e-04 - val_loss: 9.1997e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.4037e-04 - val_loss: 8.7790e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3938e-04 - val_loss: 9.8880e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3791e-04 - val_loss: 9.8843e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3641e-04 - val_loss: 9.6193e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3536e-04 - val_loss: 9.5914e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3395e-04 - val_loss: 9.2705e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3332e-04 - val_loss: 9.3289e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3274e-04 - val_loss: 9.8939e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3207e-04 - val_loss: 9.4028e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3049e-04 - val_loss: 9.3795e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3036e-04 - val_loss: 9.6711e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2961e-04 - val_loss: 9.5236e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2803e-04 - val_loss: 9.7658e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2728e-04 - val_loss: 9.5621e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2770e-04 - val_loss: 1.0837e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2675e-04 - val_loss: 1.0763e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2601e-04 - val_loss: 1.0891e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2479e-04 - val_loss: 1.1426e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2410e-04 - val_loss: 1.1034e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2455e-04 - val_loss: 1.0243e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2291e-04 - val_loss: 1.0354e-04\n",
      "Validation RMSE:  0.010175361 \n",
      "\n",
      "Model took 1012.52 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 24s 611us/step - loss: 0.0045 - val_loss: 0.0028\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 6.4020e-04 - val_loss: 5.4246e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 5.2673e-04 - val_loss: 4.1069e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 4.6526e-04 - val_loss: 3.5598e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 4.1570e-04 - val_loss: 3.5849e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.8779e-04 - val_loss: 3.1475e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.6205e-04 - val_loss: 2.9195e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.4467e-04 - val_loss: 2.5446e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.2990e-04 - val_loss: 2.4467e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.2047e-04 - val_loss: 2.3244e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 3.0853e-04 - val_loss: 2.3825e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.0051e-04 - val_loss: 2.2867e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.9069e-04 - val_loss: 2.4971e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.8366e-04 - val_loss: 2.3208e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.7392e-04 - val_loss: 2.3256e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.6411e-04 - val_loss: 2.2113e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.5650e-04 - val_loss: 2.1272e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.4624e-04 - val_loss: 2.1046e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.2901e-04 - val_loss: 2.3320e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.2171e-04 - val_loss: 1.8095e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.1924e-04 - val_loss: 1.7044e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.0971e-04 - val_loss: 1.7627e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.9869e-04 - val_loss: 1.5838e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8953e-04 - val_loss: 1.3595e-04\n",
      "Epoch 26/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.8253e-04 - val_loss: 1.1941e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.7829e-04 - val_loss: 1.3346e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7394e-04 - val_loss: 1.3355e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.6980e-04 - val_loss: 1.3751e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.6665e-04 - val_loss: 1.3803e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6365e-04 - val_loss: 1.3158e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.6146e-04 - val_loss: 1.3193e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5916e-04 - val_loss: 1.2746e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5715e-04 - val_loss: 1.2945e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5534e-04 - val_loss: 1.2452e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.5354e-04 - val_loss: 1.2565e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5178e-04 - val_loss: 1.2078e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5021e-04 - val_loss: 1.1624e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4853e-04 - val_loss: 1.1401e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.4670e-04 - val_loss: 1.0335e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4503e-04 - val_loss: 1.0073e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4317e-04 - val_loss: 1.0151e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4257e-04 - val_loss: 9.4706e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.4079e-04 - val_loss: 9.4731e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3960e-04 - val_loss: 9.4774e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3887e-04 - val_loss: 9.7080e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3782e-04 - val_loss: 9.4466e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3534e-04 - val_loss: 9.1828e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3662e-04 - val_loss: 9.1453e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3571e-04 - val_loss: 9.3194e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3381e-04 - val_loss: 9.2453e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3232e-04 - val_loss: 9.0413e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.3196e-04 - val_loss: 8.7895e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3037e-04 - val_loss: 8.9884e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2844e-04 - val_loss: 9.2407e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2849e-04 - val_loss: 9.4623e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2583e-04 - val_loss: 9.0575e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2630e-04 - val_loss: 9.5757e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2399e-04 - val_loss: 9.3646e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2457e-04 - val_loss: 9.6088e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2435e-04 - val_loss: 9.2538e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2263e-04 - val_loss: 9.5427e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2163e-04 - val_loss: 9.2704e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2137e-04 - val_loss: 9.5616e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2107e-04 - val_loss: 9.4741e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2047e-04 - val_loss: 9.4177e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1949e-04 - val_loss: 9.2497e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1897e-04 - val_loss: 9.3348e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1821e-04 - val_loss: 9.0955e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1795e-04 - val_loss: 9.0398e-05\n",
      "Validation RMSE:  0.0095077865 \n",
      "\n",
      "Model took 1261.77 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 24s 615us/step - loss: 0.0086 - val_loss: 0.0046\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 7.4170e-04 - val_loss: 0.0012\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 5.3676e-04 - val_loss: 5.3712e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 4.5456e-04 - val_loss: 3.9907e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 4.0206e-04 - val_loss: 4.0483e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.6730e-04 - val_loss: 3.2852e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.4405e-04 - val_loss: 2.9290e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.2637e-04 - val_loss: 2.6997e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.1266e-04 - val_loss: 2.5485e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.0231e-04 - val_loss: 2.5360e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.9353e-04 - val_loss: 2.4942e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.8315e-04 - val_loss: 2.5020e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.7583e-04 - val_loss: 2.4367e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.6619e-04 - val_loss: 2.4462e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.5860e-04 - val_loss: 2.3677e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.5092e-04 - val_loss: 2.2839e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.4288e-04 - val_loss: 2.4575e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.3376e-04 - val_loss: 2.5881e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.2386e-04 - val_loss: 2.5435e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.1642e-04 - val_loss: 2.3606e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.1362e-04 - val_loss: 2.2714e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.0978e-04 - val_loss: 1.7652e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.9923e-04 - val_loss: 1.6709e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.9042e-04 - val_loss: 1.6096e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.8391e-04 - val_loss: 1.2867e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.7727e-04 - val_loss: 1.1877e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6880e-04 - val_loss: 1.1482e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.6140e-04 - val_loss: 1.0300e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5480e-04 - val_loss: 9.9040e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5038e-04 - val_loss: 9.2618e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4582e-04 - val_loss: 9.0542e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4247e-04 - val_loss: 8.8392e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4003e-04 - val_loss: 8.8264e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3807e-04 - val_loss: 8.7860e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3599e-04 - val_loss: 8.9121e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3348e-04 - val_loss: 8.5830e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3222e-04 - val_loss: 9.0253e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3128e-04 - val_loss: 9.0168e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2925e-04 - val_loss: 8.3705e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2756e-04 - val_loss: 8.7663e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2549e-04 - val_loss: 8.5806e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2475e-04 - val_loss: 8.3111e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2344e-04 - val_loss: 8.3135e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2190e-04 - val_loss: 8.3376e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2075e-04 - val_loss: 8.2898e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1889e-04 - val_loss: 8.4019e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1810e-04 - val_loss: 8.3984e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1629e-04 - val_loss: 8.7631e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1608e-04 - val_loss: 8.3508e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1272e-04 - val_loss: 9.7819e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1407e-04 - val_loss: 8.2108e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1207e-04 - val_loss: 8.4280e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1207e-04 - val_loss: 8.2601e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.0966e-04 - val_loss: 9.0968e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1001e-04 - val_loss: 7.8061e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.0713e-04 - val_loss: 8.1267e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.0816e-04 - val_loss: 8.2230e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.0746e-04 - val_loss: 8.0132e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.0645e-04 - val_loss: 8.2648e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.0567e-04 - val_loss: 8.2208e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.0430e-04 - val_loss: 7.6806e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.0441e-04 - val_loss: 8.1349e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.0352e-04 - val_loss: 8.0143e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.0250e-04 - val_loss: 7.3834e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.0174e-04 - val_loss: 7.8980e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.0226e-04 - val_loss: 8.0110e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.0081e-04 - val_loss: 7.8071e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.0067e-04 - val_loss: 7.8550e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 9.9071e-05 - val_loss: 7.6533e-05\n",
      "Validation RMSE:  0.008748299 \n",
      "\n",
      "Model took 1268.19 seconds to train\n",
      "11 \t4     \t0.00913167\t0.00040677 \t0.0087483 \t0.0101754\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 24s 616us/step - loss: 0.0045 - val_loss: 0.0026\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 9.8842e-04 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 6.2034e-04 - val_loss: 4.6479e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 5.2865e-04 - val_loss: 3.4847e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 4.6386e-04 - val_loss: 2.7920e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 4.1615e-04 - val_loss: 2.7330e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.8053e-04 - val_loss: 2.7755e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.5491e-04 - val_loss: 2.6122e-04\n",
      "Epoch 9/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.3626e-04 - val_loss: 2.5660e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.2432e-04 - val_loss: 2.5073e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.1393e-04 - val_loss: 2.4230e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.0182e-04 - val_loss: 2.5962e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.9388e-04 - val_loss: 2.7564e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.8481e-04 - val_loss: 2.8247e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.7920e-04 - val_loss: 2.3204e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.7199e-04 - val_loss: 2.3654e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.6156e-04 - val_loss: 2.6135e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.5382e-04 - val_loss: 2.2125e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.4656e-04 - val_loss: 2.0837e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.4402e-04 - val_loss: 2.0253e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.3670e-04 - val_loss: 2.2338e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.3326e-04 - val_loss: 2.1322e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.2818e-04 - val_loss: 2.0625e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.2390e-04 - val_loss: 1.8705e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.1806e-04 - val_loss: 1.7988e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.0967e-04 - val_loss: 1.7391e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.0412e-04 - val_loss: 1.6919e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.0256e-04 - val_loss: 1.5613e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.0041e-04 - val_loss: 1.5562e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.9083e-04 - val_loss: 1.4147e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.8354e-04 - val_loss: 1.2994e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.7546e-04 - val_loss: 1.3743e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.7001e-04 - val_loss: 1.2644e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.6404e-04 - val_loss: 1.2413e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5919e-04 - val_loss: 1.1982e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5570e-04 - val_loss: 1.1807e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5159e-04 - val_loss: 1.1343e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4837e-04 - val_loss: 1.1010e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.4555e-04 - val_loss: 1.1359e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4264e-04 - val_loss: 1.1229e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4027e-04 - val_loss: 1.1642e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3811e-04 - val_loss: 1.1350e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3665e-04 - val_loss: 1.1214e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3472e-04 - val_loss: 1.1853e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3161e-04 - val_loss: 1.2045e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3068e-04 - val_loss: 1.1045e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2913e-04 - val_loss: 1.1245e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2763e-04 - val_loss: 1.1585e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2710e-04 - val_loss: 1.0778e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2491e-04 - val_loss: 1.0962e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2341e-04 - val_loss: 1.0707e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2261e-04 - val_loss: 1.0862e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2136e-04 - val_loss: 1.0630e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1994e-04 - val_loss: 1.0581e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1911e-04 - val_loss: 1.0588e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1815e-04 - val_loss: 1.0333e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1765e-04 - val_loss: 1.0936e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1731e-04 - val_loss: 1.0473e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1640e-04 - val_loss: 1.1116e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1572e-04 - val_loss: 1.0441e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1457e-04 - val_loss: 9.9868e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.1382e-04 - val_loss: 1.0095e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1332e-04 - val_loss: 1.0505e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1294e-04 - val_loss: 9.8840e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1262e-04 - val_loss: 9.7166e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1194e-04 - val_loss: 9.0711e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1114e-04 - val_loss: 9.2021e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.0953e-04 - val_loss: 8.7653e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.0880e-04 - val_loss: 8.9851e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 1.0765e-04 - val_loss: 8.7573e-05\n",
      "Validation RMSE:  0.009358038 \n",
      "\n",
      "Model took 1263.34 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 24s 622us/step - loss: 0.0059 - val_loss: 0.0029\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 0.0011 - val_loss: 9.6792e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 6.0670e-04 - val_loss: 5.0931e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 4.9256e-04 - val_loss: 3.2108e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 4.3065e-04 - val_loss: 3.5139e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.9360e-04 - val_loss: 3.5636e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.6698e-04 - val_loss: 3.1057e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.4756e-04 - val_loss: 2.8869e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.3171e-04 - val_loss: 2.6659e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.2072e-04 - val_loss: 2.4863e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.0957e-04 - val_loss: 2.3226e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.0221e-04 - val_loss: 2.3434e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.9320e-04 - val_loss: 2.4108e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.8446e-04 - val_loss: 2.3521e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.7271e-04 - val_loss: 2.3268e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.6316e-04 - val_loss: 2.2166e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.5549e-04 - val_loss: 2.1634e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.4487e-04 - val_loss: 2.1694e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.3311e-04 - val_loss: 2.2359e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.2562e-04 - val_loss: 1.9837e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.1699e-04 - val_loss: 1.7540e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.0807e-04 - val_loss: 1.7655e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.0166e-04 - val_loss: 1.6243e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.9433e-04 - val_loss: 1.5853e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.8729e-04 - val_loss: 1.5053e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.8182e-04 - val_loss: 1.3826e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7658e-04 - val_loss: 1.3187e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.7277e-04 - val_loss: 1.2395e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6954e-04 - val_loss: 1.2375e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6625e-04 - val_loss: 1.1688e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.6349e-04 - val_loss: 1.1177e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.6091e-04 - val_loss: 1.0977e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5786e-04 - val_loss: 1.0644e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.5547e-04 - val_loss: 1.0346e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5323e-04 - val_loss: 1.0031e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5129e-04 - val_loss: 9.8665e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4904e-04 - val_loss: 1.0090e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4703e-04 - val_loss: 1.0157e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4537e-04 - val_loss: 9.9636e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4314e-04 - val_loss: 1.0105e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4120e-04 - val_loss: 1.0106e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3965e-04 - val_loss: 1.0417e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3772e-04 - val_loss: 9.7247e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3603e-04 - val_loss: 9.7504e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3301e-04 - val_loss: 9.5288e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3145e-04 - val_loss: 9.7883e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3100e-04 - val_loss: 9.7443e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2956e-04 - val_loss: 9.8553e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2825e-04 - val_loss: 1.0119e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2713e-04 - val_loss: 9.7269e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2613e-04 - val_loss: 9.6712e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2493e-04 - val_loss: 9.6256e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2402e-04 - val_loss: 9.6459e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2341e-04 - val_loss: 9.3359e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2231e-04 - val_loss: 9.2376e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2145e-04 - val_loss: 8.8549e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2052e-04 - val_loss: 8.8310e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1930e-04 - val_loss: 8.7025e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1852e-04 - val_loss: 8.2357e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1762e-04 - val_loss: 8.2140e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1660e-04 - val_loss: 8.0981e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1634e-04 - val_loss: 7.9724e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1528e-04 - val_loss: 7.8107e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1468e-04 - val_loss: 7.8167e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1380e-04 - val_loss: 7.8535e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1216e-04 - val_loss: 7.5839e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1222e-04 - val_loss: 7.7560e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.1042e-04 - val_loss: 8.1647e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.0920e-04 - val_loss: 7.8482e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.0792e-04 - val_loss: 7.6088e-05\n",
      "Validation RMSE:  0.0087228445 \n",
      "\n",
      "Model took 1266.49 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 24s 621us/step - loss: 0.0056 - val_loss: 0.0038\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 6.3069e-04 - val_loss: 7.2422e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 4.7652e-04 - val_loss: 4.1933e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 4.1505e-04 - val_loss: 3.3969e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 3.7750e-04 - val_loss: 2.9804e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.5322e-04 - val_loss: 2.7119e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.3821e-04 - val_loss: 2.5682e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.2491e-04 - val_loss: 2.6119e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.1368e-04 - val_loss: 2.4957e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 3.0406e-04 - val_loss: 2.4468e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.9520e-04 - val_loss: 2.4651e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.8959e-04 - val_loss: 2.4049e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.8056e-04 - val_loss: 2.4689e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.7384e-04 - val_loss: 2.5492e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.6725e-04 - val_loss: 2.6109e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.6036e-04 - val_loss: 2.6638e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.5147e-04 - val_loss: 2.6270e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.4146e-04 - val_loss: 2.5438e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.2199e-04 - val_loss: 1.9773e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.0559e-04 - val_loss: 2.2306e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.9819e-04 - val_loss: 2.0213e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.8801e-04 - val_loss: 1.5407e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7653e-04 - val_loss: 1.3311e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.6969e-04 - val_loss: 1.0857e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6323e-04 - val_loss: 1.0680e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5972e-04 - val_loss: 1.0355e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5682e-04 - val_loss: 1.0219e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.5437e-04 - val_loss: 1.0362e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5113e-04 - val_loss: 1.0606e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4839e-04 - val_loss: 1.0242e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4533e-04 - val_loss: 1.0214e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4304e-04 - val_loss: 1.1101e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4107e-04 - val_loss: 9.4276e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4146e-04 - val_loss: 9.3840e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3782e-04 - val_loss: 9.2275e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3673e-04 - val_loss: 9.5382e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3533e-04 - val_loss: 9.6051e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3421e-04 - val_loss: 9.5243e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3299e-04 - val_loss: 9.7603e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3086e-04 - val_loss: 9.9473e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3028e-04 - val_loss: 1.0046e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2870e-04 - val_loss: 1.0030e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2856e-04 - val_loss: 1.0119e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2748e-04 - val_loss: 1.0254e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2669e-04 - val_loss: 1.0297e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2553e-04 - val_loss: 1.0643e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.2505e-04 - val_loss: 1.0667e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2386e-04 - val_loss: 1.0366e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2275e-04 - val_loss: 1.0452e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2210e-04 - val_loss: 1.0731e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2242e-04 - val_loss: 1.1124e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1888e-04 - val_loss: 1.1751e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2043e-04 - val_loss: 1.0801e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1980e-04 - val_loss: 1.0168e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1726e-04 - val_loss: 9.5690e-05\n",
      "Validation RMSE:  0.009782114 \n",
      "\n",
      "Model took 1013.70 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 25s 628us/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 6.2471e-04 - val_loss: 6.0914e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 4.9552e-04 - val_loss: 3.6329e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 4.2326e-04 - val_loss: 3.1335e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 3.7865e-04 - val_loss: 3.0621e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.5342e-04 - val_loss: 2.7996e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.3921e-04 - val_loss: 2.7684e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.2642e-04 - val_loss: 2.4870e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.1474e-04 - val_loss: 2.3928e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.0736e-04 - val_loss: 2.3671e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.9862e-04 - val_loss: 2.3766e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.8968e-04 - val_loss: 2.4120e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.8144e-04 - val_loss: 2.3556e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.7094e-04 - val_loss: 2.3541e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.6200e-04 - val_loss: 2.2902e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.4878e-04 - val_loss: 2.2315e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.3728e-04 - val_loss: 2.0157e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.2682e-04 - val_loss: 1.8209e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.2190e-04 - val_loss: 1.5678e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.1266e-04 - val_loss: 1.5341e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.0186e-04 - val_loss: 1.5155e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.9472e-04 - val_loss: 1.1445e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.8528e-04 - val_loss: 1.1226e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.8006e-04 - val_loss: 1.0624e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.7674e-04 - val_loss: 9.9825e-05\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.7327e-04 - val_loss: 1.0438e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6863e-04 - val_loss: 9.8679e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.6530e-04 - val_loss: 1.0522e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6344e-04 - val_loss: 9.8523e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.6040e-04 - val_loss: 9.7112e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5799e-04 - val_loss: 9.6632e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.5629e-04 - val_loss: 8.9940e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.5337e-04 - val_loss: 8.9920e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5263e-04 - val_loss: 8.7997e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5079e-04 - val_loss: 8.8687e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4918e-04 - val_loss: 8.7560e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4586e-04 - val_loss: 8.8540e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4408e-04 - val_loss: 8.7957e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4189e-04 - val_loss: 8.6549e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4091e-04 - val_loss: 8.3471e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3979e-04 - val_loss: 8.7141e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3864e-04 - val_loss: 8.5884e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3692e-04 - val_loss: 8.9734e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3661e-04 - val_loss: 8.8874e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3575e-04 - val_loss: 8.9028e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3420e-04 - val_loss: 8.6160e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3347e-04 - val_loss: 9.3233e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3253e-04 - val_loss: 9.1720e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3193e-04 - val_loss: 9.1648e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3119e-04 - val_loss: 9.1120e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2981e-04 - val_loss: 8.8064e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2983e-04 - val_loss: 8.8269e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2887e-04 - val_loss: 8.7613e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2745e-04 - val_loss: 8.4512e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2726e-04 - val_loss: 8.6044e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2529e-04 - val_loss: 8.4318e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2515e-04 - val_loss: 8.8390e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2463e-04 - val_loss: 8.3811e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2365e-04 - val_loss: 8.3309e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2264e-04 - val_loss: 9.1409e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2198e-04 - val_loss: 9.9564e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2213e-04 - val_loss: 8.5575e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1985e-04 - val_loss: 9.5214e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1968e-04 - val_loss: 9.2447e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1964e-04 - val_loss: 8.5783e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1683e-04 - val_loss: 8.6362e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1737e-04 - val_loss: 8.5642e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1641e-04 - val_loss: 8.7712e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1554e-04 - val_loss: 9.3459e-05\n",
      "Validation RMSE:  0.009667439 \n",
      "\n",
      "Model took 1268.89 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 25s 630us/step - loss: 0.0062 - val_loss: 0.0039\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 6.6694e-04 - val_loss: 6.5541e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 5.1622e-04 - val_loss: 3.7571e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 4.5086e-04 - val_loss: 3.8959e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 3.9718e-04 - val_loss: 3.1831e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 3.6183e-04 - val_loss: 2.8104e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.3778e-04 - val_loss: 2.6403e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.2218e-04 - val_loss: 2.6465e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.1019e-04 - val_loss: 2.5859e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.9931e-04 - val_loss: 2.8038e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.8812e-04 - val_loss: 2.7643e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.7922e-04 - val_loss: 2.7080e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.6888e-04 - val_loss: 2.4928e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.5954e-04 - val_loss: 2.4587e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.5095e-04 - val_loss: 2.3882e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.4439e-04 - val_loss: 2.1963e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.3774e-04 - val_loss: 2.1967e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.3133e-04 - val_loss: 2.0322e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.2283e-04 - val_loss: 1.8785e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 452us/step - loss: 2.1456e-04 - val_loss: 1.8698e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.0588e-04 - val_loss: 2.1074e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.0081e-04 - val_loss: 1.6801e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.9441e-04 - val_loss: 1.6336e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.8808e-04 - val_loss: 1.5624e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.8101e-04 - val_loss: 1.4938e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7654e-04 - val_loss: 1.4407e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.7254e-04 - val_loss: 1.3592e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6952e-04 - val_loss: 1.2926e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6604e-04 - val_loss: 1.3181e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6316e-04 - val_loss: 1.2907e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.6149e-04 - val_loss: 1.2850e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5917e-04 - val_loss: 1.2730e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5693e-04 - val_loss: 1.2484e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5459e-04 - val_loss: 1.2493e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5221e-04 - val_loss: 1.2163e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.5013e-04 - val_loss: 1.2784e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4754e-04 - val_loss: 1.2316e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4680e-04 - val_loss: 1.3393e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4500e-04 - val_loss: 1.1913e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4291e-04 - val_loss: 1.2132e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.4110e-04 - val_loss: 1.1829e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3938e-04 - val_loss: 1.3245e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.3748e-04 - val_loss: 1.1593e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3515e-04 - val_loss: 1.1498e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3434e-04 - val_loss: 1.2815e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3367e-04 - val_loss: 1.1309e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3205e-04 - val_loss: 1.0602e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3091e-04 - val_loss: 1.0089e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2941e-04 - val_loss: 9.8564e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2674e-04 - val_loss: 9.6280e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2528e-04 - val_loss: 9.5524e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2433e-04 - val_loss: 9.7184e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2318e-04 - val_loss: 9.3768e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2246e-04 - val_loss: 8.9605e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2172e-04 - val_loss: 8.4434e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2062e-04 - val_loss: 8.2670e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1913e-04 - val_loss: 8.7584e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1749e-04 - val_loss: 8.6387e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1599e-04 - val_loss: 8.4777e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1484e-04 - val_loss: 8.2396e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1414e-04 - val_loss: 8.2376e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.1316e-04 - val_loss: 8.2093e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1250e-04 - val_loss: 8.2877e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1056e-04 - val_loss: 8.0787e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1073e-04 - val_loss: 7.9850e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.0981e-04 - val_loss: 7.9679e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.0896e-04 - val_loss: 8.4986e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.0762e-04 - val_loss: 8.4365e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.0700e-04 - val_loss: 8.5070e-05\n",
      "Validation RMSE:  0.009223365 \n",
      "\n",
      "Model took 1267.72 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 25s 642us/step - loss: 0.0045 - val_loss: 0.0022\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 8.7656e-04 - val_loss: 7.5784e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 6.0147e-04 - val_loss: 4.3735e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 5.1821e-04 - val_loss: 3.4196e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 4.5740e-04 - val_loss: 3.1132e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 4.1424e-04 - val_loss: 2.9774e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.8561e-04 - val_loss: 3.1094e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.6440e-04 - val_loss: 3.1851e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.5336e-04 - val_loss: 3.3444e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.4144e-04 - val_loss: 3.1177e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 3.2920e-04 - val_loss: 2.9551e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.1805e-04 - val_loss: 3.1270e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 3.0500e-04 - val_loss: 3.0678e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.9203e-04 - val_loss: 3.0231e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.8190e-04 - val_loss: 3.1307e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.7295e-04 - val_loss: 3.1362e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 2.6389e-04 - val_loss: 3.2276e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.5711e-04 - val_loss: 2.9130e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.4885e-04 - val_loss: 2.6720e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.4032e-04 - val_loss: 2.6223e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.2868e-04 - val_loss: 2.4449e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.1807e-04 - val_loss: 2.0299e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 2.1134e-04 - val_loss: 1.7981e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.0699e-04 - val_loss: 1.5552e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.0281e-04 - val_loss: 1.5451e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.9255e-04 - val_loss: 1.6063e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.8703e-04 - val_loss: 1.4922e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7950e-04 - val_loss: 1.3993e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.7413e-04 - val_loss: 1.3138e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.6902e-04 - val_loss: 1.2133e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6602e-04 - val_loss: 1.2870e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6283e-04 - val_loss: 1.2312e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6015e-04 - val_loss: 1.1546e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.5829e-04 - val_loss: 1.1617e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5592e-04 - val_loss: 1.1582e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5357e-04 - val_loss: 1.1512e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.5237e-04 - val_loss: 1.1617e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5073e-04 - val_loss: 1.1573e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4859e-04 - val_loss: 1.0971e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.4619e-04 - val_loss: 1.0749e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4510e-04 - val_loss: 1.0241e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4330e-04 - val_loss: 9.9679e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4158e-04 - val_loss: 9.9849e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4161e-04 - val_loss: 1.0348e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3836e-04 - val_loss: 1.0455e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.3753e-04 - val_loss: 9.4951e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3618e-04 - val_loss: 9.1135e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3401e-04 - val_loss: 9.5001e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3136e-04 - val_loss: 9.6013e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3120e-04 - val_loss: 9.0423e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2967e-04 - val_loss: 9.0019e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2832e-04 - val_loss: 8.9326e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2820e-04 - val_loss: 9.1275e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2665e-04 - val_loss: 9.0531e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2548e-04 - val_loss: 8.9622e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2518e-04 - val_loss: 9.5659e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2346e-04 - val_loss: 9.1137e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2048e-04 - val_loss: 9.9140e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2162e-04 - val_loss: 9.0332e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.2019e-04 - val_loss: 9.5449e-05\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1892e-04 - val_loss: 9.8445e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1831e-04 - val_loss: 9.8782e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1782e-04 - val_loss: 1.0133e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1642e-04 - val_loss: 1.0085e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 451us/step - loss: 1.1596e-04 - val_loss: 1.0351e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1486e-04 - val_loss: 9.4484e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1396e-04 - val_loss: 9.5147e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1410e-04 - val_loss: 9.3730e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1289e-04 - val_loss: 9.7902e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1141e-04 - val_loss: 9.6940e-05\n",
      "Validation RMSE:  0.009845821 \n",
      "\n",
      "Model took 1265.59 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 25s 641us/step - loss: 0.0064 - val_loss: 0.0040\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 0.0015 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 7.1868e-04 - val_loss: 5.4840e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 5.3847e-04 - val_loss: 4.3054e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 4.6793e-04 - val_loss: 3.7478e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 4.1899e-04 - val_loss: 4.0913e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.8307e-04 - val_loss: 3.6309e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.5910e-04 - val_loss: 3.0363e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 3.4004e-04 - val_loss: 2.9900e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.2726e-04 - val_loss: 2.9460e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.1583e-04 - val_loss: 2.8635e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.0661e-04 - val_loss: 2.7376e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.9660e-04 - val_loss: 2.8018e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.8419e-04 - val_loss: 2.7132e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.7474e-04 - val_loss: 2.7914e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.6421e-04 - val_loss: 2.8854e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.5052e-04 - val_loss: 2.4591e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.3870e-04 - val_loss: 3.0221e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.3181e-04 - val_loss: 1.7612e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 2.2017e-04 - val_loss: 1.7832e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.1518e-04 - val_loss: 1.8752e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.0427e-04 - val_loss: 1.3350e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.9950e-04 - val_loss: 1.3382e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.9182e-04 - val_loss: 1.0822e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.8518e-04 - val_loss: 1.0609e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.7970e-04 - val_loss: 1.0500e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.7514e-04 - val_loss: 9.9381e-05\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.7110e-04 - val_loss: 1.0021e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.6747e-04 - val_loss: 1.0077e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6399e-04 - val_loss: 1.0023e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.6072e-04 - val_loss: 1.0054e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5843e-04 - val_loss: 9.9701e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5584e-04 - val_loss: 9.3374e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5404e-04 - val_loss: 9.5715e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.5262e-04 - val_loss: 9.3852e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.5074e-04 - val_loss: 9.2074e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4795e-04 - val_loss: 9.1719e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4604e-04 - val_loss: 9.1242e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4299e-04 - val_loss: 9.1461e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.4241e-04 - val_loss: 9.1201e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4113e-04 - val_loss: 8.8853e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3981e-04 - val_loss: 8.9535e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3769e-04 - val_loss: 8.8636e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3690e-04 - val_loss: 8.8402e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3574e-04 - val_loss: 9.1436e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3517e-04 - val_loss: 9.2363e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3348e-04 - val_loss: 9.1498e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3161e-04 - val_loss: 9.6483e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3123e-04 - val_loss: 9.6734e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2989e-04 - val_loss: 1.0299e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2915e-04 - val_loss: 1.0275e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2796e-04 - val_loss: 9.9211e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.2602e-04 - val_loss: 1.0452e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2536e-04 - val_loss: 9.5920e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2363e-04 - val_loss: 9.6476e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2276e-04 - val_loss: 9.2769e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2195e-04 - val_loss: 9.2999e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2110e-04 - val_loss: 8.7822e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1910e-04 - val_loss: 9.7556e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1969e-04 - val_loss: 8.8747e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1937e-04 - val_loss: 8.6629e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1760e-04 - val_loss: 9.0372e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 454us/step - loss: 1.1805e-04 - val_loss: 8.4001e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1738e-04 - val_loss: 8.5118e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1697e-04 - val_loss: 8.8099e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1610e-04 - val_loss: 8.7595e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1536e-04 - val_loss: 8.7161e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1451e-04 - val_loss: 8.5458e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1340e-04 - val_loss: 8.5696e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1216e-04 - val_loss: 8.5797e-05\n",
      "Validation RMSE:  0.0092626875 \n",
      "\n",
      "Model took 1267.17 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 25s 644us/step - loss: 0.0078 - val_loss: 0.0048\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 7.7428e-04 - val_loss: 6.8181e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 5.2986e-04 - val_loss: 4.1855e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 4.4586e-04 - val_loss: 3.8914e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 3.9613e-04 - val_loss: 3.5128e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.6768e-04 - val_loss: 2.8681e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.4475e-04 - val_loss: 2.6346e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.2630e-04 - val_loss: 2.4924e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.0981e-04 - val_loss: 2.5313e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.0076e-04 - val_loss: 2.6713e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.9305e-04 - val_loss: 2.6638e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.8076e-04 - val_loss: 2.5831e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.7324e-04 - val_loss: 2.6262e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.6385e-04 - val_loss: 2.6956e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.5523e-04 - val_loss: 2.6180e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.4640e-04 - val_loss: 2.4490e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.3608e-04 - val_loss: 2.3180e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.2444e-04 - val_loss: 2.1257e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.1354e-04 - val_loss: 1.9669e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.0644e-04 - val_loss: 2.1180e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.0230e-04 - val_loss: 1.4448e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.9307e-04 - val_loss: 1.4997e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.8596e-04 - val_loss: 1.2678e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.7877e-04 - val_loss: 1.1481e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.7190e-04 - val_loss: 1.1072e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6594e-04 - val_loss: 1.0725e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6152e-04 - val_loss: 1.0331e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.5772e-04 - val_loss: 1.0206e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5438e-04 - val_loss: 9.9865e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.5225e-04 - val_loss: 1.0062e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.5019e-04 - val_loss: 1.0011e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4765e-04 - val_loss: 1.0236e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4534e-04 - val_loss: 9.8071e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4249e-04 - val_loss: 9.5932e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4044e-04 - val_loss: 9.6170e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3816e-04 - val_loss: 9.6023e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3673e-04 - val_loss: 9.7023e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3469e-04 - val_loss: 9.8955e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3287e-04 - val_loss: 9.9766e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3064e-04 - val_loss: 1.0197e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2823e-04 - val_loss: 1.0259e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2623e-04 - val_loss: 9.9670e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2589e-04 - val_loss: 1.1165e-04\n",
      "Epoch 45/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2412e-04 - val_loss: 1.1139e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2329e-04 - val_loss: 1.1627e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2181e-04 - val_loss: 1.1464e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2006e-04 - val_loss: 1.0968e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1890e-04 - val_loss: 1.0757e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1757e-04 - val_loss: 1.0869e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1669e-04 - val_loss: 1.0598e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1559e-04 - val_loss: 1.0436e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1447e-04 - val_loss: 9.9307e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1366e-04 - val_loss: 9.4577e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1201e-04 - val_loss: 9.0447e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1113e-04 - val_loss: 8.9232e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1126e-04 - val_loss: 8.9008e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1047e-04 - val_loss: 8.6472e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.0911e-04 - val_loss: 8.5326e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.0888e-04 - val_loss: 8.2196e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.0910e-04 - val_loss: 8.2012e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.0806e-04 - val_loss: 8.0083e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.0724e-04 - val_loss: 7.9093e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.0646e-04 - val_loss: 7.6856e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.0531e-04 - val_loss: 7.5327e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.0616e-04 - val_loss: 7.5403e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.0467e-04 - val_loss: 7.4154e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.0395e-04 - val_loss: 7.3575e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.0298e-04 - val_loss: 7.6161e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.0271e-04 - val_loss: 7.4657e-05\n",
      "Validation RMSE:  0.008640446 \n",
      "\n",
      "Model took 1271.95 seconds to train\n",
      "12 \t8     \t0.00921515\t0.000428503\t0.00864045\t0.00984582\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 25s 642us/step - loss: 0.0044 - val_loss: 0.0031\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 7.1572e-04 - val_loss: 5.3376e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 5.8216e-04 - val_loss: 5.4739e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 5.1235e-04 - val_loss: 5.2802e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 4.6334e-04 - val_loss: 4.2587e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 4.1912e-04 - val_loss: 4.1671e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.9527e-04 - val_loss: 3.6635e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.7574e-04 - val_loss: 3.2111e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.5929e-04 - val_loss: 2.9301e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.4380e-04 - val_loss: 2.5188e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.2791e-04 - val_loss: 2.6218e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.1291e-04 - val_loss: 2.6555e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.0446e-04 - val_loss: 2.5459e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.9658e-04 - val_loss: 2.4743e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.8666e-04 - val_loss: 2.5729e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.7566e-04 - val_loss: 2.2079e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.6125e-04 - val_loss: 2.0210e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.4900e-04 - val_loss: 1.6313e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.4149e-04 - val_loss: 1.4429e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.3232e-04 - val_loss: 1.3986e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.2070e-04 - val_loss: 1.3813e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.1489e-04 - val_loss: 1.3696e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.0579e-04 - val_loss: 1.2868e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.9960e-04 - val_loss: 1.2553e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.9190e-04 - val_loss: 1.2768e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.8687e-04 - val_loss: 1.2593e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.8105e-04 - val_loss: 1.2396e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.7742e-04 - val_loss: 1.2146e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.7276e-04 - val_loss: 1.2060e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.7004e-04 - val_loss: 1.1587e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.6704e-04 - val_loss: 1.1726e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6454e-04 - val_loss: 1.1490e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.6254e-04 - val_loss: 1.1092e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6105e-04 - val_loss: 1.1035e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 453us/step - loss: 1.5930e-04 - val_loss: 1.0906e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5686e-04 - val_loss: 1.0550e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5539e-04 - val_loss: 1.0201e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.5429e-04 - val_loss: 1.0217e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5187e-04 - val_loss: 9.9426e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.5028e-04 - val_loss: 1.0311e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4870e-04 - val_loss: 1.0123e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4741e-04 - val_loss: 1.0665e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4580e-04 - val_loss: 1.0674e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.4413e-04 - val_loss: 1.0962e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4254e-04 - val_loss: 1.1214e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4085e-04 - val_loss: 1.1082e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3829e-04 - val_loss: 1.0775e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3578e-04 - val_loss: 9.9761e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3340e-04 - val_loss: 1.0932e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3280e-04 - val_loss: 1.0595e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3109e-04 - val_loss: 1.0335e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2776e-04 - val_loss: 1.1452e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2655e-04 - val_loss: 1.1016e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.2687e-04 - val_loss: 1.0065e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2594e-04 - val_loss: 9.4530e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2348e-04 - val_loss: 9.7733e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2256e-04 - val_loss: 9.5176e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2180e-04 - val_loss: 9.9609e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2109e-04 - val_loss: 1.0123e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1998e-04 - val_loss: 1.0385e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1938e-04 - val_loss: 1.1068e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1861e-04 - val_loss: 1.1028e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1714e-04 - val_loss: 1.1162e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1840e-04 - val_loss: 1.0549e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1772e-04 - val_loss: 1.0434e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1672e-04 - val_loss: 9.9810e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1656e-04 - val_loss: 9.6381e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1589e-04 - val_loss: 9.6150e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1497e-04 - val_loss: 9.3937e-05\n",
      "Validation RMSE:  0.009692106 \n",
      "\n",
      "Model took 1269.40 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 25s 648us/step - loss: 0.0075 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 7.0411e-04 - val_loss: 9.6408e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 5.4043e-04 - val_loss: 4.9877e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 4.7002e-04 - val_loss: 4.4943e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 4.2308e-04 - val_loss: 4.8298e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.8611e-04 - val_loss: 3.9805e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.6260e-04 - val_loss: 3.9624e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.4176e-04 - val_loss: 3.4358e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.2516e-04 - val_loss: 3.3719e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 3.1556e-04 - val_loss: 2.5170e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 3.0373e-04 - val_loss: 2.4067e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.9642e-04 - val_loss: 2.3751e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.8688e-04 - val_loss: 2.4033e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.7416e-04 - val_loss: 2.5525e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.6448e-04 - val_loss: 2.5443e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.5113e-04 - val_loss: 2.6059e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.4775e-04 - val_loss: 2.2647e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.3799e-04 - val_loss: 2.1744e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.2653e-04 - val_loss: 2.2541e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.2205e-04 - val_loss: 2.1823e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.0986e-04 - val_loss: 1.9016e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.0296e-04 - val_loss: 1.3246e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.9270e-04 - val_loss: 1.3263e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.8812e-04 - val_loss: 1.1251e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.8045e-04 - val_loss: 1.0174e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.7270e-04 - val_loss: 9.9005e-05\n",
      "Epoch 28/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.6753e-04 - val_loss: 9.6497e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6408e-04 - val_loss: 1.0079e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.6051e-04 - val_loss: 1.0101e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.5707e-04 - val_loss: 1.0596e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5458e-04 - val_loss: 1.1190e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.5282e-04 - val_loss: 1.2308e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4917e-04 - val_loss: 1.3059e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4507e-04 - val_loss: 1.2125e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.4536e-04 - val_loss: 1.2099e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4238e-04 - val_loss: 1.0908e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4122e-04 - val_loss: 1.1996e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3919e-04 - val_loss: 1.1702e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3660e-04 - val_loss: 1.1747e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3478e-04 - val_loss: 1.1922e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3275e-04 - val_loss: 1.1260e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3242e-04 - val_loss: 1.1751e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3087e-04 - val_loss: 1.1149e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2848e-04 - val_loss: 1.0799e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2758e-04 - val_loss: 1.0380e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2644e-04 - val_loss: 1.0561e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2544e-04 - val_loss: 1.0605e-04\n",
      "Validation RMSE:  0.010297928 \n",
      "\n",
      "Model took 878.00 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 25s 648us/step - loss: 0.0065 - val_loss: 0.0040\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 0.0013 - val_loss: 0.0025\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 6.4227e-04 - val_loss: 7.0922e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 5.0867e-04 - val_loss: 4.5575e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 4.4533e-04 - val_loss: 3.3301e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 4.0346e-04 - val_loss: 2.8203e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.7398e-04 - val_loss: 2.9717e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 3.5542e-04 - val_loss: 2.9448e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.4259e-04 - val_loss: 2.7915e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 3.3050e-04 - val_loss: 2.6368e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.1929e-04 - val_loss: 2.6260e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 3.0912e-04 - val_loss: 2.4300e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.9955e-04 - val_loss: 2.3480e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.8678e-04 - val_loss: 2.2719e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.7415e-04 - val_loss: 2.2365e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.6513e-04 - val_loss: 2.1383e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.5751e-04 - val_loss: 2.1152e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.4555e-04 - val_loss: 1.9541e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.3605e-04 - val_loss: 1.8669e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.2548e-04 - val_loss: 1.4792e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.1919e-04 - val_loss: 1.4926e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.0904e-04 - val_loss: 1.7618e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.0240e-04 - val_loss: 1.3152e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.9314e-04 - val_loss: 1.3788e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.8997e-04 - val_loss: 1.1779e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.8287e-04 - val_loss: 1.1027e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.7747e-04 - val_loss: 1.0690e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.7323e-04 - val_loss: 1.0192e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6954e-04 - val_loss: 1.0066e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6610e-04 - val_loss: 1.0115e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6313e-04 - val_loss: 1.0096e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6042e-04 - val_loss: 9.6906e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.5787e-04 - val_loss: 9.3094e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.5515e-04 - val_loss: 9.1551e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.5312e-04 - val_loss: 8.7889e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5091e-04 - val_loss: 9.1369e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4930e-04 - val_loss: 9.1996e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4658e-04 - val_loss: 9.0189e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4666e-04 - val_loss: 8.9320e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4560e-04 - val_loss: 8.9780e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4410e-04 - val_loss: 9.3842e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4302e-04 - val_loss: 9.2428e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.4122e-04 - val_loss: 9.2354e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3922e-04 - val_loss: 9.2436e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3791e-04 - val_loss: 9.3340e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3692e-04 - val_loss: 9.2935e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3621e-04 - val_loss: 9.1203e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3457e-04 - val_loss: 9.2585e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3356e-04 - val_loss: 9.4002e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3107e-04 - val_loss: 9.1873e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3138e-04 - val_loss: 8.7825e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3017e-04 - val_loss: 8.5802e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2840e-04 - val_loss: 8.3254e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2720e-04 - val_loss: 8.2451e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2571e-04 - val_loss: 8.1085e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2560e-04 - val_loss: 8.0458e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.2524e-04 - val_loss: 7.9692e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.2381e-04 - val_loss: 7.9025e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.2280e-04 - val_loss: 7.8803e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2131e-04 - val_loss: 7.9347e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2039e-04 - val_loss: 7.7297e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1861e-04 - val_loss: 7.7789e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1811e-04 - val_loss: 7.7995e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1716e-04 - val_loss: 7.6945e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1424e-04 - val_loss: 7.8748e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1469e-04 - val_loss: 7.8436e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1369e-04 - val_loss: 8.0290e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1286e-04 - val_loss: 7.7518e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.1188e-04 - val_loss: 7.3636e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1071e-04 - val_loss: 7.5374e-05\n",
      "Validation RMSE:  0.008681792 \n",
      "\n",
      "Model took 1274.06 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 26s 655us/step - loss: 0.0054 - val_loss: 0.0043\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 7.3069e-04 - val_loss: 7.2614e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 5.3757e-04 - val_loss: 3.6823e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 4.5648e-04 - val_loss: 3.5484e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 4.0490e-04 - val_loss: 3.5259e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.7262e-04 - val_loss: 3.1641e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.4869e-04 - val_loss: 3.0939e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.3160e-04 - val_loss: 2.8238e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 3.1758e-04 - val_loss: 2.7082e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.0604e-04 - val_loss: 2.6278e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.9537e-04 - val_loss: 2.5835e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.8835e-04 - val_loss: 2.5483e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 2.8076e-04 - val_loss: 2.5051e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.7176e-04 - val_loss: 2.3891e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.6416e-04 - val_loss: 2.4101e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.5237e-04 - val_loss: 2.3857e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.4220e-04 - val_loss: 2.2998e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 2.3386e-04 - val_loss: 2.1628e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.2757e-04 - val_loss: 1.9173e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.2111e-04 - val_loss: 1.7262e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.1827e-04 - val_loss: 1.6053e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.0977e-04 - val_loss: 1.4575e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.9782e-04 - val_loss: 1.3393e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.8954e-04 - val_loss: 1.1430e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.8254e-04 - val_loss: 1.0317e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.7583e-04 - val_loss: 1.0118e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.7019e-04 - val_loss: 1.0359e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6616e-04 - val_loss: 9.4796e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.6245e-04 - val_loss: 9.3974e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.5858e-04 - val_loss: 9.6452e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.5535e-04 - val_loss: 9.3434e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.5299e-04 - val_loss: 9.1766e-05\n",
      "Epoch 34/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.5109e-04 - val_loss: 8.7348e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4901e-04 - val_loss: 8.8815e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4688e-04 - val_loss: 8.6081e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4501e-04 - val_loss: 8.6095e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4328e-04 - val_loss: 8.8235e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4195e-04 - val_loss: 8.8878e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4082e-04 - val_loss: 8.5781e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3911e-04 - val_loss: 8.7730e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3743e-04 - val_loss: 8.6086e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3601e-04 - val_loss: 8.4279e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3384e-04 - val_loss: 8.3617e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3337e-04 - val_loss: 8.8545e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.3192e-04 - val_loss: 8.6378e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3063e-04 - val_loss: 8.5463e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3010e-04 - val_loss: 8.6184e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2818e-04 - val_loss: 8.4757e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2757e-04 - val_loss: 8.3890e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2730e-04 - val_loss: 8.1620e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2629e-04 - val_loss: 8.1483e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2511e-04 - val_loss: 8.2483e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2377e-04 - val_loss: 8.1864e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2209e-04 - val_loss: 7.7932e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2124e-04 - val_loss: 8.0048e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2034e-04 - val_loss: 8.0341e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.1968e-04 - val_loss: 8.1591e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1867e-04 - val_loss: 8.1742e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.1751e-04 - val_loss: 8.3828e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1675e-04 - val_loss: 8.5419e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1605e-04 - val_loss: 8.6393e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1499e-04 - val_loss: 8.3621e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1445e-04 - val_loss: 8.2900e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1372e-04 - val_loss: 8.2115e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.1264e-04 - val_loss: 8.0520e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1176e-04 - val_loss: 7.8244e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1061e-04 - val_loss: 7.9003e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1007e-04 - val_loss: 7.9886e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.0926e-04 - val_loss: 7.8080e-05\n",
      "Validation RMSE:  0.008836296 \n",
      "\n",
      "Model took 1273.84 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 26s 658us/step - loss: 0.0055 - val_loss: 0.0039\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 7.6277e-04 - val_loss: 7.7038e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 5.6085e-04 - val_loss: 5.6449e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 4.8048e-04 - val_loss: 3.5109e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 4.1867e-04 - val_loss: 4.4924e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.8163e-04 - val_loss: 3.9306e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.5787e-04 - val_loss: 3.2468e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 3.3811e-04 - val_loss: 2.8272e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 3.2647e-04 - val_loss: 2.7066e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.1542e-04 - val_loss: 2.4953e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 3.0569e-04 - val_loss: 2.4893e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.9626e-04 - val_loss: 2.5231e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.9145e-04 - val_loss: 2.4685e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.8381e-04 - val_loss: 2.3327e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.7195e-04 - val_loss: 2.3204e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.6386e-04 - val_loss: 2.3083e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.5529e-04 - val_loss: 2.3179e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.4825e-04 - val_loss: 2.1925e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.3464e-04 - val_loss: 2.1656e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.2444e-04 - val_loss: 1.6770e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 2.1631e-04 - val_loss: 1.6569e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.1181e-04 - val_loss: 1.5984e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.0547e-04 - val_loss: 1.4386e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.9615e-04 - val_loss: 1.3390e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.8749e-04 - val_loss: 1.2955e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.8177e-04 - val_loss: 1.1852e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.7566e-04 - val_loss: 1.2304e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.7214e-04 - val_loss: 1.1005e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.6815e-04 - val_loss: 1.0604e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.6448e-04 - val_loss: 1.0745e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.6105e-04 - val_loss: 1.0456e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.5869e-04 - val_loss: 1.0059e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.5509e-04 - val_loss: 9.7760e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.5066e-04 - val_loss: 1.0581e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.5003e-04 - val_loss: 8.8926e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.4749e-04 - val_loss: 9.9241e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4575e-04 - val_loss: 9.2348e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.4463e-04 - val_loss: 1.0046e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4352e-04 - val_loss: 9.7358e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4197e-04 - val_loss: 9.3346e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4037e-04 - val_loss: 9.1644e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.3882e-04 - val_loss: 9.1322e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3685e-04 - val_loss: 9.3638e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3559e-04 - val_loss: 9.2787e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3522e-04 - val_loss: 9.0065e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3486e-04 - val_loss: 9.2568e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3414e-04 - val_loss: 9.4058e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3285e-04 - val_loss: 9.3361e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3158e-04 - val_loss: 9.1444e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3039e-04 - val_loss: 9.3211e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.2932e-04 - val_loss: 9.1945e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.2693e-04 - val_loss: 8.8111e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2564e-04 - val_loss: 8.7761e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2458e-04 - val_loss: 8.7728e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2289e-04 - val_loss: 9.1186e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.2198e-04 - val_loss: 9.2717e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.2136e-04 - val_loss: 9.6100e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2016e-04 - val_loss: 9.0204e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1858e-04 - val_loss: 8.9006e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1812e-04 - val_loss: 9.2685e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1801e-04 - val_loss: 9.3325e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1697e-04 - val_loss: 9.3950e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1549e-04 - val_loss: 9.2447e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1598e-04 - val_loss: 8.8215e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1512e-04 - val_loss: 8.7359e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.1410e-04 - val_loss: 8.9664e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1392e-04 - val_loss: 8.7289e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 455us/step - loss: 1.1337e-04 - val_loss: 8.3212e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1265e-04 - val_loss: 8.5102e-05\n",
      "Validation RMSE:  0.009225097 \n",
      "\n",
      "Model took 1277.36 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 26s 662us/step - loss: 0.0057 - val_loss: 0.0046\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 0.0018 - val_loss: 0.0018\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 8.1231e-04 - val_loss: 7.5848e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 5.6890e-04 - val_loss: 3.9585e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 4.8416e-04 - val_loss: 2.9051e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 4.2500e-04 - val_loss: 3.1674e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.8807e-04 - val_loss: 3.1302e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.5626e-04 - val_loss: 2.7066e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 3.3703e-04 - val_loss: 2.5222e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 3.2461e-04 - val_loss: 2.4963e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 3.1420e-04 - val_loss: 2.4032e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.0491e-04 - val_loss: 2.2760e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.9624e-04 - val_loss: 2.2421e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.8629e-04 - val_loss: 2.2118e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.7758e-04 - val_loss: 2.2432e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.6867e-04 - val_loss: 2.1705e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.5881e-04 - val_loss: 2.2565e-04\n",
      "Epoch 18/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 459us/step - loss: 2.5069e-04 - val_loss: 2.1937e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.4292e-04 - val_loss: 2.2185e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.4051e-04 - val_loss: 2.1363e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.3632e-04 - val_loss: 2.1753e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.3128e-04 - val_loss: 2.3526e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.2248e-04 - val_loss: 2.2931e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.1265e-04 - val_loss: 1.9192e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.0366e-04 - val_loss: 1.5168e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.9510e-04 - val_loss: 1.4374e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.8585e-04 - val_loss: 1.3456e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.7849e-04 - val_loss: 1.1846e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.7319e-04 - val_loss: 1.0655e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.6838e-04 - val_loss: 1.0386e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.6405e-04 - val_loss: 1.0066e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.6044e-04 - val_loss: 1.0091e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.5701e-04 - val_loss: 9.9816e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.5436e-04 - val_loss: 1.0104e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5188e-04 - val_loss: 9.6408e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 456us/step - loss: 1.5066e-04 - val_loss: 9.8546e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4917e-04 - val_loss: 9.8178e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4829e-04 - val_loss: 9.4179e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.4556e-04 - val_loss: 9.7309e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4365e-04 - val_loss: 9.3524e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4230e-04 - val_loss: 9.7149e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.4060e-04 - val_loss: 9.5019e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3967e-04 - val_loss: 9.2323e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3880e-04 - val_loss: 8.9375e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3779e-04 - val_loss: 8.7967e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3602e-04 - val_loss: 9.1278e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3361e-04 - val_loss: 9.1605e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3265e-04 - val_loss: 8.9667e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3070e-04 - val_loss: 8.7390e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2986e-04 - val_loss: 8.6116e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2774e-04 - val_loss: 8.5889e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2582e-04 - val_loss: 8.6978e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2467e-04 - val_loss: 8.3333e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2267e-04 - val_loss: 8.5512e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2086e-04 - val_loss: 8.6548e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1998e-04 - val_loss: 8.6478e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1873e-04 - val_loss: 8.4030e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1718e-04 - val_loss: 8.3654e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1582e-04 - val_loss: 8.1796e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1526e-04 - val_loss: 8.3579e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.1392e-04 - val_loss: 8.1829e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1312e-04 - val_loss: 8.3583e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1240e-04 - val_loss: 8.4455e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1137e-04 - val_loss: 8.4451e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.0995e-04 - val_loss: 8.5985e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1043e-04 - val_loss: 9.5462e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.0798e-04 - val_loss: 9.0012e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.0743e-04 - val_loss: 9.7090e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.0763e-04 - val_loss: 1.0326e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.0713e-04 - val_loss: 9.9210e-05\n",
      "Validation RMSE:  0.009960398 \n",
      "\n",
      "Model took 1280.54 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 72\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 17 1\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 254\n",
      "3rd Window Start: 1020\n",
      "4th Window Start: 1260\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1, 1, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48964, 61)\n",
      "Training and test data length 39171 9793\n",
      "trainX.shape[0]:  39171\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39171, 60)\n",
      "Train X shape after np.reshape (39171, 20, 3)\n",
      "Test X shape after np.reshape (9793, 20, 3)\n",
      "Train Y Shape (39171,)\n",
      "Test Y Shape (9793,)\n",
      "Train on 39171 samples, validate on 9793 samples\n",
      "Epoch 1/70\n",
      "39171/39171 [==============================] - 25s 647us/step - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 2/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 7.1883e-04 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39171/39171 [==============================] - 17s 441us/step - loss: 6.1927e-04 - val_loss: 9.3349e-04\n",
      "Epoch 4/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 5.7189e-04 - val_loss: 6.3153e-04\n",
      "Epoch 5/70\n",
      "39171/39171 [==============================] - 17s 437us/step - loss: 5.4069e-04 - val_loss: 5.8542e-04\n",
      "Epoch 6/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 5.0992e-04 - val_loss: 5.7195e-04\n",
      "Epoch 7/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 4.8752e-04 - val_loss: 5.1713e-04\n",
      "Epoch 8/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 4.6879e-04 - val_loss: 5.3652e-04\n",
      "Epoch 9/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 4.5792e-04 - val_loss: 5.2554e-04\n",
      "Epoch 10/70\n",
      "39171/39171 [==============================] - 17s 436us/step - loss: 4.3909e-04 - val_loss: 4.8858e-04\n",
      "Epoch 11/70\n",
      "39171/39171 [==============================] - 17s 437us/step - loss: 4.1207e-04 - val_loss: 4.6577e-04\n",
      "Epoch 12/70\n",
      "39171/39171 [==============================] - 17s 437us/step - loss: 3.9018e-04 - val_loss: 4.3380e-04\n",
      "Epoch 13/70\n",
      "39171/39171 [==============================] - 17s 437us/step - loss: 3.6758e-04 - val_loss: 4.1103e-04\n",
      "Epoch 14/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 3.5529e-04 - val_loss: 3.8376e-04\n",
      "Epoch 15/70\n",
      "39171/39171 [==============================] - 17s 436us/step - loss: 3.4463e-04 - val_loss: 3.6802e-04\n",
      "Epoch 16/70\n",
      "39171/39171 [==============================] - 17s 437us/step - loss: 3.2792e-04 - val_loss: 3.7119e-04\n",
      "Epoch 17/70\n",
      "39171/39171 [==============================] - 17s 440us/step - loss: 3.2406e-04 - val_loss: 4.0427e-04\n",
      "Epoch 18/70\n",
      "39171/39171 [==============================] - 17s 436us/step - loss: 3.2140e-04 - val_loss: 3.8646e-04\n",
      "Epoch 19/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 3.1020e-04 - val_loss: 3.9446e-04\n",
      "Epoch 20/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 2.9629e-04 - val_loss: 4.9929e-04\n",
      "Epoch 21/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 3.0191e-04 - val_loss: 4.0290e-04\n",
      "Epoch 22/70\n",
      "39171/39171 [==============================] - 17s 436us/step - loss: 2.9362e-04 - val_loss: 4.2418e-04\n",
      "Epoch 23/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 2.8713e-04 - val_loss: 3.9802e-04\n",
      "Epoch 24/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 2.8371e-04 - val_loss: 3.8009e-04\n",
      "Epoch 25/70\n",
      "39171/39171 [==============================] - 17s 436us/step - loss: 2.7647e-04 - val_loss: 3.5899e-04\n",
      "Epoch 26/70\n",
      "39171/39171 [==============================] - 17s 441us/step - loss: 2.7113e-04 - val_loss: 3.2033e-04\n",
      "Epoch 27/70\n",
      "39171/39171 [==============================] - 17s 439us/step - loss: 2.6357e-04 - val_loss: 3.1263e-04\n",
      "Epoch 28/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 2.6149e-04 - val_loss: 2.9064e-04\n",
      "Epoch 29/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 2.5431e-04 - val_loss: 2.8576e-04\n",
      "Epoch 30/70\n",
      "39171/39171 [==============================] - 17s 440us/step - loss: 2.4969e-04 - val_loss: 2.9519e-04\n",
      "Epoch 31/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 2.4387e-04 - val_loss: 3.0661e-04\n",
      "Epoch 32/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 2.4173e-04 - val_loss: 3.1100e-04\n",
      "Epoch 33/70\n",
      "39171/39171 [==============================] - 17s 436us/step - loss: 2.3796e-04 - val_loss: 3.4558e-04\n",
      "Epoch 34/70\n",
      "39171/39171 [==============================] - 17s 437us/step - loss: 2.3367e-04 - val_loss: 3.4056e-04\n",
      "Epoch 35/70\n",
      "39171/39171 [==============================] - 17s 437us/step - loss: 2.2861e-04 - val_loss: 3.3269e-04\n",
      "Epoch 36/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 2.2581e-04 - val_loss: 3.4787e-04\n",
      "Epoch 37/70\n",
      "39171/39171 [==============================] - 17s 442us/step - loss: 2.2773e-04 - val_loss: 3.7607e-04\n",
      "Epoch 38/70\n",
      "39171/39171 [==============================] - 17s 437us/step - loss: 2.2544e-04 - val_loss: 3.7565e-04\n",
      "Epoch 39/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 2.1709e-04 - val_loss: 4.1740e-04\n",
      "Epoch 40/70\n",
      "39171/39171 [==============================] - 17s 439us/step - loss: 2.1821e-04 - val_loss: 3.7731e-04\n",
      "Epoch 41/70\n",
      "39171/39171 [==============================] - 17s 437us/step - loss: 2.1727e-04 - val_loss: 3.8722e-04\n",
      "Epoch 42/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 2.1572e-04 - val_loss: 3.7371e-04\n",
      "Epoch 43/70\n",
      "39171/39171 [==============================] - 17s 439us/step - loss: 2.0832e-04 - val_loss: 4.0362e-04\n",
      "Epoch 44/70\n",
      "39171/39171 [==============================] - 17s 436us/step - loss: 2.0811e-04 - val_loss: 3.8859e-04\n",
      "Epoch 45/70\n",
      "39171/39171 [==============================] - 17s 436us/step - loss: 2.0636e-04 - val_loss: 4.0616e-04\n",
      "Epoch 46/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 2.0024e-04 - val_loss: 4.4690e-04\n",
      "Epoch 47/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 2.0003e-04 - val_loss: 4.3473e-04\n",
      "Epoch 48/70\n",
      "39171/39171 [==============================] - 17s 438us/step - loss: 1.9702e-04 - val_loss: 4.2733e-04\n",
      "Epoch 49/70\n",
      "39171/39171 [==============================] - 17s 435us/step - loss: 1.9331e-04 - val_loss: 4.4632e-04\n",
      "Validation RMSE:  0.021126242 \n",
      "\n",
      "Model took 854.38 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 26s 672us/step - loss: 0.0058 - val_loss: 0.0049\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 8.6026e-04 - val_loss: 0.0010\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 5.8190e-04 - val_loss: 5.8298e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 4.8719e-04 - val_loss: 4.4570e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 4.3101e-04 - val_loss: 4.2703e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 3.8713e-04 - val_loss: 3.4013e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 3.5725e-04 - val_loss: 2.4948e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 3.3777e-04 - val_loss: 2.3338e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.2431e-04 - val_loss: 2.4536e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.0992e-04 - val_loss: 2.4186e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 3.0272e-04 - val_loss: 2.2455e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.9212e-04 - val_loss: 2.3015e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.8353e-04 - val_loss: 2.3443e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.7836e-04 - val_loss: 2.2754e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.7019e-04 - val_loss: 2.3376e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.6407e-04 - val_loss: 2.5094e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.5732e-04 - val_loss: 2.5081e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.5145e-04 - val_loss: 2.6178e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.4391e-04 - val_loss: 2.7188e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.3493e-04 - val_loss: 2.7644e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.3073e-04 - val_loss: 2.5279e-04\n",
      "Epoch 23/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.2580e-04 - val_loss: 2.2599e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.1994e-04 - val_loss: 2.1957e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.0903e-04 - val_loss: 1.7965e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.9803e-04 - val_loss: 1.5734e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.8975e-04 - val_loss: 1.5264e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.8026e-04 - val_loss: 1.5016e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.7286e-04 - val_loss: 1.2947e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.6900e-04 - val_loss: 1.2711e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.6518e-04 - val_loss: 1.2738e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.6164e-04 - val_loss: 1.3022e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.5772e-04 - val_loss: 1.3865e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.5365e-04 - val_loss: 1.3761e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.5269e-04 - val_loss: 1.4165e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5045e-04 - val_loss: 1.4290e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.4994e-04 - val_loss: 1.3948e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4858e-04 - val_loss: 1.3719e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.4749e-04 - val_loss: 1.2817e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4608e-04 - val_loss: 1.2375e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4453e-04 - val_loss: 1.2165e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4349e-04 - val_loss: 1.2289e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.4208e-04 - val_loss: 1.1201e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.4040e-04 - val_loss: 1.1917e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3871e-04 - val_loss: 1.1517e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3742e-04 - val_loss: 1.1358e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3629e-04 - val_loss: 1.0794e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3454e-04 - val_loss: 1.0662e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3321e-04 - val_loss: 1.0069e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.3172e-04 - val_loss: 9.8581e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2971e-04 - val_loss: 9.3147e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2799e-04 - val_loss: 8.9562e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2652e-04 - val_loss: 8.9277e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.2568e-04 - val_loss: 8.8223e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2375e-04 - val_loss: 8.6884e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2150e-04 - val_loss: 8.3155e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.2053e-04 - val_loss: 8.2964e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1922e-04 - val_loss: 8.2216e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1950e-04 - val_loss: 8.2947e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1842e-04 - val_loss: 8.7339e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1723e-04 - val_loss: 8.6721e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1701e-04 - val_loss: 8.5309e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.1603e-04 - val_loss: 8.8480e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1545e-04 - val_loss: 8.7812e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1378e-04 - val_loss: 8.5050e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1319e-04 - val_loss: 9.2012e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.1329e-04 - val_loss: 9.3613e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.1297e-04 - val_loss: 9.5701e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1176e-04 - val_loss: 8.9390e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1058e-04 - val_loss: 9.1676e-05\n",
      "Validation RMSE:  0.009574739 \n",
      "\n",
      "Model took 1283.27 seconds to train\n",
      "13 \t8     \t0.010484  \t0.00358847 \t0.00868179\t0.0211262 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 27s 681us/step - loss: 0.0065 - val_loss: 0.0043\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 6.8423e-04 - val_loss: 8.1751e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 5.2377e-04 - val_loss: 4.9084e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 4.5689e-04 - val_loss: 3.6873e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 4.0721e-04 - val_loss: 3.3089e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 3.7292e-04 - val_loss: 3.1458e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 3.4927e-04 - val_loss: 3.4092e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 3.2816e-04 - val_loss: 3.1137e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.1202e-04 - val_loss: 2.7583e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.0141e-04 - val_loss: 2.7220e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.8841e-04 - val_loss: 2.8128e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.7955e-04 - val_loss: 2.5758e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.7242e-04 - val_loss: 2.4336e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.6353e-04 - val_loss: 2.2600e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 2.5606e-04 - val_loss: 2.4404e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.4976e-04 - val_loss: 2.3400e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.4468e-04 - val_loss: 2.0765e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.3680e-04 - val_loss: 1.8847e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.3534e-04 - val_loss: 1.9370e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.3058e-04 - val_loss: 1.8649e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.2642e-04 - val_loss: 1.8653e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.1687e-04 - val_loss: 1.8354e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.0809e-04 - val_loss: 1.7217e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.0387e-04 - val_loss: 1.7088e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.9488e-04 - val_loss: 1.3567e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.8590e-04 - val_loss: 1.2689e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.7991e-04 - val_loss: 1.0399e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.7097e-04 - val_loss: 9.4841e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.6448e-04 - val_loss: 8.8612e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.5920e-04 - val_loss: 8.8081e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.5552e-04 - val_loss: 8.6224e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.5247e-04 - val_loss: 8.3294e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.5033e-04 - val_loss: 8.1666e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4828e-04 - val_loss: 8.0720e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4649e-04 - val_loss: 8.0169e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4500e-04 - val_loss: 7.9203e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4365e-04 - val_loss: 7.9884e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4289e-04 - val_loss: 8.0446e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.4141e-04 - val_loss: 8.1050e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4030e-04 - val_loss: 8.1917e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3888e-04 - val_loss: 8.0991e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3711e-04 - val_loss: 8.2016e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3688e-04 - val_loss: 8.1661e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 457us/step - loss: 1.3570e-04 - val_loss: 8.2948e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3395e-04 - val_loss: 8.4348e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.3204e-04 - val_loss: 8.6417e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3187e-04 - val_loss: 8.8208e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3110e-04 - val_loss: 8.5950e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.2973e-04 - val_loss: 8.6710e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2849e-04 - val_loss: 8.6658e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2604e-04 - val_loss: 8.6505e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.2548e-04 - val_loss: 8.3055e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2506e-04 - val_loss: 8.4531e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2344e-04 - val_loss: 8.2687e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2141e-04 - val_loss: 8.2255e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2034e-04 - val_loss: 8.2392e-05\n",
      "Validation RMSE:  0.009077025 \n",
      "\n",
      "Model took 1047.03 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 72\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 23 8\n",
      "1st Window Start: 46\n",
      "2nd Window Start: 442\n",
      "3rd Window Start: 940\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 1, 1, 1, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 1, 1, 1, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962]\n",
      "Lags length:\n",
      " 69\n",
      "Length Used:\n",
      " 23\n",
      "New Dataset shape after Lags: (49038, 70)\n",
      "Training and test data length 39230 9808\n",
      "trainX.shape[0]:  39230\n",
      "window_length:  23\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39230, 69)\n",
      "Train X shape after np.reshape (39230, 23, 3)\n",
      "Test X shape after np.reshape (9808, 23, 3)\n",
      "Train Y Shape (39230,)\n",
      "Test Y Shape (9808,)\n",
      "Train on 39230 samples, validate on 9808 samples\n",
      "Epoch 1/70\n",
      "39230/39230 [==============================] - 28s 709us/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 2/70\n",
      "39230/39230 [==============================] - 19s 497us/step - loss: 0.0072 - val_loss: 0.0080\n",
      "Epoch 3/70\n",
      "39230/39230 [==============================] - 19s 492us/step - loss: 0.0066 - val_loss: 0.0083\n",
      "Epoch 4/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 0.0059 - val_loss: 0.0085\n",
      "Epoch 5/70\n",
      "39230/39230 [==============================] - 19s 493us/step - loss: 0.0043 - val_loss: 0.0077\n",
      "Epoch 6/70\n",
      "39230/39230 [==============================] - 19s 493us/step - loss: 0.0030 - val_loss: 0.0046\n",
      "Epoch 7/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 0.0023 - val_loss: 0.0040\n",
      "Epoch 8/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 9/70\n",
      "39230/39230 [==============================] - 19s 492us/step - loss: 0.0017 - val_loss: 0.0032\n",
      "Epoch 10/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 0.0015 - val_loss: 0.0029\n",
      "Epoch 11/70\n",
      "39230/39230 [==============================] - 19s 492us/step - loss: 0.0014 - val_loss: 0.0027\n",
      "Epoch 12/70\n",
      "39230/39230 [==============================] - 19s 493us/step - loss: 0.0013 - val_loss: 0.0027\n",
      "Epoch 13/70\n",
      "39230/39230 [==============================] - 20s 502us/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 14/70\n",
      "39230/39230 [==============================] - 19s 492us/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 15/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 16/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 0.0011 - val_loss: 0.0022\n",
      "Epoch 17/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 0.0010 - val_loss: 0.0020\n",
      "Epoch 18/70\n",
      "39230/39230 [==============================] - 19s 493us/step - loss: 9.9331e-04 - val_loss: 0.0020\n",
      "Epoch 19/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 9.6926e-04 - val_loss: 0.0020\n",
      "Epoch 20/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39230/39230 [==============================] - 19s 495us/step - loss: 9.3786e-04 - val_loss: 0.0019\n",
      "Epoch 21/70\n",
      "39230/39230 [==============================] - 20s 499us/step - loss: 9.0908e-04 - val_loss: 0.0020\n",
      "Epoch 22/70\n",
      "39230/39230 [==============================] - 20s 500us/step - loss: 8.8868e-04 - val_loss: 0.0019\n",
      "Epoch 23/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 8.6365e-04 - val_loss: 0.0018\n",
      "Epoch 24/70\n",
      "39230/39230 [==============================] - 19s 493us/step - loss: 8.3012e-04 - val_loss: 0.0018\n",
      "Epoch 25/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 8.1658e-04 - val_loss: 0.0016\n",
      "Epoch 26/70\n",
      "39230/39230 [==============================] - 19s 497us/step - loss: 7.9926e-04 - val_loss: 0.0015\n",
      "Epoch 27/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 7.8444e-04 - val_loss: 0.0015\n",
      "Epoch 28/70\n",
      "39230/39230 [==============================] - 20s 500us/step - loss: 7.6729e-04 - val_loss: 0.0014\n",
      "Epoch 29/70\n",
      "39230/39230 [==============================] - 19s 492us/step - loss: 7.5368e-04 - val_loss: 0.0014\n",
      "Epoch 30/70\n",
      "39230/39230 [==============================] - 19s 493us/step - loss: 7.3731e-04 - val_loss: 0.0013\n",
      "Epoch 31/70\n",
      "39230/39230 [==============================] - 19s 492us/step - loss: 7.1585e-04 - val_loss: 0.0013\n",
      "Epoch 32/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 7.1511e-04 - val_loss: 0.0013\n",
      "Epoch 33/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 7.0661e-04 - val_loss: 0.0012\n",
      "Epoch 34/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 6.9164e-04 - val_loss: 0.0013\n",
      "Epoch 35/70\n",
      "39230/39230 [==============================] - 19s 491us/step - loss: 6.7883e-04 - val_loss: 0.0012\n",
      "Epoch 36/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 6.8060e-04 - val_loss: 0.0011\n",
      "Epoch 37/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 6.7227e-04 - val_loss: 0.0012\n",
      "Epoch 38/70\n",
      "39230/39230 [==============================] - 20s 498us/step - loss: 6.6071e-04 - val_loss: 0.0012\n",
      "Epoch 39/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 6.4085e-04 - val_loss: 0.0012\n",
      "Epoch 40/70\n",
      "39230/39230 [==============================] - 20s 497us/step - loss: 6.3034e-04 - val_loss: 0.0011\n",
      "Epoch 41/70\n",
      "39230/39230 [==============================] - 19s 497us/step - loss: 5.9255e-04 - val_loss: 0.0011\n",
      "Epoch 42/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 6.0511e-04 - val_loss: 0.0010\n",
      "Epoch 43/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 5.7508e-04 - val_loss: 0.0011\n",
      "Epoch 44/70\n",
      "39230/39230 [==============================] - 19s 493us/step - loss: 5.6909e-04 - val_loss: 0.0012\n",
      "Epoch 45/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 5.4849e-04 - val_loss: 0.0011\n",
      "Epoch 46/70\n",
      "39230/39230 [==============================] - 19s 492us/step - loss: 5.4526e-04 - val_loss: 0.0010\n",
      "Epoch 47/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 5.1746e-04 - val_loss: 9.1114e-04\n",
      "Epoch 48/70\n",
      "39230/39230 [==============================] - 20s 497us/step - loss: 5.2647e-04 - val_loss: 0.0011\n",
      "Epoch 49/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 5.1353e-04 - val_loss: 0.0010\n",
      "Epoch 50/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 5.0340e-04 - val_loss: 8.9125e-04\n",
      "Epoch 51/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 4.8928e-04 - val_loss: 8.7312e-04\n",
      "Epoch 52/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 4.8758e-04 - val_loss: 7.1097e-04\n",
      "Epoch 53/70\n",
      "39230/39230 [==============================] - 19s 493us/step - loss: 4.8039e-04 - val_loss: 7.3310e-04\n",
      "Epoch 54/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 4.7133e-04 - val_loss: 6.7267e-04\n",
      "Epoch 55/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 4.6587e-04 - val_loss: 7.5453e-04\n",
      "Epoch 56/70\n",
      "39230/39230 [==============================] - 20s 498us/step - loss: 4.4623e-04 - val_loss: 7.0288e-04\n",
      "Epoch 57/70\n",
      "39230/39230 [==============================] - 20s 501us/step - loss: 4.3736e-04 - val_loss: 4.5401e-04\n",
      "Epoch 58/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 4.3618e-04 - val_loss: 6.2401e-04\n",
      "Epoch 59/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 4.3079e-04 - val_loss: 5.7430e-04\n",
      "Epoch 60/70\n",
      "39230/39230 [==============================] - 20s 498us/step - loss: 4.2353e-04 - val_loss: 4.5985e-04\n",
      "Epoch 61/70\n",
      "39230/39230 [==============================] - 19s 493us/step - loss: 4.1323e-04 - val_loss: 5.1297e-04\n",
      "Epoch 62/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 4.1007e-04 - val_loss: 4.3652e-04\n",
      "Epoch 63/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 3.8981e-04 - val_loss: 4.7696e-04\n",
      "Epoch 64/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 3.9130e-04 - val_loss: 4.5053e-04\n",
      "Epoch 65/70\n",
      "39230/39230 [==============================] - 19s 496us/step - loss: 3.8615e-04 - val_loss: 3.8278e-04\n",
      "Epoch 66/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 3.8090e-04 - val_loss: 3.6998e-04\n",
      "Epoch 67/70\n",
      "39230/39230 [==============================] - 19s 495us/step - loss: 3.7169e-04 - val_loss: 3.7943e-04\n",
      "Epoch 68/70\n",
      "39230/39230 [==============================] - 19s 492us/step - loss: 3.6515e-04 - val_loss: 3.8831e-04\n",
      "Epoch 69/70\n",
      "39230/39230 [==============================] - 19s 494us/step - loss: 3.5788e-04 - val_loss: 4.0438e-04\n",
      "Epoch 70/70\n",
      "39230/39230 [==============================] - 19s 492us/step - loss: 3.6436e-04 - val_loss: 3.4422e-04\n",
      "Validation RMSE:  0.018553039 \n",
      "\n",
      "Model took 1375.23 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 27s 680us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 7.8774e-04 - val_loss: 8.0927e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 5.6722e-04 - val_loss: 6.3645e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 4.8024e-04 - val_loss: 6.6463e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 4.2962e-04 - val_loss: 5.7724e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.9133e-04 - val_loss: 4.2981e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.6078e-04 - val_loss: 3.8018e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.4650e-04 - val_loss: 3.2352e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.3151e-04 - val_loss: 2.9220e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.1649e-04 - val_loss: 2.6576e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.0559e-04 - val_loss: 2.5425e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.9362e-04 - val_loss: 2.4089e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.8063e-04 - val_loss: 2.5187e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.7270e-04 - val_loss: 2.4068e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.6145e-04 - val_loss: 2.2809e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.4893e-04 - val_loss: 2.0532e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.3973e-04 - val_loss: 1.7594e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.2732e-04 - val_loss: 2.1407e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.1935e-04 - val_loss: 1.9007e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.1280e-04 - val_loss: 1.3232e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.0287e-04 - val_loss: 1.2058e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.9614e-04 - val_loss: 1.0824e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.8975e-04 - val_loss: 1.1096e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.8312e-04 - val_loss: 1.1085e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.7777e-04 - val_loss: 1.0777e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.7205e-04 - val_loss: 1.1326e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.6759e-04 - val_loss: 1.0344e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.6382e-04 - val_loss: 9.8716e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.6077e-04 - val_loss: 9.7968e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.5880e-04 - val_loss: 9.5841e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5603e-04 - val_loss: 9.4270e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5419e-04 - val_loss: 9.4246e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.5077e-04 - val_loss: 9.0742e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.4853e-04 - val_loss: 9.3846e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4730e-04 - val_loss: 9.2538e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4493e-04 - val_loss: 9.1292e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4426e-04 - val_loss: 9.1631e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4319e-04 - val_loss: 9.2039e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4234e-04 - val_loss: 9.1346e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4037e-04 - val_loss: 9.3305e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3947e-04 - val_loss: 9.3714e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3819e-04 - val_loss: 8.9950e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3709e-04 - val_loss: 9.0030e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3546e-04 - val_loss: 8.8287e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3417e-04 - val_loss: 8.7710e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3361e-04 - val_loss: 8.8128e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3304e-04 - val_loss: 9.3258e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3157e-04 - val_loss: 9.3088e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3086e-04 - val_loss: 9.6647e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3004e-04 - val_loss: 9.6677e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2884e-04 - val_loss: 1.0082e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2767e-04 - val_loss: 1.0419e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2685e-04 - val_loss: 9.4773e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.2545e-04 - val_loss: 8.9693e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2261e-04 - val_loss: 1.3076e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2237e-04 - val_loss: 9.2636e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2105e-04 - val_loss: 9.3738e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1905e-04 - val_loss: 9.2107e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1846e-04 - val_loss: 9.4345e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1792e-04 - val_loss: 9.7045e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1771e-04 - val_loss: 9.1538e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1679e-04 - val_loss: 8.7214e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1537e-04 - val_loss: 8.9138e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.1487e-04 - val_loss: 8.9876e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1431e-04 - val_loss: 9.3306e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1376e-04 - val_loss: 9.1813e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1251e-04 - val_loss: 9.5968e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1114e-04 - val_loss: 1.0384e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.0975e-04 - val_loss: 1.0677e-04\n",
      "Validation RMSE:  0.010332895 \n",
      "\n",
      "Model took 1290.91 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 27s 687us/step - loss: 0.0083 - val_loss: 0.0039\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 7.3111e-04 - val_loss: 6.8478e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 5.3858e-04 - val_loss: 4.9956e-04\n",
      "Epoch 5/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 464us/step - loss: 4.7175e-04 - val_loss: 4.5105e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 4.2359e-04 - val_loss: 4.5001e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 3.9135e-04 - val_loss: 3.8607e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 3.6309e-04 - val_loss: 3.4955e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.4104e-04 - val_loss: 3.2802e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.2781e-04 - val_loss: 2.9652e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.1734e-04 - val_loss: 2.9076e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 3.0672e-04 - val_loss: 2.8368e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.9880e-04 - val_loss: 2.6897e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.9018e-04 - val_loss: 2.6127e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.8253e-04 - val_loss: 2.5072e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.7432e-04 - val_loss: 2.6716e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.6546e-04 - val_loss: 2.7201e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.5778e-04 - val_loss: 2.7068e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.5125e-04 - val_loss: 2.6232e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.4562e-04 - val_loss: 2.4909e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.4111e-04 - val_loss: 2.2785e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.3744e-04 - val_loss: 2.1817e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.3361e-04 - val_loss: 2.0706e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 2.2908e-04 - val_loss: 1.8936e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.2349e-04 - val_loss: 1.7476e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.1235e-04 - val_loss: 1.6919e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.0272e-04 - val_loss: 1.5784e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.9431e-04 - val_loss: 1.4912e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.8617e-04 - val_loss: 1.4093e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.7932e-04 - val_loss: 1.2047e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.7408e-04 - val_loss: 1.1238e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.6902e-04 - val_loss: 1.0624e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.6398e-04 - val_loss: 9.9774e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.6008e-04 - val_loss: 9.9734e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.5828e-04 - val_loss: 1.0386e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.5633e-04 - val_loss: 1.0757e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 1.5307e-04 - val_loss: 1.0621e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.5098e-04 - val_loss: 9.9888e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4888e-04 - val_loss: 1.0054e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4710e-04 - val_loss: 9.7318e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.4510e-04 - val_loss: 9.2194e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4138e-04 - val_loss: 9.3181e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.4061e-04 - val_loss: 9.8083e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3986e-04 - val_loss: 9.2852e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3903e-04 - val_loss: 9.3034e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3773e-04 - val_loss: 9.1604e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3546e-04 - val_loss: 8.7645e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3432e-04 - val_loss: 8.7961e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3269e-04 - val_loss: 8.9115e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3222e-04 - val_loss: 9.0369e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3156e-04 - val_loss: 8.8984e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.3020e-04 - val_loss: 9.2577e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 458us/step - loss: 1.2929e-04 - val_loss: 9.3429e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2675e-04 - val_loss: 8.6762e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2633e-04 - val_loss: 8.5406e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2580e-04 - val_loss: 8.5204e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2405e-04 - val_loss: 8.2246e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 460us/step - loss: 1.2290e-04 - val_loss: 8.2493e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2146e-04 - val_loss: 8.4205e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.1989e-04 - val_loss: 8.3918e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1836e-04 - val_loss: 8.3506e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1716e-04 - val_loss: 8.0677e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1525e-04 - val_loss: 8.1846e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1478e-04 - val_loss: 8.4384e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1376e-04 - val_loss: 8.2770e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.1195e-04 - val_loss: 8.2763e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1025e-04 - val_loss: 7.8736e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.0858e-04 - val_loss: 7.9172e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.0665e-04 - val_loss: 8.2548e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.0630e-04 - val_loss: 7.7406e-05\n",
      "Validation RMSE:  0.008798077 \n",
      "\n",
      "Model took 1284.36 seconds to train\n",
      "14 \t4     \t0.00995593\t0.00290525 \t0.00868179\t0.018553  \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 27s 698us/step - loss: 0.0049 - val_loss: 0.0031\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 6.3580e-04 - val_loss: 5.8232e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 5.1519e-04 - val_loss: 4.0410e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 4.4267e-04 - val_loss: 4.4597e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 4.0860e-04 - val_loss: 3.2740e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 3.7799e-04 - val_loss: 2.7960e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.5692e-04 - val_loss: 2.7820e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.4290e-04 - val_loss: 2.8618e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.3071e-04 - val_loss: 2.7403e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.1676e-04 - val_loss: 2.4854e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.0841e-04 - val_loss: 2.4545e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.9701e-04 - val_loss: 2.3929e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.8577e-04 - val_loss: 2.2719e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.7417e-04 - val_loss: 2.2184e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.6563e-04 - val_loss: 2.1868e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.6010e-04 - val_loss: 2.1010e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.5336e-04 - val_loss: 2.2266e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.4144e-04 - val_loss: 2.1572e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.3314e-04 - val_loss: 2.0867e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.2872e-04 - val_loss: 1.7312e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.2146e-04 - val_loss: 1.5954e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.1433e-04 - val_loss: 1.5910e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.0853e-04 - val_loss: 1.4970e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.9932e-04 - val_loss: 1.4753e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.9100e-04 - val_loss: 1.3471e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.8388e-04 - val_loss: 1.3213e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.7887e-04 - val_loss: 1.1828e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.7356e-04 - val_loss: 1.1527e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.6867e-04 - val_loss: 1.1341e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.6438e-04 - val_loss: 1.1645e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.6097e-04 - val_loss: 1.1274e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.5767e-04 - val_loss: 1.1114e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.5466e-04 - val_loss: 1.0831e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.5134e-04 - val_loss: 1.0737e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4835e-04 - val_loss: 1.0521e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4585e-04 - val_loss: 1.0251e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4366e-04 - val_loss: 9.9515e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.4103e-04 - val_loss: 9.4867e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3910e-04 - val_loss: 9.2593e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3783e-04 - val_loss: 8.9736e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3670e-04 - val_loss: 9.0665e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.3552e-04 - val_loss: 8.9776e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3420e-04 - val_loss: 9.1000e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3289e-04 - val_loss: 9.0796e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3195e-04 - val_loss: 9.0614e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3079e-04 - val_loss: 9.2639e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3028e-04 - val_loss: 9.0990e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2903e-04 - val_loss: 9.2657e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2798e-04 - val_loss: 9.4167e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2680e-04 - val_loss: 9.4960e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.2617e-04 - val_loss: 8.4463e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2514e-04 - val_loss: 8.8815e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2337e-04 - val_loss: 8.6944e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2210e-04 - val_loss: 8.4351e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2069e-04 - val_loss: 8.1799e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1975e-04 - val_loss: 8.3039e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1957e-04 - val_loss: 8.2000e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1833e-04 - val_loss: 7.8476e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1732e-04 - val_loss: 7.8610e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1740e-04 - val_loss: 8.1574e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1626e-04 - val_loss: 8.1965e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1595e-04 - val_loss: 8.2224e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1528e-04 - val_loss: 8.4038e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1443e-04 - val_loss: 8.2419e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1352e-04 - val_loss: 8.4846e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1280e-04 - val_loss: 8.2590e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1106e-04 - val_loss: 8.3050e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.0997e-04 - val_loss: 8.3879e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.0980e-04 - val_loss: 8.4047e-05\n",
      "Validation RMSE:  0.009167706 \n",
      "\n",
      "Model took 1293.01 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 69\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 4 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 366\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1900\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 366, 367, 368, 369, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 27s 692us/step - loss: 0.0030 - val_loss: 0.0017\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 8.6630e-04 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 6.6313e-04 - val_loss: 9.9682e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 5.7735e-04 - val_loss: 6.9208e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 5.2088e-04 - val_loss: 6.5730e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 4.7876e-04 - val_loss: 5.8518e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 4.4602e-04 - val_loss: 5.4277e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 4.2109e-04 - val_loss: 5.1042e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.9807e-04 - val_loss: 3.6178e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.7652e-04 - val_loss: 3.3525e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.5119e-04 - val_loss: 3.4612e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.2991e-04 - val_loss: 4.3886e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.1468e-04 - val_loss: 3.7823e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.0039e-04 - val_loss: 3.6693e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.8920e-04 - val_loss: 3.0954e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.7350e-04 - val_loss: 2.7369e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.6327e-04 - val_loss: 2.3772e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.5449e-04 - val_loss: 2.0417e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.4726e-04 - val_loss: 2.5177e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.4282e-04 - val_loss: 2.3762e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.3619e-04 - val_loss: 2.1476e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.3207e-04 - val_loss: 1.8019e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.2621e-04 - val_loss: 1.6389e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.2048e-04 - val_loss: 1.5836e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.1683e-04 - val_loss: 1.4528e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.0923e-04 - val_loss: 1.5376e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.0364e-04 - val_loss: 1.4777e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.9716e-04 - val_loss: 1.4378e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.9023e-04 - val_loss: 1.3740e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.8642e-04 - val_loss: 1.3328e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.8252e-04 - val_loss: 1.2302e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.7758e-04 - val_loss: 1.1983e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.7411e-04 - val_loss: 1.1911e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.7106e-04 - val_loss: 1.1194e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.6706e-04 - val_loss: 1.0630e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.6609e-04 - val_loss: 1.0562e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.6189e-04 - val_loss: 1.0230e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5837e-04 - val_loss: 1.0683e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.5802e-04 - val_loss: 1.0343e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.5544e-04 - val_loss: 1.0127e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.5314e-04 - val_loss: 9.8854e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.5155e-04 - val_loss: 1.0521e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4970e-04 - val_loss: 1.0778e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4842e-04 - val_loss: 1.1038e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.4870e-04 - val_loss: 1.0661e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4782e-04 - val_loss: 1.0710e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4467e-04 - val_loss: 1.1671e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4331e-04 - val_loss: 1.1212e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.4267e-04 - val_loss: 1.2982e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.4457e-04 - val_loss: 1.0994e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4309e-04 - val_loss: 1.2628e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4135e-04 - val_loss: 1.3569e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4017e-04 - val_loss: 1.5021e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3939e-04 - val_loss: 1.4691e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3984e-04 - val_loss: 1.3394e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3746e-04 - val_loss: 1.6087e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3623e-04 - val_loss: 1.4858e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3688e-04 - val_loss: 1.6029e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3589e-04 - val_loss: 1.5966e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3481e-04 - val_loss: 1.6304e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3373e-04 - val_loss: 1.6039e-04\n",
      "Validation RMSE:  0.012664363 \n",
      "\n",
      "Model took 1131.97 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 27s 700us/step - loss: 0.0059 - val_loss: 0.0048\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 9.1512e-04 - val_loss: 8.1842e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 5.9650e-04 - val_loss: 4.2595e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 4.9308e-04 - val_loss: 3.4249e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 459us/step - loss: 4.3685e-04 - val_loss: 3.3747e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.9986e-04 - val_loss: 3.1715e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.7393e-04 - val_loss: 3.0606e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.5675e-04 - val_loss: 2.7390e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.4159e-04 - val_loss: 2.7919e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.3006e-04 - val_loss: 2.7084e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 3.1890e-04 - val_loss: 3.0649e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.0789e-04 - val_loss: 2.8825e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.9646e-04 - val_loss: 2.6358e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.8460e-04 - val_loss: 2.5975e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.7297e-04 - val_loss: 2.3610e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.6499e-04 - val_loss: 2.0643e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.4966e-04 - val_loss: 2.3809e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.3665e-04 - val_loss: 1.8112e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.2901e-04 - val_loss: 1.6457e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.2052e-04 - val_loss: 1.6519e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.0988e-04 - val_loss: 1.6831e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.0324e-04 - val_loss: 1.4032e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.9938e-04 - val_loss: 1.2057e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.9422e-04 - val_loss: 1.0947e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.8768e-04 - val_loss: 9.9678e-05\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.8225e-04 - val_loss: 1.0061e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.7813e-04 - val_loss: 1.0155e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.7383e-04 - val_loss: 1.0014e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.7002e-04 - val_loss: 9.9358e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.6680e-04 - val_loss: 9.8318e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.6104e-04 - val_loss: 9.4069e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.5992e-04 - val_loss: 9.0808e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.5755e-04 - val_loss: 9.1736e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.5678e-04 - val_loss: 9.1433e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.5435e-04 - val_loss: 9.7307e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.5341e-04 - val_loss: 8.9640e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.5155e-04 - val_loss: 1.0009e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.5064e-04 - val_loss: 8.6471e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.4917e-04 - val_loss: 9.5863e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4970e-04 - val_loss: 8.8342e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4801e-04 - val_loss: 9.0944e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.4714e-04 - val_loss: 8.8731e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4547e-04 - val_loss: 8.6569e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.4316e-04 - val_loss: 8.6092e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.4081e-04 - val_loss: 9.0626e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3996e-04 - val_loss: 9.2191e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3836e-04 - val_loss: 9.5608e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3584e-04 - val_loss: 9.6333e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.3299e-04 - val_loss: 9.6052e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.3050e-04 - val_loss: 9.8520e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3014e-04 - val_loss: 9.4117e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.2909e-04 - val_loss: 9.7636e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.2734e-04 - val_loss: 9.5330e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2621e-04 - val_loss: 9.6354e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2620e-04 - val_loss: 9.5721e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.2525e-04 - val_loss: 9.6774e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 461us/step - loss: 1.2361e-04 - val_loss: 1.0012e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2126e-04 - val_loss: 9.1603e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1984e-04 - val_loss: 8.7462e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1634e-04 - val_loss: 9.3350e-05\n",
      "Epoch 62/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1607e-04 - val_loss: 1.0911e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1676e-04 - val_loss: 8.4581e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1638e-04 - val_loss: 9.0517e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1390e-04 - val_loss: 8.8139e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1481e-04 - val_loss: 8.4752e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1441e-04 - val_loss: 8.1044e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1159e-04 - val_loss: 8.6549e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1271e-04 - val_loss: 8.0553e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1204e-04 - val_loss: 7.8158e-05\n",
      "Validation RMSE:  0.008840709 \n",
      "\n",
      "Model took 1291.17 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 28s 705us/step - loss: 0.0064 - val_loss: 0.0039\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 7.0594e-04 - val_loss: 4.8394e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 5.4775e-04 - val_loss: 3.0277e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 4.7464e-04 - val_loss: 2.8440e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 4.2302e-04 - val_loss: 2.6910e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.8333e-04 - val_loss: 2.6530e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.5794e-04 - val_loss: 2.5903e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.4129e-04 - val_loss: 2.4799e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.2415e-04 - val_loss: 2.3234e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.1396e-04 - val_loss: 2.4206e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.0313e-04 - val_loss: 2.3414e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.9499e-04 - val_loss: 2.4166e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.8149e-04 - val_loss: 2.4221e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.7157e-04 - val_loss: 2.3798e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.6157e-04 - val_loss: 2.3242e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.5482e-04 - val_loss: 2.1251e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.4252e-04 - val_loss: 2.1985e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.3404e-04 - val_loss: 1.9293e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.3039e-04 - val_loss: 1.9711e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.1848e-04 - val_loss: 1.7699e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.0975e-04 - val_loss: 1.8832e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.9923e-04 - val_loss: 1.3561e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.9255e-04 - val_loss: 1.3386e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.8545e-04 - val_loss: 1.3122e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.7934e-04 - val_loss: 1.2889e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.7252e-04 - val_loss: 1.2675e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.6749e-04 - val_loss: 1.2865e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.6199e-04 - val_loss: 1.2853e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.5797e-04 - val_loss: 1.3029e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5551e-04 - val_loss: 1.1992e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.5380e-04 - val_loss: 1.2242e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5093e-04 - val_loss: 1.2211e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4853e-04 - val_loss: 1.1971e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4779e-04 - val_loss: 1.0352e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4619e-04 - val_loss: 1.0276e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4492e-04 - val_loss: 1.0388e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4388e-04 - val_loss: 9.2521e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4265e-04 - val_loss: 9.2962e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4100e-04 - val_loss: 9.4319e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4007e-04 - val_loss: 9.3952e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3829e-04 - val_loss: 9.4914e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3753e-04 - val_loss: 9.5367e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3595e-04 - val_loss: 9.4475e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3564e-04 - val_loss: 9.3736e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3538e-04 - val_loss: 9.6535e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3454e-04 - val_loss: 9.1754e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3381e-04 - val_loss: 9.5302e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.3198e-04 - val_loss: 9.8983e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3181e-04 - val_loss: 9.4547e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3036e-04 - val_loss: 9.8358e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2869e-04 - val_loss: 9.4911e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2819e-04 - val_loss: 9.7065e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2697e-04 - val_loss: 9.4682e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2610e-04 - val_loss: 9.1945e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2483e-04 - val_loss: 9.6091e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2362e-04 - val_loss: 1.0200e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2234e-04 - val_loss: 9.0055e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.2087e-04 - val_loss: 1.0317e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1939e-04 - val_loss: 8.8146e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1793e-04 - val_loss: 8.9352e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1633e-04 - val_loss: 9.3063e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1446e-04 - val_loss: 9.4183e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1286e-04 - val_loss: 9.3107e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1215e-04 - val_loss: 9.2592e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1114e-04 - val_loss: 8.9343e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.0981e-04 - val_loss: 8.7079e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.0898e-04 - val_loss: 8.4560e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.0917e-04 - val_loss: 8.2277e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.0848e-04 - val_loss: 8.4988e-05\n",
      "Validation RMSE:  0.009218913 \n",
      "\n",
      "Model took 1300.83 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 28s 713us/step - loss: 0.0048 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 6.7645e-04 - val_loss: 6.6855e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 5.4457e-04 - val_loss: 5.5148e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 4.7216e-04 - val_loss: 3.5941e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 4.2230e-04 - val_loss: 4.2749e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.8754e-04 - val_loss: 3.5983e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.5685e-04 - val_loss: 3.5154e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.3729e-04 - val_loss: 3.5197e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.2307e-04 - val_loss: 3.2840e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.1198e-04 - val_loss: 3.4649e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.0157e-04 - val_loss: 3.6989e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.9090e-04 - val_loss: 3.3341e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.7957e-04 - val_loss: 3.2577e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.7053e-04 - val_loss: 2.9730e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.6218e-04 - val_loss: 3.0299e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.5573e-04 - val_loss: 3.0040e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.4529e-04 - val_loss: 2.5210e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.4186e-04 - val_loss: 2.6982e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.3382e-04 - val_loss: 2.8237e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.3714e-04 - val_loss: 2.1718e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.2971e-04 - val_loss: 2.1320e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.2422e-04 - val_loss: 2.4848e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.2026e-04 - val_loss: 2.6346e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.1586e-04 - val_loss: 2.3960e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.1499e-04 - val_loss: 2.3789e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.1173e-04 - val_loss: 2.5146e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.0821e-04 - val_loss: 2.5537e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.0694e-04 - val_loss: 2.6808e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.0383e-04 - val_loss: 2.6817e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.9961e-04 - val_loss: 2.5596e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.9695e-04 - val_loss: 2.3947e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.8862e-04 - val_loss: 2.3580e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.8107e-04 - val_loss: 1.7624e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.7167e-04 - val_loss: 1.5763e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.6709e-04 - val_loss: 1.4640e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.6279e-04 - val_loss: 1.1407e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5866e-04 - val_loss: 1.2343e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5609e-04 - val_loss: 1.1888e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.5305e-04 - val_loss: 1.1437e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.5092e-04 - val_loss: 1.1211e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4845e-04 - val_loss: 1.0750e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4618e-04 - val_loss: 1.0473e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4370e-04 - val_loss: 1.0230e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4109e-04 - val_loss: 1.0084e-04\n",
      "Epoch 46/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3837e-04 - val_loss: 1.0425e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3607e-04 - val_loss: 9.6870e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3393e-04 - val_loss: 9.8798e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3173e-04 - val_loss: 1.0163e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3071e-04 - val_loss: 1.0238e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2933e-04 - val_loss: 1.0292e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2874e-04 - val_loss: 9.9521e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2667e-04 - val_loss: 9.6821e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2596e-04 - val_loss: 9.8207e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2542e-04 - val_loss: 9.8187e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2379e-04 - val_loss: 9.5529e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2257e-04 - val_loss: 1.0032e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.2210e-04 - val_loss: 9.3540e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2141e-04 - val_loss: 9.2829e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1985e-04 - val_loss: 9.8046e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1894e-04 - val_loss: 9.7037e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1756e-04 - val_loss: 9.1245e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1816e-04 - val_loss: 9.1620e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1772e-04 - val_loss: 9.2468e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1666e-04 - val_loss: 9.3669e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1614e-04 - val_loss: 9.2590e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1493e-04 - val_loss: 9.3939e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1414e-04 - val_loss: 8.9426e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1285e-04 - val_loss: 9.4667e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1236e-04 - val_loss: 9.0877e-05\n",
      "Validation RMSE:  0.009532941 \n",
      "\n",
      "Model took 1296.19 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 28s 712us/step - loss: 0.0060 - val_loss: 0.0039\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 6.5714e-04 - val_loss: 5.3950e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 5.3276e-04 - val_loss: 4.5492e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 4.6454e-04 - val_loss: 4.2315e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 4.1473e-04 - val_loss: 3.9848e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.7778e-04 - val_loss: 3.8761e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.5687e-04 - val_loss: 3.5310e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.4652e-04 - val_loss: 3.2614e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.2947e-04 - val_loss: 2.9229e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 3.1733e-04 - val_loss: 2.7080e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 3.0440e-04 - val_loss: 2.6089e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.9272e-04 - val_loss: 2.4976e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.8212e-04 - val_loss: 2.4914e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.7079e-04 - val_loss: 2.5743e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.6480e-04 - val_loss: 2.4867e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.5875e-04 - val_loss: 2.4783e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.5430e-04 - val_loss: 2.4081e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 2.5056e-04 - val_loss: 2.3591e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 2.4461e-04 - val_loss: 2.3119e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.3662e-04 - val_loss: 2.1399e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.2830e-04 - val_loss: 1.9168e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.1581e-04 - val_loss: 1.7640e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.0788e-04 - val_loss: 1.5115e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.9870e-04 - val_loss: 1.4018e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.9014e-04 - val_loss: 1.1764e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.8247e-04 - val_loss: 1.1254e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.7626e-04 - val_loss: 1.0757e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.6671e-04 - val_loss: 1.0126e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.6530e-04 - val_loss: 1.0067e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.6291e-04 - val_loss: 1.0125e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.6074e-04 - val_loss: 9.8085e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.5857e-04 - val_loss: 9.8291e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.5618e-04 - val_loss: 9.7270e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5418e-04 - val_loss: 9.5243e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.5327e-04 - val_loss: 9.3584e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.5174e-04 - val_loss: 9.7105e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.5017e-04 - val_loss: 9.8825e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.4860e-04 - val_loss: 9.7869e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4725e-04 - val_loss: 9.6125e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.4536e-04 - val_loss: 9.1964e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4414e-04 - val_loss: 9.2209e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4340e-04 - val_loss: 8.8906e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4145e-04 - val_loss: 8.6697e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4056e-04 - val_loss: 8.6483e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3883e-04 - val_loss: 8.7810e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3781e-04 - val_loss: 8.8051e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3625e-04 - val_loss: 8.6656e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3537e-04 - val_loss: 8.8654e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3380e-04 - val_loss: 8.8059e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.3164e-04 - val_loss: 8.5602e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3088e-04 - val_loss: 8.4785e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3006e-04 - val_loss: 8.7265e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2937e-04 - val_loss: 8.8099e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.2809e-04 - val_loss: 8.7098e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2735e-04 - val_loss: 8.7250e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2646e-04 - val_loss: 9.1781e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2456e-04 - val_loss: 8.8973e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2287e-04 - val_loss: 8.9937e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.2195e-04 - val_loss: 9.0165e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2121e-04 - val_loss: 9.0726e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2051e-04 - val_loss: 9.2254e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 462us/step - loss: 1.1960e-04 - val_loss: 9.1464e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1897e-04 - val_loss: 9.2658e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1821e-04 - val_loss: 9.3732e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1726e-04 - val_loss: 9.5391e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.1668e-04 - val_loss: 9.7126e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1589e-04 - val_loss: 9.8746e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1514e-04 - val_loss: 9.9057e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1378e-04 - val_loss: 9.9318e-05\n",
      "Validation RMSE:  0.009965857 \n",
      "\n",
      "Model took 1295.15 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 28s 718us/step - loss: 0.0056 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 6.7779e-04 - val_loss: 6.3691e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 5.3861e-04 - val_loss: 5.6161e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 4.7831e-04 - val_loss: 3.8791e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 4.2656e-04 - val_loss: 3.4601e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.9422e-04 - val_loss: 3.3164e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.7378e-04 - val_loss: 3.2348e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.5437e-04 - val_loss: 2.9590e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.4040e-04 - val_loss: 3.0898e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.2678e-04 - val_loss: 2.8899e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.1770e-04 - val_loss: 2.9545e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.0780e-04 - val_loss: 2.9063e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.9714e-04 - val_loss: 2.9822e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.8629e-04 - val_loss: 3.0574e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.7518e-04 - val_loss: 3.0446e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.6499e-04 - val_loss: 3.0002e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.5490e-04 - val_loss: 2.8316e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.4156e-04 - val_loss: 2.5036e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.2956e-04 - val_loss: 2.0330e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.2291e-04 - val_loss: 1.7315e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.2090e-04 - val_loss: 1.5844e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.1330e-04 - val_loss: 1.5973e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.0807e-04 - val_loss: 1.5120e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.0172e-04 - val_loss: 1.7052e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.9548e-04 - val_loss: 1.5978e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.8904e-04 - val_loss: 1.5835e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.8320e-04 - val_loss: 1.5549e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.7627e-04 - val_loss: 1.3893e-04\n",
      "Epoch 30/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.7241e-04 - val_loss: 1.2767e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.6916e-04 - val_loss: 1.3296e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.6538e-04 - val_loss: 1.2539e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.6316e-04 - val_loss: 1.2356e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.6013e-04 - val_loss: 1.3822e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5742e-04 - val_loss: 1.3667e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5550e-04 - val_loss: 1.3939e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5382e-04 - val_loss: 1.3934e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5155e-04 - val_loss: 1.4236e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4928e-04 - val_loss: 1.3706e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4771e-04 - val_loss: 1.3472e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4432e-04 - val_loss: 1.2549e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4351e-04 - val_loss: 1.2374e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4102e-04 - val_loss: 1.2418e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3846e-04 - val_loss: 1.2972e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.3725e-04 - val_loss: 1.1766e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3502e-04 - val_loss: 1.1800e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3313e-04 - val_loss: 1.0495e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3305e-04 - val_loss: 1.0417e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3066e-04 - val_loss: 1.0404e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2945e-04 - val_loss: 1.0343e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2891e-04 - val_loss: 1.0238e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2730e-04 - val_loss: 1.0460e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2573e-04 - val_loss: 1.0001e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2444e-04 - val_loss: 9.9781e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2343e-04 - val_loss: 9.9296e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2185e-04 - val_loss: 9.4835e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2055e-04 - val_loss: 9.2713e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1909e-04 - val_loss: 9.4809e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1679e-04 - val_loss: 9.5571e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1692e-04 - val_loss: 9.3061e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1562e-04 - val_loss: 9.9239e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.1544e-04 - val_loss: 9.8604e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1466e-04 - val_loss: 9.8438e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1397e-04 - val_loss: 9.8314e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.1408e-04 - val_loss: 9.6224e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1166e-04 - val_loss: 1.1377e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1225e-04 - val_loss: 9.6737e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.0987e-04 - val_loss: 9.3728e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1122e-04 - val_loss: 1.0040e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1005e-04 - val_loss: 9.5183e-05\n",
      "Validation RMSE:  0.009756202 \n",
      "\n",
      "Model took 1305.40 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 28s 718us/step - loss: 0.0059 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 7.6122e-04 - val_loss: 8.6166e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 5.4522e-04 - val_loss: 4.8593e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 4.7098e-04 - val_loss: 3.9089e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 4.2427e-04 - val_loss: 3.2176e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.9318e-04 - val_loss: 2.9444e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.6796e-04 - val_loss: 2.7137e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.4690e-04 - val_loss: 2.6325e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.3111e-04 - val_loss: 2.4925e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.1994e-04 - val_loss: 2.4271e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.1036e-04 - val_loss: 2.9657e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.0218e-04 - val_loss: 2.2899e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.9351e-04 - val_loss: 2.3037e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.8481e-04 - val_loss: 2.2481e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.7601e-04 - val_loss: 2.1791e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.6666e-04 - val_loss: 2.1312e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.5967e-04 - val_loss: 2.0858e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.4756e-04 - val_loss: 2.0345e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.3433e-04 - val_loss: 2.0509e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.2943e-04 - val_loss: 1.5960e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.2313e-04 - val_loss: 1.5932e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.1470e-04 - val_loss: 1.4654e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.0437e-04 - val_loss: 1.3893e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.9646e-04 - val_loss: 1.2703e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.9047e-04 - val_loss: 1.2029e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.8552e-04 - val_loss: 1.0776e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.8058e-04 - val_loss: 1.0158e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.7560e-04 - val_loss: 9.8856e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.7051e-04 - val_loss: 1.1600e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.6755e-04 - val_loss: 1.2572e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.6471e-04 - val_loss: 1.3270e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.6084e-04 - val_loss: 1.2838e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5880e-04 - val_loss: 1.3475e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.5751e-04 - val_loss: 1.2906e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.5478e-04 - val_loss: 1.2809e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5411e-04 - val_loss: 1.2400e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5237e-04 - val_loss: 1.2124e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5138e-04 - val_loss: 1.1803e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5009e-04 - val_loss: 1.1649e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.4868e-04 - val_loss: 1.1410e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4753e-04 - val_loss: 1.1345e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4586e-04 - val_loss: 1.1240e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4445e-04 - val_loss: 1.0952e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4343e-04 - val_loss: 1.0804e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4223e-04 - val_loss: 1.0648e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.4090e-04 - val_loss: 1.0665e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3946e-04 - val_loss: 1.0467e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3808e-04 - val_loss: 1.0607e-04\n",
      "Validation RMSE:  0.010299204 \n",
      "\n",
      "Model took 919.51 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 29s 728us/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 6.7013e-04 - val_loss: 9.1201e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 5.3401e-04 - val_loss: 3.8199e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 4.7069e-04 - val_loss: 3.4029e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 4.2695e-04 - val_loss: 3.3180e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 3.9921e-04 - val_loss: 2.8338e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.7770e-04 - val_loss: 2.7702e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.6111e-04 - val_loss: 2.6099e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 3.4821e-04 - val_loss: 2.4634e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.3729e-04 - val_loss: 2.4920e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.2457e-04 - val_loss: 2.4101e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.1361e-04 - val_loss: 2.3640e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.0066e-04 - val_loss: 2.5208e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.8419e-04 - val_loss: 2.4365e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.7644e-04 - val_loss: 2.4457e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.6154e-04 - val_loss: 1.8907e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.4554e-04 - val_loss: 1.8959e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.3614e-04 - val_loss: 1.8184e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.2830e-04 - val_loss: 1.5662e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.1652e-04 - val_loss: 1.4371e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 2.1141e-04 - val_loss: 1.2669e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.0464e-04 - val_loss: 1.1283e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.9742e-04 - val_loss: 1.0598e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.9233e-04 - val_loss: 1.1054e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.8746e-04 - val_loss: 1.1421e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.8316e-04 - val_loss: 1.1559e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.7903e-04 - val_loss: 1.0403e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.7623e-04 - val_loss: 1.1523e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.7237e-04 - val_loss: 1.1510e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.6856e-04 - val_loss: 1.0898e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.6516e-04 - val_loss: 1.1002e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.6280e-04 - val_loss: 1.0957e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5964e-04 - val_loss: 1.0833e-04\n",
      "Epoch 35/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5602e-04 - val_loss: 1.0781e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5429e-04 - val_loss: 1.0628e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5172e-04 - val_loss: 1.0256e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4910e-04 - val_loss: 9.9778e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.4810e-04 - val_loss: 9.5525e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4608e-04 - val_loss: 9.6172e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4418e-04 - val_loss: 9.5188e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4194e-04 - val_loss: 9.3574e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.4092e-04 - val_loss: 9.2246e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3963e-04 - val_loss: 9.4887e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3846e-04 - val_loss: 9.4087e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.3627e-04 - val_loss: 9.4764e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3549e-04 - val_loss: 9.6400e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.3415e-04 - val_loss: 9.8320e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3268e-04 - val_loss: 9.8596e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3302e-04 - val_loss: 9.6239e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3193e-04 - val_loss: 9.6860e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3081e-04 - val_loss: 9.4985e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2979e-04 - val_loss: 9.3983e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2876e-04 - val_loss: 9.5587e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2784e-04 - val_loss: 9.6632e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2677e-04 - val_loss: 1.0092e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2554e-04 - val_loss: 1.0110e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2460e-04 - val_loss: 1.0218e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2361e-04 - val_loss: 1.0188e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2270e-04 - val_loss: 1.0054e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2166e-04 - val_loss: 1.0377e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2082e-04 - val_loss: 1.0915e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2001e-04 - val_loss: 1.0797e-04\n",
      "Validation RMSE:  0.010390775 \n",
      "\n",
      "Model took 1183.12 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 29s 728us/step - loss: 0.0045 - val_loss: 0.0025\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 0.0010 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 6.4214e-04 - val_loss: 4.3368e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 5.3678e-04 - val_loss: 2.8936e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 4.7389e-04 - val_loss: 2.6602e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 4.2977e-04 - val_loss: 2.5376e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.9909e-04 - val_loss: 2.5426e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.7555e-04 - val_loss: 2.5246e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.5859e-04 - val_loss: 2.4496e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.4275e-04 - val_loss: 2.2472e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.3251e-04 - val_loss: 2.1667e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.2083e-04 - val_loss: 2.1125e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.1136e-04 - val_loss: 2.1151e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.0233e-04 - val_loss: 2.0880e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.9291e-04 - val_loss: 2.0788e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.8279e-04 - val_loss: 2.1010e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.6471e-04 - val_loss: 2.0952e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.4975e-04 - val_loss: 2.0746e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.3534e-04 - val_loss: 2.1065e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.3132e-04 - val_loss: 1.7372e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.2300e-04 - val_loss: 1.4647e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.1349e-04 - val_loss: 1.3695e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.0338e-04 - val_loss: 1.3907e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.9351e-04 - val_loss: 1.3035e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.8884e-04 - val_loss: 1.2135e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.8326e-04 - val_loss: 1.1620e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.7742e-04 - val_loss: 1.1251e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.7119e-04 - val_loss: 1.0811e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.6571e-04 - val_loss: 1.0706e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.6221e-04 - val_loss: 1.0217e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.5972e-04 - val_loss: 1.0126e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.5699e-04 - val_loss: 9.6818e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.5193e-04 - val_loss: 9.6454e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.5015e-04 - val_loss: 9.6669e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4960e-04 - val_loss: 9.4176e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4823e-04 - val_loss: 9.0589e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.4605e-04 - val_loss: 9.0012e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4384e-04 - val_loss: 9.0166e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.4138e-04 - val_loss: 9.0520e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3914e-04 - val_loss: 9.3677e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3733e-04 - val_loss: 9.1218e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.3416e-04 - val_loss: 9.1428e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3297e-04 - val_loss: 9.0309e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3153e-04 - val_loss: 8.7846e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3070e-04 - val_loss: 8.8763e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2787e-04 - val_loss: 8.6548e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2633e-04 - val_loss: 8.5361e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2596e-04 - val_loss: 8.1613e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2509e-04 - val_loss: 7.9731e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2428e-04 - val_loss: 7.9319e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.2310e-04 - val_loss: 8.0133e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.2238e-04 - val_loss: 8.2995e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2136e-04 - val_loss: 7.8834e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2022e-04 - val_loss: 7.7278e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1979e-04 - val_loss: 7.5454e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1755e-04 - val_loss: 8.3384e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1764e-04 - val_loss: 7.4867e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1696e-04 - val_loss: 7.7918e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1642e-04 - val_loss: 7.7169e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1455e-04 - val_loss: 7.6251e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1381e-04 - val_loss: 7.4557e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1272e-04 - val_loss: 7.4916e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1341e-04 - val_loss: 7.2829e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1191e-04 - val_loss: 7.2027e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1116e-04 - val_loss: 7.9581e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.1108e-04 - val_loss: 7.7896e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.1140e-04 - val_loss: 7.6445e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1043e-04 - val_loss: 7.7353e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1003e-04 - val_loss: 7.7481e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.0970e-04 - val_loss: 8.1910e-05\n",
      "Validation RMSE:  0.0090504 \n",
      "\n",
      "Model took 1297.94 seconds to train\n",
      "15 \t10    \t0.00988871\t0.00105093 \t0.00884071\t0.0126644 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 29s 739us/step - loss: 0.0060 - val_loss: 0.0038\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 6.8535e-04 - val_loss: 5.1702e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 5.3230e-04 - val_loss: 3.8228e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 4.5095e-04 - val_loss: 3.6358e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 4.0265e-04 - val_loss: 3.5553e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 3.7469e-04 - val_loss: 3.1953e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.5011e-04 - val_loss: 3.0613e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.2875e-04 - val_loss: 3.0147e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 3.1186e-04 - val_loss: 2.7778e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.0331e-04 - val_loss: 3.3580e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.9414e-04 - val_loss: 3.0471e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.8414e-04 - val_loss: 2.8953e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.7379e-04 - val_loss: 2.7256e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.6243e-04 - val_loss: 2.6329e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.5451e-04 - val_loss: 2.5065e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.4624e-04 - val_loss: 2.3311e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.3923e-04 - val_loss: 2.4112e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.3636e-04 - val_loss: 1.9874e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.3619e-04 - val_loss: 1.8756e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.2259e-04 - val_loss: 1.7983e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.1264e-04 - val_loss: 1.5272e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.9761e-04 - val_loss: 2.2654e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.8781e-04 - val_loss: 1.5565e-04\n",
      "Epoch 25/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.7995e-04 - val_loss: 1.3193e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.7441e-04 - val_loss: 1.2838e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.6945e-04 - val_loss: 1.2160e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.6521e-04 - val_loss: 1.1063e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.6191e-04 - val_loss: 1.0706e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5818e-04 - val_loss: 1.0428e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5567e-04 - val_loss: 9.7008e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5317e-04 - val_loss: 9.9024e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4967e-04 - val_loss: 9.8852e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4795e-04 - val_loss: 1.0097e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.4556e-04 - val_loss: 9.5418e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4435e-04 - val_loss: 9.8201e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4343e-04 - val_loss: 9.5579e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.4128e-04 - val_loss: 9.1310e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3840e-04 - val_loss: 8.7443e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3557e-04 - val_loss: 8.5895e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3322e-04 - val_loss: 8.4187e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3097e-04 - val_loss: 8.3263e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2870e-04 - val_loss: 7.9649e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2710e-04 - val_loss: 7.7775e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2713e-04 - val_loss: 7.8369e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2429e-04 - val_loss: 7.8992e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2505e-04 - val_loss: 7.7536e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2424e-04 - val_loss: 8.1033e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2319e-04 - val_loss: 8.0012e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2284e-04 - val_loss: 8.1504e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.2183e-04 - val_loss: 8.1209e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2162e-04 - val_loss: 8.0321e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2124e-04 - val_loss: 8.0232e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2050e-04 - val_loss: 8.0807e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2010e-04 - val_loss: 8.1489e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1825e-04 - val_loss: 8.1887e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1828e-04 - val_loss: 8.4116e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1723e-04 - val_loss: 8.3460e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1640e-04 - val_loss: 8.2675e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1526e-04 - val_loss: 8.4025e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1476e-04 - val_loss: 8.2659e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1356e-04 - val_loss: 8.1130e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.1166e-04 - val_loss: 8.1151e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1125e-04 - val_loss: 8.0085e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1039e-04 - val_loss: 8.0131e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.0959e-04 - val_loss: 8.2766e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.0880e-04 - val_loss: 8.0736e-05\n",
      "Validation RMSE:  0.008985307 \n",
      "\n",
      "Model took 1253.23 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 29s 736us/step - loss: 0.0077 - val_loss: 0.0044\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 7.5396e-04 - val_loss: 9.1042e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 5.2871e-04 - val_loss: 5.6283e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 4.4373e-04 - val_loss: 5.1626e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.9486e-04 - val_loss: 3.8259e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.6188e-04 - val_loss: 3.2185e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.3770e-04 - val_loss: 2.9863e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.2201e-04 - val_loss: 2.7699e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.0849e-04 - val_loss: 2.5358e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.9999e-04 - val_loss: 2.4438e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.9201e-04 - val_loss: 2.3334e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.8359e-04 - val_loss: 2.3364e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.7433e-04 - val_loss: 2.2782e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.6576e-04 - val_loss: 2.4177e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.5565e-04 - val_loss: 2.2182e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.5108e-04 - val_loss: 2.0389e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.4682e-04 - val_loss: 1.8741e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.4328e-04 - val_loss: 1.7794e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.3154e-04 - val_loss: 1.6354e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.2197e-04 - val_loss: 1.8521e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.1350e-04 - val_loss: 1.5318e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.0149e-04 - val_loss: 1.5307e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.8906e-04 - val_loss: 1.3111e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.8416e-04 - val_loss: 1.1896e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.7822e-04 - val_loss: 1.1258e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.7352e-04 - val_loss: 1.0631e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.6836e-04 - val_loss: 9.9069e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.6591e-04 - val_loss: 1.0167e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.6277e-04 - val_loss: 9.5029e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 463us/step - loss: 1.6072e-04 - val_loss: 9.6072e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5850e-04 - val_loss: 9.5346e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5746e-04 - val_loss: 9.7539e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5627e-04 - val_loss: 9.8703e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5486e-04 - val_loss: 9.6576e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5324e-04 - val_loss: 9.6008e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5136e-04 - val_loss: 9.5306e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4920e-04 - val_loss: 9.4872e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4749e-04 - val_loss: 9.3419e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4564e-04 - val_loss: 9.5450e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4409e-04 - val_loss: 9.5967e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.4267e-04 - val_loss: 9.4715e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.4070e-04 - val_loss: 9.5590e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3934e-04 - val_loss: 9.5442e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3732e-04 - val_loss: 9.7062e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3614e-04 - val_loss: 9.4203e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 1.3481e-04 - val_loss: 9.5397e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3389e-04 - val_loss: 9.8940e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3303e-04 - val_loss: 9.8503e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.3192e-04 - val_loss: 1.0191e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3091e-04 - val_loss: 1.0154e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2977e-04 - val_loss: 1.0290e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2827e-04 - val_loss: 1.0155e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2671e-04 - val_loss: 9.8929e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2533e-04 - val_loss: 9.6187e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2375e-04 - val_loss: 9.2035e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2285e-04 - val_loss: 9.0666e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2194e-04 - val_loss: 9.6747e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2115e-04 - val_loss: 9.2688e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2011e-04 - val_loss: 9.0885e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1931e-04 - val_loss: 9.2604e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1816e-04 - val_loss: 9.4277e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1770e-04 - val_loss: 8.8835e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1663e-04 - val_loss: 9.5358e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1611e-04 - val_loss: 9.5643e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1496e-04 - val_loss: 1.0019e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1357e-04 - val_loss: 1.0084e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1281e-04 - val_loss: 9.1872e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1184e-04 - val_loss: 9.1253e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.0995e-04 - val_loss: 8.4710e-05\n",
      "Validation RMSE:  0.009203789 \n",
      "\n",
      "Model took 1302.09 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 29s 751us/step - loss: 0.0093 - val_loss: 0.0049\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 7.3831e-04 - val_loss: 8.6978e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 5.0487e-04 - val_loss: 5.6629e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 4.2920e-04 - val_loss: 4.1411e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.8329e-04 - val_loss: 3.4619e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.5855e-04 - val_loss: 2.8803e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.4124e-04 - val_loss: 2.7863e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.2213e-04 - val_loss: 2.7217e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.1271e-04 - val_loss: 2.5524e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.9986e-04 - val_loss: 2.5712e-04\n",
      "Epoch 12/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.9046e-04 - val_loss: 2.7860e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.8173e-04 - val_loss: 2.5205e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 464us/step - loss: 2.7412e-04 - val_loss: 2.6841e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.6551e-04 - val_loss: 2.5398e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.5720e-04 - val_loss: 2.4491e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.5106e-04 - val_loss: 2.5243e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.4452e-04 - val_loss: 2.3300e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.3857e-04 - val_loss: 2.3353e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.3093e-04 - val_loss: 2.3253e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.2652e-04 - val_loss: 2.2975e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.2133e-04 - val_loss: 2.2270e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 2.1714e-04 - val_loss: 2.2264e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 2.1155e-04 - val_loss: 2.0815e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 2.0605e-04 - val_loss: 2.0868e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.9923e-04 - val_loss: 2.0449e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.9291e-04 - val_loss: 2.2021e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.8692e-04 - val_loss: 1.8437e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.8090e-04 - val_loss: 1.6823e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.7189e-04 - val_loss: 1.4619e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.6556e-04 - val_loss: 1.3584e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5671e-04 - val_loss: 1.3360e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.5173e-04 - val_loss: 1.1296e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4694e-04 - val_loss: 1.0677e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.4311e-04 - val_loss: 9.9913e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3994e-04 - val_loss: 9.4705e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3731e-04 - val_loss: 9.4253e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3474e-04 - val_loss: 9.2480e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3304e-04 - val_loss: 9.2150e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3152e-04 - val_loss: 8.9606e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2927e-04 - val_loss: 8.7141e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2646e-04 - val_loss: 8.9851e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2577e-04 - val_loss: 8.9382e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2332e-04 - val_loss: 8.9133e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2163e-04 - val_loss: 8.8696e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2157e-04 - val_loss: 8.3154e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1966e-04 - val_loss: 9.0880e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1800e-04 - val_loss: 8.6946e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1697e-04 - val_loss: 8.2049e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1543e-04 - val_loss: 8.3379e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1562e-04 - val_loss: 8.2135e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1576e-04 - val_loss: 8.1410e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1470e-04 - val_loss: 7.7335e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1297e-04 - val_loss: 8.0840e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1156e-04 - val_loss: 7.9158e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1143e-04 - val_loss: 7.8095e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1031e-04 - val_loss: 8.3810e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.1041e-04 - val_loss: 7.9067e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.0905e-04 - val_loss: 8.1150e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.0811e-04 - val_loss: 7.8453e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.0832e-04 - val_loss: 7.4688e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.0635e-04 - val_loss: 7.5797e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.0674e-04 - val_loss: 7.9112e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.0571e-04 - val_loss: 7.4586e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 465us/step - loss: 1.0509e-04 - val_loss: 7.8552e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.0536e-04 - val_loss: 7.4679e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.0382e-04 - val_loss: 7.6727e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.0444e-04 - val_loss: 7.3570e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.0382e-04 - val_loss: 7.5035e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.0167e-04 - val_loss: 7.3579e-05\n",
      "Validation RMSE:  0.00857782 \n",
      "\n",
      "Model took 1302.85 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 29s 751us/step - loss: 0.0046 - val_loss: 0.0033\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 0.0011 - val_loss: 9.2821e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 6.0069e-04 - val_loss: 3.9307e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 5.0408e-04 - val_loss: 3.6539e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 4.4989e-04 - val_loss: 4.0063e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 4.0818e-04 - val_loss: 3.9340e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.7574e-04 - val_loss: 3.3021e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 3.6056e-04 - val_loss: 3.2381e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.4610e-04 - val_loss: 3.1632e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 3.3505e-04 - val_loss: 2.9645e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.2439e-04 - val_loss: 3.1837e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.1442e-04 - val_loss: 3.4907e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.0629e-04 - val_loss: 3.3381e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.9557e-04 - val_loss: 3.3598e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.8378e-04 - val_loss: 3.1390e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.7515e-04 - val_loss: 2.9112e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.5739e-04 - val_loss: 2.7834e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.4435e-04 - val_loss: 2.4302e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.4322e-04 - val_loss: 1.8905e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.3617e-04 - val_loss: 1.7207e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.2887e-04 - val_loss: 1.6416e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.2266e-04 - val_loss: 1.6039e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.1250e-04 - val_loss: 1.5201e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.1022e-04 - val_loss: 1.4383e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.0385e-04 - val_loss: 1.3115e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.9754e-04 - val_loss: 1.1999e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.9096e-04 - val_loss: 1.2346e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.8574e-04 - val_loss: 1.2193e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.7909e-04 - val_loss: 1.1627e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.7384e-04 - val_loss: 1.2307e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.7101e-04 - val_loss: 1.2884e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.6516e-04 - val_loss: 1.3676e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.6144e-04 - val_loss: 1.2010e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5792e-04 - val_loss: 1.1761e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5687e-04 - val_loss: 1.2485e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5534e-04 - val_loss: 1.2215e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5370e-04 - val_loss: 1.0898e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4893e-04 - val_loss: 1.0700e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4719e-04 - val_loss: 9.9110e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4643e-04 - val_loss: 1.0394e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4236e-04 - val_loss: 1.0409e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.4144e-04 - val_loss: 1.0056e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4084e-04 - val_loss: 1.0257e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3825e-04 - val_loss: 1.0068e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3725e-04 - val_loss: 1.0116e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3509e-04 - val_loss: 9.8057e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3245e-04 - val_loss: 9.8490e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3072e-04 - val_loss: 9.8162e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2741e-04 - val_loss: 9.3943e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2763e-04 - val_loss: 9.3624e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2588e-04 - val_loss: 9.4855e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2494e-04 - val_loss: 9.5692e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2358e-04 - val_loss: 9.4068e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2332e-04 - val_loss: 9.7112e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2293e-04 - val_loss: 1.0462e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2134e-04 - val_loss: 1.0180e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2059e-04 - val_loss: 1.0473e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1985e-04 - val_loss: 1.0027e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.1760e-04 - val_loss: 9.8658e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1755e-04 - val_loss: 9.9887e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1667e-04 - val_loss: 9.9131e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.1613e-04 - val_loss: 9.5235e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1444e-04 - val_loss: 9.4776e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1472e-04 - val_loss: 9.9094e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1326e-04 - val_loss: 1.0133e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1271e-04 - val_loss: 9.8109e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1064e-04 - val_loss: 9.9554e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1062e-04 - val_loss: 9.7402e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.0933e-04 - val_loss: 9.4211e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.0887e-04 - val_loss: 9.4279e-05\n",
      "Validation RMSE:  0.009709761 \n",
      "\n",
      "Model took 1307.03 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 72\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 6 20 19 10\n",
      "1st Window Start: 47\n",
      "2nd Window Start: 222\n",
      "3rd Window Start: 1005\n",
      "4th Window Start: 1064\n",
      "Activation Function is: tanh\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [47, 48, 49, 50, 51, 52, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48977, 61)\n",
      "Training and test data length 39181 9796\n",
      "trainX.shape[0]:  39181\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39181, 60)\n",
      "Train X shape after np.reshape (39181, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39181,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39181 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39181/39181 [==============================] - 29s 733us/step - loss: 0.0046 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39181/39181 [==============================] - 18s 448us/step - loss: 0.0011 - val_loss: 5.9271e-04\n",
      "Epoch 3/70\n",
      "39181/39181 [==============================] - 17s 441us/step - loss: 7.3655e-04 - val_loss: 3.3396e-04\n",
      "Epoch 4/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 6.0418e-04 - val_loss: 2.5515e-04\n",
      "Epoch 5/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 5.2405e-04 - val_loss: 2.1579e-04\n",
      "Epoch 6/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 4.7092e-04 - val_loss: 2.0230e-04\n",
      "Epoch 7/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 4.3752e-04 - val_loss: 1.9330e-04\n",
      "Epoch 8/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 4.0958e-04 - val_loss: 1.7238e-04\n",
      "Epoch 9/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 3.8472e-04 - val_loss: 1.7406e-04\n",
      "Epoch 10/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 3.6881e-04 - val_loss: 1.7740e-04\n",
      "Epoch 11/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 3.5057e-04 - val_loss: 1.7875e-04\n",
      "Epoch 12/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 3.3804e-04 - val_loss: 1.9684e-04\n",
      "Epoch 13/70\n",
      "39181/39181 [==============================] - 17s 446us/step - loss: 3.2445e-04 - val_loss: 1.9835e-04\n",
      "Epoch 14/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 3.1629e-04 - val_loss: 1.7786e-04\n",
      "Epoch 15/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 3.0551e-04 - val_loss: 1.9563e-04\n",
      "Epoch 16/70\n",
      "39181/39181 [==============================] - 18s 447us/step - loss: 2.9451e-04 - val_loss: 1.9283e-04\n",
      "Epoch 17/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 2.8784e-04 - val_loss: 1.9355e-04\n",
      "Epoch 18/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 2.8061e-04 - val_loss: 1.9456e-04\n",
      "Epoch 19/70\n",
      "39181/39181 [==============================] - 18s 449us/step - loss: 2.7418e-04 - val_loss: 1.9290e-04\n",
      "Epoch 20/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 2.7001e-04 - val_loss: 1.9331e-04\n",
      "Epoch 21/70\n",
      "39181/39181 [==============================] - 17s 441us/step - loss: 2.6218e-04 - val_loss: 1.8844e-04\n",
      "Epoch 22/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 2.5657e-04 - val_loss: 1.8350e-04\n",
      "Epoch 23/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 2.5394e-04 - val_loss: 1.7973e-04\n",
      "Epoch 24/70\n",
      "39181/39181 [==============================] - 17s 446us/step - loss: 2.4913e-04 - val_loss: 1.8197e-04\n",
      "Epoch 25/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 2.4242e-04 - val_loss: 1.7871e-04\n",
      "Epoch 26/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 2.3904e-04 - val_loss: 1.7311e-04\n",
      "Epoch 27/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 2.3564e-04 - val_loss: 1.7305e-04\n",
      "Epoch 28/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 2.2641e-04 - val_loss: 1.6042e-04\n",
      "Epoch 29/70\n",
      "39181/39181 [==============================] - 18s 448us/step - loss: 2.1941e-04 - val_loss: 1.7425e-04\n",
      "Epoch 30/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 2.1349e-04 - val_loss: 1.7047e-04\n",
      "Epoch 31/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 2.0635e-04 - val_loss: 1.3869e-04\n",
      "Epoch 32/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.9710e-04 - val_loss: 1.3201e-04\n",
      "Epoch 33/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 1.9356e-04 - val_loss: 1.3941e-04\n",
      "Epoch 34/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 1.8902e-04 - val_loss: 1.4246e-04\n",
      "Epoch 35/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 1.8707e-04 - val_loss: 1.4362e-04\n",
      "Epoch 36/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 1.8431e-04 - val_loss: 1.4110e-04\n",
      "Epoch 37/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 1.8198e-04 - val_loss: 1.5670e-04\n",
      "Epoch 38/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.7848e-04 - val_loss: 1.4016e-04\n",
      "Epoch 39/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.7721e-04 - val_loss: 1.8972e-04\n",
      "Epoch 40/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 1.7466e-04 - val_loss: 1.5608e-04\n",
      "Epoch 41/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.7155e-04 - val_loss: 1.5005e-04\n",
      "Epoch 42/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.6905e-04 - val_loss: 1.5120e-04\n",
      "Epoch 43/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.6727e-04 - val_loss: 1.4762e-04\n",
      "Epoch 44/70\n",
      "39181/39181 [==============================] - 18s 448us/step - loss: 1.6637e-04 - val_loss: 1.3596e-04\n",
      "Epoch 45/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 1.6567e-04 - val_loss: 1.1980e-04\n",
      "Epoch 46/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.6320e-04 - val_loss: 1.2268e-04\n",
      "Epoch 47/70\n",
      "39181/39181 [==============================] - 17s 446us/step - loss: 1.6205e-04 - val_loss: 1.1962e-04\n",
      "Epoch 48/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.5988e-04 - val_loss: 1.1318e-04\n",
      "Epoch 49/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 1.6053e-04 - val_loss: 9.8586e-05\n",
      "Epoch 50/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 1.5852e-04 - val_loss: 9.5471e-05\n",
      "Epoch 51/70\n",
      "39181/39181 [==============================] - 17s 441us/step - loss: 1.5755e-04 - val_loss: 9.6818e-05\n",
      "Epoch 52/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.5654e-04 - val_loss: 9.5512e-05\n",
      "Epoch 53/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 1.5517e-04 - val_loss: 8.6510e-05\n",
      "Epoch 54/70\n",
      "39181/39181 [==============================] - 17s 446us/step - loss: 1.5372e-04 - val_loss: 8.9885e-05\n",
      "Epoch 55/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 1.5351e-04 - val_loss: 8.4017e-05\n",
      "Epoch 56/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.5295e-04 - val_loss: 8.2480e-05\n",
      "Epoch 57/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 1.5153e-04 - val_loss: 9.6738e-05\n",
      "Epoch 58/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.5023e-04 - val_loss: 8.2043e-05\n",
      "Epoch 59/70\n",
      "39181/39181 [==============================] - 17s 446us/step - loss: 1.5056e-04 - val_loss: 9.3118e-05\n",
      "Epoch 60/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 1.4837e-04 - val_loss: 1.0225e-04\n",
      "Epoch 61/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 1.4686e-04 - val_loss: 9.8933e-05\n",
      "Epoch 62/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 1.4515e-04 - val_loss: 9.5239e-05\n",
      "Epoch 63/70\n",
      "39181/39181 [==============================] - 17s 442us/step - loss: 1.4284e-04 - val_loss: 8.6670e-05\n",
      "Epoch 64/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 1.4237e-04 - val_loss: 8.1612e-05\n",
      "Epoch 65/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 1.4111e-04 - val_loss: 7.9831e-05\n",
      "Epoch 66/70\n",
      "39181/39181 [==============================] - 17s 446us/step - loss: 1.3945e-04 - val_loss: 7.8172e-05\n",
      "Epoch 67/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.3771e-04 - val_loss: 7.7573e-05\n",
      "Epoch 68/70\n",
      "39181/39181 [==============================] - 17s 445us/step - loss: 1.3629e-04 - val_loss: 7.7751e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39181/39181 [==============================] - 17s 443us/step - loss: 1.3466e-04 - val_loss: 7.8872e-05\n",
      "Epoch 70/70\n",
      "39181/39181 [==============================] - 17s 444us/step - loss: 1.3282e-04 - val_loss: 7.7012e-05\n",
      "Validation RMSE:  0.008775671 \n",
      "\n",
      "Model took 1236.92 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 30s 765us/step - loss: 0.0057 - val_loss: 0.0033\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 0.0012 - val_loss: 9.1352e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 6.6668e-04 - val_loss: 5.5395e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 5.2626e-04 - val_loss: 4.1881e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 4.5811e-04 - val_loss: 3.7800e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 4.1037e-04 - val_loss: 3.5829e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.7904e-04 - val_loss: 3.0496e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.5771e-04 - val_loss: 2.8095e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 3.4215e-04 - val_loss: 2.8659e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 3.2583e-04 - val_loss: 2.7646e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 3.1769e-04 - val_loss: 2.6265e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.0805e-04 - val_loss: 2.6852e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.9843e-04 - val_loss: 2.5978e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.9057e-04 - val_loss: 2.4327e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.8249e-04 - val_loss: 2.2816e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.7577e-04 - val_loss: 2.2196e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.6633e-04 - val_loss: 2.1575e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 2.5768e-04 - val_loss: 2.1475e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.4946e-04 - val_loss: 2.0969e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.4132e-04 - val_loss: 2.0382e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.3208e-04 - val_loss: 1.9770e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.2331e-04 - val_loss: 1.9023e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 2.1646e-04 - val_loss: 1.6132e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.0687e-04 - val_loss: 1.6012e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.0162e-04 - val_loss: 1.5282e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.9085e-04 - val_loss: 1.4223e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.8236e-04 - val_loss: 1.3499e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.7496e-04 - val_loss: 1.3006e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.6852e-04 - val_loss: 1.1851e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.6419e-04 - val_loss: 1.1500e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.6078e-04 - val_loss: 1.1113e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5781e-04 - val_loss: 1.1564e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.5497e-04 - val_loss: 1.1094e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.5204e-04 - val_loss: 1.0542e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.5038e-04 - val_loss: 1.0965e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.4859e-04 - val_loss: 1.1743e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4612e-04 - val_loss: 1.1738e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4498e-04 - val_loss: 1.1699e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4328e-04 - val_loss: 1.1498e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.4009e-04 - val_loss: 1.1156e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3956e-04 - val_loss: 1.0655e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3738e-04 - val_loss: 1.0996e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3476e-04 - val_loss: 1.0421e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.3343e-04 - val_loss: 1.0282e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3051e-04 - val_loss: 1.0563e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3048e-04 - val_loss: 9.7035e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2920e-04 - val_loss: 9.8075e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2688e-04 - val_loss: 1.0177e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2594e-04 - val_loss: 1.0482e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2332e-04 - val_loss: 9.7627e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2160e-04 - val_loss: 1.0497e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2079e-04 - val_loss: 1.0101e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.1989e-04 - val_loss: 1.0215e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1916e-04 - val_loss: 8.6804e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1670e-04 - val_loss: 8.4602e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1664e-04 - val_loss: 8.4257e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1490e-04 - val_loss: 8.6919e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1555e-04 - val_loss: 8.4046e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1452e-04 - val_loss: 8.2306e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.1399e-04 - val_loss: 7.7989e-05\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1400e-04 - val_loss: 8.0721e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1171e-04 - val_loss: 8.3728e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1263e-04 - val_loss: 7.6646e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1094e-04 - val_loss: 8.0888e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1185e-04 - val_loss: 7.7550e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.0995e-04 - val_loss: 7.7153e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.0979e-04 - val_loss: 7.7724e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.0845e-04 - val_loss: 7.8093e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.0844e-04 - val_loss: 7.4410e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.0797e-04 - val_loss: 7.3437e-05\n",
      "Validation RMSE:  0.008569555 \n",
      "\n",
      "Model took 1311.56 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 30s 770us/step - loss: 0.0065 - val_loss: 0.0052\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 8.4306e-04 - val_loss: 0.0010\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 5.8532e-04 - val_loss: 4.1775e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 4.8851e-04 - val_loss: 3.8207e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 4.3053e-04 - val_loss: 3.2670e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.9428e-04 - val_loss: 3.0972e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.7360e-04 - val_loss: 2.8211e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.5617e-04 - val_loss: 2.6506e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.4273e-04 - val_loss: 2.7329e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.3144e-04 - val_loss: 2.6058e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 3.1922e-04 - val_loss: 2.4723e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.0961e-04 - val_loss: 2.3977e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.0358e-04 - val_loss: 2.3552e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.9901e-04 - val_loss: 2.3075e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.9127e-04 - val_loss: 2.2985e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.8438e-04 - val_loss: 2.2769e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.7858e-04 - val_loss: 2.1972e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.7068e-04 - val_loss: 2.2588e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.6246e-04 - val_loss: 2.1305e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.5594e-04 - val_loss: 1.9421e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.4860e-04 - val_loss: 2.0805e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.4289e-04 - val_loss: 2.0364e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.3948e-04 - val_loss: 1.9759e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.3454e-04 - val_loss: 2.0168e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.3281e-04 - val_loss: 2.2426e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.2915e-04 - val_loss: 2.1781e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 2.2509e-04 - val_loss: 2.1053e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.2317e-04 - val_loss: 2.1683e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.1881e-04 - val_loss: 1.9897e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.1419e-04 - val_loss: 1.8020e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.1255e-04 - val_loss: 1.9231e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 2.0603e-04 - val_loss: 1.8295e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.9845e-04 - val_loss: 1.5718e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.9208e-04 - val_loss: 1.4042e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.8404e-04 - val_loss: 1.3993e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.7568e-04 - val_loss: 1.2762e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.7353e-04 - val_loss: 1.2517e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.7120e-04 - val_loss: 1.1924e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.6747e-04 - val_loss: 1.0971e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.6155e-04 - val_loss: 1.0504e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.5733e-04 - val_loss: 1.0437e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5488e-04 - val_loss: 1.0985e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5193e-04 - val_loss: 1.1125e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.4900e-04 - val_loss: 1.1077e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.4680e-04 - val_loss: 1.0258e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4359e-04 - val_loss: 9.6341e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.4082e-04 - val_loss: 9.4901e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3964e-04 - val_loss: 9.1354e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3653e-04 - val_loss: 8.8884e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3489e-04 - val_loss: 8.9367e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.3156e-04 - val_loss: 9.0934e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3066e-04 - val_loss: 9.7263e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2992e-04 - val_loss: 9.1544e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2695e-04 - val_loss: 8.9168e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2580e-04 - val_loss: 8.8033e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2489e-04 - val_loss: 8.7495e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2444e-04 - val_loss: 8.8523e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2378e-04 - val_loss: 8.9570e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 1.2277e-04 - val_loss: 8.9550e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.2246e-04 - val_loss: 8.9645e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2122e-04 - val_loss: 9.0792e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2111e-04 - val_loss: 9.0476e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.2045e-04 - val_loss: 9.1998e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1981e-04 - val_loss: 8.9479e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1872e-04 - val_loss: 9.0368e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1715e-04 - val_loss: 8.7683e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1762e-04 - val_loss: 8.9575e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1686e-04 - val_loss: 9.1463e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1608e-04 - val_loss: 9.2154e-05\n",
      "Validation RMSE:  0.009599687 \n",
      "\n",
      "Model took 1306.90 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 30s 771us/step - loss: 0.0050 - val_loss: 0.0036\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 6.6927e-04 - val_loss: 4.9689e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 5.3324e-04 - val_loss: 3.3729e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 4.6263e-04 - val_loss: 3.1755e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 4.1733e-04 - val_loss: 2.9385e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.9117e-04 - val_loss: 3.2905e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 3.6995e-04 - val_loss: 3.0526e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.5118e-04 - val_loss: 3.1975e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.3742e-04 - val_loss: 3.5318e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.2536e-04 - val_loss: 3.7719e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 3.1467e-04 - val_loss: 3.4077e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.0320e-04 - val_loss: 3.4928e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.9109e-04 - val_loss: 3.5497e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.7903e-04 - val_loss: 3.0770e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.6824e-04 - val_loss: 2.6521e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.5449e-04 - val_loss: 2.3083e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.4616e-04 - val_loss: 2.8287e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.4439e-04 - val_loss: 1.8872e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.3617e-04 - val_loss: 2.2187e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.3132e-04 - val_loss: 1.7047e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.1713e-04 - val_loss: 1.7761e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.9994e-04 - val_loss: 1.3331e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.8396e-04 - val_loss: 1.3305e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.7775e-04 - val_loss: 1.2505e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.7273e-04 - val_loss: 1.3102e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.6972e-04 - val_loss: 1.3602e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.6611e-04 - val_loss: 1.4252e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.6398e-04 - val_loss: 1.4681e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.6123e-04 - val_loss: 1.5269e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5991e-04 - val_loss: 1.2911e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.5666e-04 - val_loss: 1.3558e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.5403e-04 - val_loss: 1.1611e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.5202e-04 - val_loss: 1.1514e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.5123e-04 - val_loss: 1.1661e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4861e-04 - val_loss: 1.1171e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4693e-04 - val_loss: 1.0707e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.4501e-04 - val_loss: 1.1184e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.4521e-04 - val_loss: 1.0750e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.4268e-04 - val_loss: 1.0153e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4208e-04 - val_loss: 9.7596e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.4014e-04 - val_loss: 8.5343e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3864e-04 - val_loss: 8.5896e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3763e-04 - val_loss: 8.5230e-05\n",
      "Epoch 45/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3667e-04 - val_loss: 8.6139e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3538e-04 - val_loss: 8.2287e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3379e-04 - val_loss: 7.9084e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3220e-04 - val_loss: 7.7598e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.3030e-04 - val_loss: 7.7106e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2869e-04 - val_loss: 7.4668e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2754e-04 - val_loss: 7.6207e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2540e-04 - val_loss: 7.6829e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2457e-04 - val_loss: 7.3059e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2251e-04 - val_loss: 8.1563e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2142e-04 - val_loss: 7.7968e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2159e-04 - val_loss: 7.9417e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.1929e-04 - val_loss: 7.8112e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1974e-04 - val_loss: 7.8179e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1705e-04 - val_loss: 7.7665e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.1709e-04 - val_loss: 7.8794e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1548e-04 - val_loss: 7.9265e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1421e-04 - val_loss: 7.5987e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1340e-04 - val_loss: 7.5580e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1184e-04 - val_loss: 7.3719e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1111e-04 - val_loss: 7.0953e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.0954e-04 - val_loss: 7.7935e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.0979e-04 - val_loss: 7.3996e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.0777e-04 - val_loss: 7.4625e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.0763e-04 - val_loss: 8.2310e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.0715e-04 - val_loss: 7.7986e-05\n",
      "Validation RMSE:  0.008830981 \n",
      "\n",
      "Model took 1312.31 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 31s 782us/step - loss: 0.0048 - val_loss: 0.0031\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 0.0011 - val_loss: 9.0330e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 6.2848e-04 - val_loss: 3.4352e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 5.2875e-04 - val_loss: 2.8163e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 4.6371e-04 - val_loss: 2.8310e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 4.0652e-04 - val_loss: 3.0798e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 3.7460e-04 - val_loss: 3.4518e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.5437e-04 - val_loss: 3.0271e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.3795e-04 - val_loss: 2.9365e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.2730e-04 - val_loss: 2.7020e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.1753e-04 - val_loss: 2.7109e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 466us/step - loss: 3.0756e-04 - val_loss: 2.7080e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.9794e-04 - val_loss: 2.7017e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.8791e-04 - val_loss: 2.7007e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.7775e-04 - val_loss: 2.5963e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.6868e-04 - val_loss: 2.3541e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 2.5753e-04 - val_loss: 2.1531e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.4336e-04 - val_loss: 2.2664e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.3830e-04 - val_loss: 1.7943e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 2.2928e-04 - val_loss: 1.4607e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.2206e-04 - val_loss: 1.6414e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.1384e-04 - val_loss: 1.5867e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.0600e-04 - val_loss: 1.3127e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.9861e-04 - val_loss: 1.2024e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.9094e-04 - val_loss: 1.1142e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.8558e-04 - val_loss: 1.0754e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.8064e-04 - val_loss: 1.0620e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.7519e-04 - val_loss: 1.0755e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.7052e-04 - val_loss: 1.0737e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.6651e-04 - val_loss: 1.0989e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.6253e-04 - val_loss: 1.0578e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.6053e-04 - val_loss: 1.0418e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5833e-04 - val_loss: 1.1189e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5595e-04 - val_loss: 1.1045e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.5244e-04 - val_loss: 9.6553e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5165e-04 - val_loss: 1.0535e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4908e-04 - val_loss: 9.4485e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.4721e-04 - val_loss: 9.3490e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4559e-04 - val_loss: 9.4745e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4392e-04 - val_loss: 9.2234e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.4205e-04 - val_loss: 9.1167e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3961e-04 - val_loss: 9.1588e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.3818e-04 - val_loss: 9.1536e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.3656e-04 - val_loss: 9.0936e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 467us/step - loss: 1.3450e-04 - val_loss: 8.8399e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3364e-04 - val_loss: 8.9548e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.3200e-04 - val_loss: 8.7877e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3013e-04 - val_loss: 8.6532e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2739e-04 - val_loss: 8.6481e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2687e-04 - val_loss: 8.5688e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2552e-04 - val_loss: 8.5164e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.2390e-04 - val_loss: 8.2602e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2309e-04 - val_loss: 8.2435e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2246e-04 - val_loss: 8.3051e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2187e-04 - val_loss: 8.2819e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1982e-04 - val_loss: 8.1559e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.1937e-04 - val_loss: 8.0059e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1844e-04 - val_loss: 7.7647e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1915e-04 - val_loss: 7.9301e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1740e-04 - val_loss: 7.8248e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 468us/step - loss: 1.1746e-04 - val_loss: 7.9240e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1693e-04 - val_loss: 7.8907e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1625e-04 - val_loss: 8.0912e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1567e-04 - val_loss: 8.0930e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1499e-04 - val_loss: 7.8929e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1435e-04 - val_loss: 7.6736e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1226e-04 - val_loss: 7.5938e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1287e-04 - val_loss: 7.9460e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.1237e-04 - val_loss: 8.1137e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1194e-04 - val_loss: 8.0740e-05\n",
      "Validation RMSE:  0.008985563 \n",
      "\n",
      "Model took 1311.61 seconds to train\n",
      "16 \t9     \t0.0090457 \t0.00037078 \t0.00856956\t0.00970976\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 31s 790us/step - loss: 0.0046 - val_loss: 0.0034\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 6.6669e-04 - val_loss: 7.7845e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 5.4126e-04 - val_loss: 4.7141e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 4.7100e-04 - val_loss: 3.5502e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 4.2390e-04 - val_loss: 2.8055e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.9354e-04 - val_loss: 2.9572e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.7310e-04 - val_loss: 2.5901e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.5447e-04 - val_loss: 2.4536e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.4087e-04 - val_loss: 2.2328e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 3.2805e-04 - val_loss: 2.2474e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 3.1936e-04 - val_loss: 2.1921e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 3.1002e-04 - val_loss: 2.1659e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.9996e-04 - val_loss: 2.1465e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.9050e-04 - val_loss: 2.1426e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.8090e-04 - val_loss: 2.1494e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.7287e-04 - val_loss: 2.0796e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.6415e-04 - val_loss: 2.0372e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.5393e-04 - val_loss: 2.0478e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.4478e-04 - val_loss: 2.2365e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.3688e-04 - val_loss: 1.9284e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.2411e-04 - val_loss: 1.9056e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.1303e-04 - val_loss: 1.6441e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.0682e-04 - val_loss: 1.5538e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.9825e-04 - val_loss: 1.5947e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.8938e-04 - val_loss: 1.4731e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.8211e-04 - val_loss: 1.4218e-04\n",
      "Epoch 28/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.7479e-04 - val_loss: 1.3495e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.7020e-04 - val_loss: 1.2372e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.6731e-04 - val_loss: 1.1191e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.6472e-04 - val_loss: 1.0738e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.6243e-04 - val_loss: 1.0162e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5993e-04 - val_loss: 9.7362e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.5731e-04 - val_loss: 9.5992e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5496e-04 - val_loss: 9.2445e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5261e-04 - val_loss: 9.2945e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.5093e-04 - val_loss: 9.6371e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.4946e-04 - val_loss: 9.4274e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.4901e-04 - val_loss: 9.3929e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4713e-04 - val_loss: 9.2883e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.4554e-04 - val_loss: 9.0317e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4473e-04 - val_loss: 8.8143e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4365e-04 - val_loss: 8.7555e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.4249e-04 - val_loss: 8.7189e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.4112e-04 - val_loss: 8.6493e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4002e-04 - val_loss: 8.8724e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.3862e-04 - val_loss: 8.9823e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.3811e-04 - val_loss: 8.6623e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3709e-04 - val_loss: 8.8076e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3608e-04 - val_loss: 8.8377e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.3474e-04 - val_loss: 9.4388e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.3409e-04 - val_loss: 9.2760e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3199e-04 - val_loss: 8.9890e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3082e-04 - val_loss: 8.7333e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2849e-04 - val_loss: 8.6725e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2748e-04 - val_loss: 8.4714e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2676e-04 - val_loss: 8.4668e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2483e-04 - val_loss: 8.9261e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2556e-04 - val_loss: 8.4239e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2461e-04 - val_loss: 7.9626e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2396e-04 - val_loss: 7.8244e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.2217e-04 - val_loss: 7.9615e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2129e-04 - val_loss: 8.3877e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1933e-04 - val_loss: 8.5130e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1892e-04 - val_loss: 8.6460e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1867e-04 - val_loss: 8.3460e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1715e-04 - val_loss: 8.3186e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1768e-04 - val_loss: 8.1695e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1750e-04 - val_loss: 8.4280e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1675e-04 - val_loss: 7.9286e-05\n",
      "Validation RMSE:  0.0089042885 \n",
      "\n",
      "Model took 1315.71 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 31s 798us/step - loss: 0.0042 - val_loss: 0.0028\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 0.0010 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 6.1017e-04 - val_loss: 6.9951e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 5.0402e-04 - val_loss: 4.7729e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 4.3537e-04 - val_loss: 3.4890e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.9824e-04 - val_loss: 3.1400e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 3.7162e-04 - val_loss: 2.8402e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.5302e-04 - val_loss: 2.8613e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.3682e-04 - val_loss: 2.5501e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.2533e-04 - val_loss: 2.4834e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.1211e-04 - val_loss: 2.3216e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 3.0189e-04 - val_loss: 2.2323e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.8987e-04 - val_loss: 2.2380e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.7460e-04 - val_loss: 2.2554e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.6159e-04 - val_loss: 2.2424e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.5595e-04 - val_loss: 2.0986e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.4704e-04 - val_loss: 1.8252e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.3318e-04 - val_loss: 2.2397e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.2684e-04 - val_loss: 1.6299e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.2142e-04 - val_loss: 1.4300e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.1306e-04 - val_loss: 1.3652e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.0386e-04 - val_loss: 1.2666e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.9674e-04 - val_loss: 1.8740e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.9396e-04 - val_loss: 1.3701e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.8912e-04 - val_loss: 1.2712e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.8413e-04 - val_loss: 1.3218e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.8043e-04 - val_loss: 1.2017e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.7608e-04 - val_loss: 1.2087e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.7262e-04 - val_loss: 1.2998e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.6962e-04 - val_loss: 1.3664e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.6691e-04 - val_loss: 1.4058e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6476e-04 - val_loss: 1.3532e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.6231e-04 - val_loss: 1.3580e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6065e-04 - val_loss: 1.3302e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5823e-04 - val_loss: 1.3481e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.5625e-04 - val_loss: 1.3816e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5527e-04 - val_loss: 1.2871e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.5251e-04 - val_loss: 1.1916e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5120e-04 - val_loss: 1.1832e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4859e-04 - val_loss: 1.1884e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4695e-04 - val_loss: 1.1642e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4626e-04 - val_loss: 1.1273e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4411e-04 - val_loss: 1.1073e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4381e-04 - val_loss: 1.1410e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4229e-04 - val_loss: 1.1119e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4088e-04 - val_loss: 1.1155e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3935e-04 - val_loss: 1.0826e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3852e-04 - val_loss: 1.0532e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3831e-04 - val_loss: 1.0426e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3727e-04 - val_loss: 1.0449e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3694e-04 - val_loss: 1.0483e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.3616e-04 - val_loss: 1.0146e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.3512e-04 - val_loss: 9.7456e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3407e-04 - val_loss: 9.7846e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3375e-04 - val_loss: 9.5029e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3244e-04 - val_loss: 9.2609e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3237e-04 - val_loss: 8.7143e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3146e-04 - val_loss: 8.9417e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3060e-04 - val_loss: 8.5368e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2939e-04 - val_loss: 8.7362e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2663e-04 - val_loss: 8.3911e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2802e-04 - val_loss: 8.2762e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2472e-04 - val_loss: 7.7324e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2443e-04 - val_loss: 7.7871e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2418e-04 - val_loss: 7.5930e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2263e-04 - val_loss: 7.6995e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2095e-04 - val_loss: 7.6029e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2018e-04 - val_loss: 7.3757e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1942e-04 - val_loss: 7.3068e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1847e-04 - val_loss: 7.4415e-05\n",
      "Validation RMSE:  0.008626414 \n",
      "\n",
      "Model took 1327.22 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 31s 793us/step - loss: 0.0056 - val_loss: 0.0034\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 6.0979e-04 - val_loss: 7.3963e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 5.0370e-04 - val_loss: 3.8120e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 4.4090e-04 - val_loss: 4.0155e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 4.0276e-04 - val_loss: 3.2684e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.7323e-04 - val_loss: 3.2689e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.5427e-04 - val_loss: 3.0606e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 3.3459e-04 - val_loss: 2.9087e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 3.2130e-04 - val_loss: 2.7923e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 3.0694e-04 - val_loss: 2.7121e-04\n",
      "Epoch 12/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 18s 470us/step - loss: 2.9231e-04 - val_loss: 2.7785e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.8169e-04 - val_loss: 2.7147e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 2.7183e-04 - val_loss: 2.6832e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.6387e-04 - val_loss: 2.8013e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.5296e-04 - val_loss: 2.4548e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.3929e-04 - val_loss: 1.9612e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.2929e-04 - val_loss: 1.6816e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.2379e-04 - val_loss: 1.4629e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.1726e-04 - val_loss: 1.4050e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.0914e-04 - val_loss: 1.3329e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.0124e-04 - val_loss: 1.3096e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.9476e-04 - val_loss: 1.2100e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.8820e-04 - val_loss: 1.1481e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 469us/step - loss: 1.8221e-04 - val_loss: 1.1222e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.7734e-04 - val_loss: 1.0818e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.7327e-04 - val_loss: 1.0560e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.6982e-04 - val_loss: 1.0313e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.6641e-04 - val_loss: 1.0677e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.6349e-04 - val_loss: 9.8397e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.6055e-04 - val_loss: 9.6910e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5744e-04 - val_loss: 9.6874e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5586e-04 - val_loss: 9.4891e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.5373e-04 - val_loss: 9.6082e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.5197e-04 - val_loss: 9.6682e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5008e-04 - val_loss: 9.6536e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4853e-04 - val_loss: 9.6231e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4698e-04 - val_loss: 9.5959e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.4493e-04 - val_loss: 9.5382e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4346e-04 - val_loss: 9.4095e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4074e-04 - val_loss: 9.4782e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4022e-04 - val_loss: 9.3914e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.3866e-04 - val_loss: 9.2272e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3756e-04 - val_loss: 9.2432e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3615e-04 - val_loss: 8.9082e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3549e-04 - val_loss: 8.8899e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.3428e-04 - val_loss: 8.8064e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.3337e-04 - val_loss: 8.7020e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.3145e-04 - val_loss: 8.5033e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3062e-04 - val_loss: 8.3970e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2958e-04 - val_loss: 8.3054e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.2863e-04 - val_loss: 8.2905e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2777e-04 - val_loss: 8.3038e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2618e-04 - val_loss: 8.4116e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2536e-04 - val_loss: 8.4253e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2437e-04 - val_loss: 8.6085e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2290e-04 - val_loss: 8.5868e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2253e-04 - val_loss: 8.4636e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2126e-04 - val_loss: 8.6399e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.1982e-04 - val_loss: 8.6532e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.1903e-04 - val_loss: 8.7637e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2055e-04 - val_loss: 8.8027e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1927e-04 - val_loss: 8.8148e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1913e-04 - val_loss: 8.8009e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1751e-04 - val_loss: 8.7288e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 1.1641e-04 - val_loss: 8.9794e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1604e-04 - val_loss: 9.0037e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1499e-04 - val_loss: 8.8965e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1375e-04 - val_loss: 8.7549e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1323e-04 - val_loss: 8.8755e-05\n",
      "Validation RMSE:  0.009420991 \n",
      "\n",
      "Model took 1318.73 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 31s 795us/step - loss: 0.0054 - val_loss: 0.0038\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 7.1382e-04 - val_loss: 8.1121e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 5.3172e-04 - val_loss: 4.1281e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 4.5901e-04 - val_loss: 3.4893e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 4.0466e-04 - val_loss: 3.3149e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 3.7157e-04 - val_loss: 3.2170e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.5173e-04 - val_loss: 2.5696e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.3419e-04 - val_loss: 2.4420e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 3.2248e-04 - val_loss: 2.4200e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.1203e-04 - val_loss: 2.4078e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.0471e-04 - val_loss: 2.3638e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.9614e-04 - val_loss: 2.2810e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.8877e-04 - val_loss: 2.1834e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.8066e-04 - val_loss: 2.2329e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.7116e-04 - val_loss: 2.2995e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.6250e-04 - val_loss: 2.2522e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.5428e-04 - val_loss: 2.1995e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.4495e-04 - val_loss: 2.1864e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.3827e-04 - val_loss: 2.0899e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.3049e-04 - val_loss: 2.0635e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.2144e-04 - val_loss: 1.9503e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.1200e-04 - val_loss: 1.7821e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.0616e-04 - val_loss: 1.6714e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.9908e-04 - val_loss: 1.5788e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.9523e-04 - val_loss: 1.5799e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.9049e-04 - val_loss: 1.6937e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.8513e-04 - val_loss: 1.6356e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.8129e-04 - val_loss: 1.2181e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.7540e-04 - val_loss: 1.2665e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.7331e-04 - val_loss: 1.2103e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.7031e-04 - val_loss: 1.1529e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6684e-04 - val_loss: 1.0994e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.6405e-04 - val_loss: 1.0828e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.6151e-04 - val_loss: 1.0737e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5851e-04 - val_loss: 1.0881e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.5613e-04 - val_loss: 1.0789e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.5355e-04 - val_loss: 1.0960e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.5204e-04 - val_loss: 1.0697e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.5019e-04 - val_loss: 1.0505e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4797e-04 - val_loss: 1.0405e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4632e-04 - val_loss: 1.0463e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4394e-04 - val_loss: 1.0288e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4281e-04 - val_loss: 1.0166e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4093e-04 - val_loss: 1.0235e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4036e-04 - val_loss: 1.0152e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3899e-04 - val_loss: 1.0055e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3800e-04 - val_loss: 1.0078e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3712e-04 - val_loss: 1.0076e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3597e-04 - val_loss: 1.0053e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3489e-04 - val_loss: 1.0145e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3395e-04 - val_loss: 1.0094e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3253e-04 - val_loss: 1.0212e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3137e-04 - val_loss: 1.0278e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.3028e-04 - val_loss: 1.0263e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2917e-04 - val_loss: 1.0159e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.2812e-04 - val_loss: 1.0192e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2680e-04 - val_loss: 1.0315e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2589e-04 - val_loss: 1.0300e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2469e-04 - val_loss: 1.0646e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2354e-04 - val_loss: 1.0370e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2217e-04 - val_loss: 1.0675e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2111e-04 - val_loss: 1.0811e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1984e-04 - val_loss: 1.0845e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1835e-04 - val_loss: 1.0687e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1665e-04 - val_loss: 1.0529e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1549e-04 - val_loss: 1.0546e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1493e-04 - val_loss: 1.0343e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1375e-04 - val_loss: 1.0347e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1240e-04 - val_loss: 9.6879e-05\n",
      "Validation RMSE:  0.009842729 \n",
      "\n",
      "Model took 1325.87 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 31s 804us/step - loss: 0.0051 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 6.4196e-04 - val_loss: 4.5844e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 5.2425e-04 - val_loss: 3.3063e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 4.5835e-04 - val_loss: 3.7672e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 4.1099e-04 - val_loss: 4.1223e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.8116e-04 - val_loss: 3.6813e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 3.5646e-04 - val_loss: 3.6300e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 3.3740e-04 - val_loss: 3.4329e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.2087e-04 - val_loss: 3.1929e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.0959e-04 - val_loss: 3.2631e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.9711e-04 - val_loss: 3.0176e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.8568e-04 - val_loss: 3.2543e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.7863e-04 - val_loss: 2.9438e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.7060e-04 - val_loss: 2.7899e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.6255e-04 - val_loss: 2.7551e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.5390e-04 - val_loss: 2.7032e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.4565e-04 - val_loss: 2.7263e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.3861e-04 - val_loss: 2.3481e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.3097e-04 - val_loss: 2.4046e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.2605e-04 - val_loss: 2.2668e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.2139e-04 - val_loss: 2.2328e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.1643e-04 - val_loss: 2.0710e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.0686e-04 - val_loss: 2.1574e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.0061e-04 - val_loss: 1.8089e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.9537e-04 - val_loss: 1.7592e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.8790e-04 - val_loss: 1.9935e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.8293e-04 - val_loss: 1.3454e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.7569e-04 - val_loss: 1.2430e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.6811e-04 - val_loss: 1.4396e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6361e-04 - val_loss: 1.4224e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.5989e-04 - val_loss: 1.3600e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.5735e-04 - val_loss: 1.3032e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.5484e-04 - val_loss: 1.2316e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5207e-04 - val_loss: 1.1874e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4961e-04 - val_loss: 1.1454e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4611e-04 - val_loss: 1.1198e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4490e-04 - val_loss: 1.0058e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4445e-04 - val_loss: 9.8759e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4277e-04 - val_loss: 9.3893e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3923e-04 - val_loss: 9.3311e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3802e-04 - val_loss: 9.2350e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3613e-04 - val_loss: 8.9832e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3583e-04 - val_loss: 8.9568e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3370e-04 - val_loss: 8.3779e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3263e-04 - val_loss: 8.3334e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3138e-04 - val_loss: 8.4312e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3064e-04 - val_loss: 8.6537e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2971e-04 - val_loss: 8.5267e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2748e-04 - val_loss: 8.3772e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2637e-04 - val_loss: 8.4429e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2631e-04 - val_loss: 8.5105e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2461e-04 - val_loss: 8.3939e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2479e-04 - val_loss: 8.3924e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2382e-04 - val_loss: 8.5234e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2291e-04 - val_loss: 8.5785e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2220e-04 - val_loss: 8.5738e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2174e-04 - val_loss: 8.4556e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2052e-04 - val_loss: 8.4498e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1894e-04 - val_loss: 8.5306e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1781e-04 - val_loss: 8.3762e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1818e-04 - val_loss: 8.3265e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1616e-04 - val_loss: 8.1110e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1610e-04 - val_loss: 8.5202e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1462e-04 - val_loss: 8.6185e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1379e-04 - val_loss: 9.1353e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1262e-04 - val_loss: 8.4376e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1172e-04 - val_loss: 8.2317e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1122e-04 - val_loss: 8.4368e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1161e-04 - val_loss: 8.5132e-05\n",
      "Validation RMSE:  0.009226684 \n",
      "\n",
      "Model took 1329.67 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 31s 795us/step - loss: 0.0060 - val_loss: 0.0039\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 7.2255e-04 - val_loss: 0.0014\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 5.1427e-04 - val_loss: 5.1049e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 4.3126e-04 - val_loss: 5.2710e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 3.8482e-04 - val_loss: 3.9127e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.5371e-04 - val_loss: 3.2232e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.3314e-04 - val_loss: 3.0553e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 18s 470us/step - loss: 3.2003e-04 - val_loss: 2.9681e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.0709e-04 - val_loss: 3.0011e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 2.9535e-04 - val_loss: 2.8197e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.8648e-04 - val_loss: 2.7716e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.7716e-04 - val_loss: 2.6902e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.6763e-04 - val_loss: 2.5508e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.5870e-04 - val_loss: 2.4916e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.4937e-04 - val_loss: 2.4009e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 2.4408e-04 - val_loss: 2.4326e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.3625e-04 - val_loss: 2.3216e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.2580e-04 - val_loss: 2.0127e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.1475e-04 - val_loss: 1.6833e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.1024e-04 - val_loss: 1.5508e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.0356e-04 - val_loss: 1.4451e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.9228e-04 - val_loss: 1.2558e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.8223e-04 - val_loss: 1.1384e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.7636e-04 - val_loss: 9.9749e-05\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.7146e-04 - val_loss: 9.9510e-05\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.6771e-04 - val_loss: 9.8688e-05\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.6407e-04 - val_loss: 9.7973e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.6141e-04 - val_loss: 9.4533e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5880e-04 - val_loss: 9.1639e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5588e-04 - val_loss: 9.1752e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.5314e-04 - val_loss: 9.0501e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.5071e-04 - val_loss: 9.0220e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4728e-04 - val_loss: 8.8872e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4629e-04 - val_loss: 8.6185e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.4369e-04 - val_loss: 8.4573e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4212e-04 - val_loss: 8.5392e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4074e-04 - val_loss: 8.6979e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3867e-04 - val_loss: 8.5982e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.3726e-04 - val_loss: 8.5699e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3586e-04 - val_loss: 8.3220e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.3451e-04 - val_loss: 8.3123e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.3244e-04 - val_loss: 8.1968e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.3200e-04 - val_loss: 8.1576e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3038e-04 - val_loss: 7.9985e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2987e-04 - val_loss: 7.9788e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2878e-04 - val_loss: 8.0050e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2760e-04 - val_loss: 7.9160e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2618e-04 - val_loss: 7.8154e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2454e-04 - val_loss: 7.4906e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2224e-04 - val_loss: 7.6126e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2121e-04 - val_loss: 7.6062e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2004e-04 - val_loss: 7.7766e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1917e-04 - val_loss: 7.7654e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1728e-04 - val_loss: 7.7863e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1737e-04 - val_loss: 7.7469e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1679e-04 - val_loss: 7.8461e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1582e-04 - val_loss: 7.8586e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1505e-04 - val_loss: 7.9492e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1397e-04 - val_loss: 7.7538e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1340e-04 - val_loss: 7.8006e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1194e-04 - val_loss: 7.8278e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1194e-04 - val_loss: 8.0152e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1213e-04 - val_loss: 7.5553e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1137e-04 - val_loss: 7.5046e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 18s 471us/step - loss: 1.1082e-04 - val_loss: 7.5328e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.0980e-04 - val_loss: 7.6302e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 472us/step - loss: 1.0917e-04 - val_loss: 7.5915e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.0880e-04 - val_loss: 7.3372e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.0649e-04 - val_loss: 7.4563e-05\n",
      "Validation RMSE:  0.008635005 \n",
      "\n",
      "Model took 1322.55 seconds to train\n",
      "17 \t6     \t0.00893584\t0.000415765\t0.00856956\t0.00984273\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 32s 813us/step - loss: 0.0052 - val_loss: 0.0032\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 7.0703e-04 - val_loss: 5.1169e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 5.5729e-04 - val_loss: 4.0552e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 4.8542e-04 - val_loss: 3.5935e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 4.3352e-04 - val_loss: 3.0600e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.9801e-04 - val_loss: 2.7355e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 3.7464e-04 - val_loss: 2.5516e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.5805e-04 - val_loss: 2.4659e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.4512e-04 - val_loss: 2.5076e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.3257e-04 - val_loss: 2.4215e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.2167e-04 - val_loss: 2.3926e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.0798e-04 - val_loss: 2.3580e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.9782e-04 - val_loss: 2.3788e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.8920e-04 - val_loss: 2.4229e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.7760e-04 - val_loss: 2.3828e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.6919e-04 - val_loss: 2.3566e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.6116e-04 - val_loss: 2.2877e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.5251e-04 - val_loss: 2.2259e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.3782e-04 - val_loss: 2.1471e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.2416e-04 - val_loss: 1.8957e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.2037e-04 - val_loss: 1.7855e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.1247e-04 - val_loss: 1.5089e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.0191e-04 - val_loss: 1.4383e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.9545e-04 - val_loss: 1.4068e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.8938e-04 - val_loss: 1.3040e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.8266e-04 - val_loss: 1.1999e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.7548e-04 - val_loss: 1.1726e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.6900e-04 - val_loss: 1.1848e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.6281e-04 - val_loss: 1.1803e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5716e-04 - val_loss: 1.0571e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5323e-04 - val_loss: 1.0695e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5126e-04 - val_loss: 1.0322e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4862e-04 - val_loss: 1.0157e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4587e-04 - val_loss: 9.8281e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4352e-04 - val_loss: 9.7771e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.4103e-04 - val_loss: 1.0103e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3906e-04 - val_loss: 1.0044e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3734e-04 - val_loss: 9.9711e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3591e-04 - val_loss: 9.5607e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3496e-04 - val_loss: 9.7519e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3337e-04 - val_loss: 9.6385e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3142e-04 - val_loss: 9.4477e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2891e-04 - val_loss: 9.4801e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2736e-04 - val_loss: 9.5686e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2623e-04 - val_loss: 9.5247e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2502e-04 - val_loss: 9.6938e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2388e-04 - val_loss: 9.4858e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2190e-04 - val_loss: 9.3676e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2123e-04 - val_loss: 9.0119e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1985e-04 - val_loss: 8.8102e-05\n",
      "Epoch 52/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1857e-04 - val_loss: 8.7890e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1769e-04 - val_loss: 8.6653e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1609e-04 - val_loss: 8.5780e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1625e-04 - val_loss: 8.3738e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1499e-04 - val_loss: 8.1792e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1445e-04 - val_loss: 8.1693e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1297e-04 - val_loss: 7.9747e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1231e-04 - val_loss: 7.7039e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1123e-04 - val_loss: 7.7069e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1043e-04 - val_loss: 7.7517e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.0926e-04 - val_loss: 7.7911e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.0846e-04 - val_loss: 7.7186e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.0755e-04 - val_loss: 7.7778e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.0674e-04 - val_loss: 7.4413e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.0505e-04 - val_loss: 7.4226e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.0429e-04 - val_loss: 7.4214e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.0356e-04 - val_loss: 7.4861e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.0232e-04 - val_loss: 7.3458e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.0216e-04 - val_loss: 7.1850e-05\n",
      "Validation RMSE:  0.008476414 \n",
      "\n",
      "Model took 1334.66 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 32s 813us/step - loss: 0.0070 - val_loss: 0.0040\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 7.9246e-04 - val_loss: 6.5791e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 5.9963e-04 - val_loss: 2.8348e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 5.1504e-04 - val_loss: 2.4337e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 4.5967e-04 - val_loss: 2.6130e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 4.1963e-04 - val_loss: 3.0923e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.8136e-04 - val_loss: 2.6585e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.6109e-04 - val_loss: 3.0381e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 3.4343e-04 - val_loss: 2.9291e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 3.2453e-04 - val_loss: 2.7082e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 3.1592e-04 - val_loss: 2.7388e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.9676e-04 - val_loss: 2.8215e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.8329e-04 - val_loss: 3.1397e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.7491e-04 - val_loss: 2.8661e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.6711e-04 - val_loss: 2.7090e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.5991e-04 - val_loss: 2.5863e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.5257e-04 - val_loss: 2.4843e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.4537e-04 - val_loss: 2.5255e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.3692e-04 - val_loss: 2.6364e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.2140e-04 - val_loss: 2.1676e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.1418e-04 - val_loss: 1.9791e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.0241e-04 - val_loss: 1.7133e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.9199e-04 - val_loss: 1.7323e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.8756e-04 - val_loss: 1.3435e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.8171e-04 - val_loss: 1.2974e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.7655e-04 - val_loss: 1.2984e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.7125e-04 - val_loss: 1.1906e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.6861e-04 - val_loss: 1.0690e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.6432e-04 - val_loss: 1.0929e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.6159e-04 - val_loss: 1.0752e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5827e-04 - val_loss: 1.0472e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.5474e-04 - val_loss: 9.5322e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5027e-04 - val_loss: 9.3972e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.4838e-04 - val_loss: 9.7562e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4852e-04 - val_loss: 9.5891e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4620e-04 - val_loss: 9.1618e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4427e-04 - val_loss: 8.9347e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4363e-04 - val_loss: 9.0309e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4202e-04 - val_loss: 8.8721e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3958e-04 - val_loss: 8.6497e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3991e-04 - val_loss: 8.3328e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3839e-04 - val_loss: 8.8764e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3692e-04 - val_loss: 8.8944e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3513e-04 - val_loss: 8.9627e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3225e-04 - val_loss: 9.0752e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3066e-04 - val_loss: 8.6347e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3019e-04 - val_loss: 8.7591e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2847e-04 - val_loss: 8.4472e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2569e-04 - val_loss: 8.6413e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2608e-04 - val_loss: 8.3316e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2533e-04 - val_loss: 8.3162e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2395e-04 - val_loss: 8.4403e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2331e-04 - val_loss: 8.3561e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2235e-04 - val_loss: 8.3425e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2172e-04 - val_loss: 8.3214e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2134e-04 - val_loss: 8.3804e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2052e-04 - val_loss: 8.3486e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1999e-04 - val_loss: 8.2071e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1906e-04 - val_loss: 8.4042e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1871e-04 - val_loss: 8.4237e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1787e-04 - val_loss: 8.3530e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1601e-04 - val_loss: 8.3807e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1568e-04 - val_loss: 8.2767e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1518e-04 - val_loss: 8.5283e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1525e-04 - val_loss: 8.4401e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1395e-04 - val_loss: 8.4091e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1356e-04 - val_loss: 8.2529e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1288e-04 - val_loss: 8.3367e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1249e-04 - val_loss: 8.1746e-05\n",
      "Validation RMSE:  0.009041354 \n",
      "\n",
      "Model took 1334.36 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 32s 829us/step - loss: 0.0063 - val_loss: 0.0043\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 6.5478e-04 - val_loss: 6.0962e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 4.9641e-04 - val_loss: 3.7517e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 4.3341e-04 - val_loss: 3.8279e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.9765e-04 - val_loss: 2.8809e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.7221e-04 - val_loss: 2.6402e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.5008e-04 - val_loss: 2.6287e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.3686e-04 - val_loss: 2.5227e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 3.2670e-04 - val_loss: 2.5144e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.1634e-04 - val_loss: 2.4845e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.0703e-04 - val_loss: 2.3640e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.9709e-04 - val_loss: 2.1742e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.9005e-04 - val_loss: 2.3112e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.8271e-04 - val_loss: 2.3121e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.7302e-04 - val_loss: 2.1602e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.6437e-04 - val_loss: 2.1534e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.5701e-04 - val_loss: 1.9894e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.5080e-04 - val_loss: 2.0654e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.4155e-04 - val_loss: 2.1304e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.3957e-04 - val_loss: 2.0552e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.3403e-04 - val_loss: 1.9594e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.2434e-04 - val_loss: 1.8322e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.1389e-04 - val_loss: 1.6699e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.0861e-04 - val_loss: 1.5406e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 2.0160e-04 - val_loss: 1.7236e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.9252e-04 - val_loss: 1.2053e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.8415e-04 - val_loss: 1.1675e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.7752e-04 - val_loss: 1.0145e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.7267e-04 - val_loss: 9.6113e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.6701e-04 - val_loss: 9.4638e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.6205e-04 - val_loss: 9.4346e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.5820e-04 - val_loss: 9.5638e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.5492e-04 - val_loss: 9.3180e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.5206e-04 - val_loss: 9.2576e-05\n",
      "Epoch 36/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.4965e-04 - val_loss: 8.9677e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4671e-04 - val_loss: 8.9055e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4520e-04 - val_loss: 8.9216e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4269e-04 - val_loss: 8.5439e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4112e-04 - val_loss: 8.3966e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3997e-04 - val_loss: 7.9525e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3954e-04 - val_loss: 8.1423e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.3881e-04 - val_loss: 8.3937e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3680e-04 - val_loss: 8.0763e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3639e-04 - val_loss: 8.0788e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3442e-04 - val_loss: 8.2290e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3307e-04 - val_loss: 8.1813e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3136e-04 - val_loss: 8.1304e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3171e-04 - val_loss: 7.9382e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3016e-04 - val_loss: 8.1887e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2908e-04 - val_loss: 8.3624e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2897e-04 - val_loss: 8.4863e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2785e-04 - val_loss: 8.2223e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2709e-04 - val_loss: 8.3190e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2580e-04 - val_loss: 8.1919e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2430e-04 - val_loss: 8.1894e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2338e-04 - val_loss: 7.9893e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1900e-04 - val_loss: 9.9709e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2081e-04 - val_loss: 8.1826e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2029e-04 - val_loss: 8.3802e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1879e-04 - val_loss: 8.0794e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1843e-04 - val_loss: 8.3616e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1735e-04 - val_loss: 8.4262e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1665e-04 - val_loss: 8.3263e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 1.1536e-04 - val_loss: 8.4920e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1433e-04 - val_loss: 8.5212e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1274e-04 - val_loss: 8.2556e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1200e-04 - val_loss: 8.5176e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1099e-04 - val_loss: 8.4648e-05\n",
      "Validation RMSE:  0.009200459 \n",
      "\n",
      "Model took 1308.48 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 32s 818us/step - loss: 0.0069 - val_loss: 0.0041\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 6.8458e-04 - val_loss: 9.1146e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 5.0118e-04 - val_loss: 4.0404e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 4.2940e-04 - val_loss: 5.1979e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.8558e-04 - val_loss: 4.0872e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.5463e-04 - val_loss: 4.0054e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 3.3491e-04 - val_loss: 3.7075e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.1861e-04 - val_loss: 3.4821e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.0776e-04 - val_loss: 3.1864e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.9519e-04 - val_loss: 3.0715e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.8718e-04 - val_loss: 3.0127e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.7737e-04 - val_loss: 3.0028e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.6312e-04 - val_loss: 2.8648e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.4421e-04 - val_loss: 2.9030e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.3567e-04 - val_loss: 1.9543e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 18s 472us/step - loss: 2.3156e-04 - val_loss: 1.5334e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.2046e-04 - val_loss: 1.5222e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 2.0553e-04 - val_loss: 1.8086e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.9498e-04 - val_loss: 1.3905e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.8550e-04 - val_loss: 1.2409e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.8002e-04 - val_loss: 1.2778e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.7359e-04 - val_loss: 1.2159e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.6791e-04 - val_loss: 1.1345e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6383e-04 - val_loss: 1.0470e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6022e-04 - val_loss: 1.0562e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.5708e-04 - val_loss: 1.0212e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.5404e-04 - val_loss: 9.6924e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.5093e-04 - val_loss: 9.5926e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4822e-04 - val_loss: 9.0758e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4581e-04 - val_loss: 9.0462e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4394e-04 - val_loss: 8.9573e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4189e-04 - val_loss: 9.1970e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3937e-04 - val_loss: 9.0784e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3772e-04 - val_loss: 8.9785e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3604e-04 - val_loss: 9.0200e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3320e-04 - val_loss: 9.0842e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3184e-04 - val_loss: 8.7032e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3053e-04 - val_loss: 8.8204e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2934e-04 - val_loss: 9.1762e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2703e-04 - val_loss: 9.0839e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2623e-04 - val_loss: 9.4372e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2529e-04 - val_loss: 9.7067e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2398e-04 - val_loss: 9.7403e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2252e-04 - val_loss: 9.7428e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2108e-04 - val_loss: 9.7798e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2028e-04 - val_loss: 9.7838e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1971e-04 - val_loss: 9.7834e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1925e-04 - val_loss: 9.7830e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1846e-04 - val_loss: 9.7178e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1742e-04 - val_loss: 9.2191e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1709e-04 - val_loss: 9.3391e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1633e-04 - val_loss: 9.5067e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1495e-04 - val_loss: 9.5077e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1356e-04 - val_loss: 9.2175e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1345e-04 - val_loss: 9.8501e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1274e-04 - val_loss: 9.7335e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1231e-04 - val_loss: 9.9348e-05\n",
      "Validation RMSE:  0.009967351 \n",
      "\n",
      "Model took 1104.19 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 33s 837us/step - loss: 0.0054 - val_loss: 0.0041\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 6.8722e-04 - val_loss: 8.1723e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 5.2430e-04 - val_loss: 4.1824e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 4.4562e-04 - val_loss: 3.4916e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.9856e-04 - val_loss: 3.3265e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.7087e-04 - val_loss: 3.1766e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.4981e-04 - val_loss: 3.0502e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.3530e-04 - val_loss: 3.0541e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 3.2242e-04 - val_loss: 2.9379e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.0952e-04 - val_loss: 2.9209e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.0206e-04 - val_loss: 2.8824e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.9195e-04 - val_loss: 2.7692e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.8327e-04 - val_loss: 2.8386e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.7447e-04 - val_loss: 2.7083e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.6721e-04 - val_loss: 2.5799e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.5869e-04 - val_loss: 2.4367e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.5016e-04 - val_loss: 2.2512e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.4350e-04 - val_loss: 2.4060e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.3661e-04 - val_loss: 2.4191e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.2460e-04 - val_loss: 2.1776e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.1726e-04 - val_loss: 1.8716e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.1235e-04 - val_loss: 1.7015e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.0586e-04 - val_loss: 2.0251e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.0193e-04 - val_loss: 1.7511e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.9295e-04 - val_loss: 1.6692e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.8813e-04 - val_loss: 1.8722e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.8548e-04 - val_loss: 1.4707e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.7735e-04 - val_loss: 1.4492e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.7194e-04 - val_loss: 1.3392e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.6687e-04 - val_loss: 1.3429e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.6207e-04 - val_loss: 1.2982e-04\n",
      "Epoch 33/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5774e-04 - val_loss: 1.2519e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.5463e-04 - val_loss: 1.1858e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.5101e-04 - val_loss: 1.1841e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4821e-04 - val_loss: 1.2084e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4546e-04 - val_loss: 1.1842e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.4284e-04 - val_loss: 1.1296e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4225e-04 - val_loss: 1.0293e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3966e-04 - val_loss: 9.7577e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3868e-04 - val_loss: 9.8881e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3619e-04 - val_loss: 9.5384e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3387e-04 - val_loss: 9.5562e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3291e-04 - val_loss: 9.3095e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3163e-04 - val_loss: 9.3035e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3055e-04 - val_loss: 9.3200e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2982e-04 - val_loss: 8.9229e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2866e-04 - val_loss: 8.8868e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2900e-04 - val_loss: 8.6673e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2753e-04 - val_loss: 8.9741e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2694e-04 - val_loss: 8.6092e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2630e-04 - val_loss: 8.4935e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2523e-04 - val_loss: 8.7199e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2440e-04 - val_loss: 8.5401e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2320e-04 - val_loss: 8.4718e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2226e-04 - val_loss: 8.5519e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2012e-04 - val_loss: 8.3952e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1915e-04 - val_loss: 8.7143e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1799e-04 - val_loss: 8.5721e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1661e-04 - val_loss: 8.5642e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1490e-04 - val_loss: 8.7678e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1526e-04 - val_loss: 8.9203e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1350e-04 - val_loss: 8.8754e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1257e-04 - val_loss: 9.0411e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1159e-04 - val_loss: 8.8079e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1132e-04 - val_loss: 9.0654e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1047e-04 - val_loss: 9.3148e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1017e-04 - val_loss: 9.2167e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.0888e-04 - val_loss: 9.2718e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.0739e-04 - val_loss: 1.0227e-04\n",
      "Validation RMSE:  0.010113067 \n",
      "\n",
      "Model took 1340.51 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 33s 850us/step - loss: 0.0069 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 7.2991e-04 - val_loss: 6.7561e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 5.5851e-04 - val_loss: 4.7061e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 4.9224e-04 - val_loss: 4.1059e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 4.4244e-04 - val_loss: 4.0897e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 4.0296e-04 - val_loss: 3.5483e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 3.7686e-04 - val_loss: 3.1247e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.6449e-04 - val_loss: 3.2106e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 3.5094e-04 - val_loss: 3.0125e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 3.4108e-04 - val_loss: 2.8574e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.3255e-04 - val_loss: 2.7879e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.2425e-04 - val_loss: 2.5110e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.1507e-04 - val_loss: 2.7234e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.0449e-04 - val_loss: 2.6059e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.9370e-04 - val_loss: 2.5696e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.8294e-04 - val_loss: 2.4480e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.7251e-04 - val_loss: 2.3237e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.5810e-04 - val_loss: 2.2800e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.4805e-04 - val_loss: 2.1085e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.4151e-04 - val_loss: 1.6118e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.3096e-04 - val_loss: 1.6052e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.1595e-04 - val_loss: 1.6015e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.0859e-04 - val_loss: 1.3013e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.9902e-04 - val_loss: 1.1798e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.9210e-04 - val_loss: 1.1052e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.8646e-04 - val_loss: 1.0811e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.8135e-04 - val_loss: 1.1482e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.7606e-04 - val_loss: 1.1488e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.7199e-04 - val_loss: 1.1251e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.6705e-04 - val_loss: 1.1098e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.6305e-04 - val_loss: 1.0644e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6042e-04 - val_loss: 1.0441e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5846e-04 - val_loss: 1.0617e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.5420e-04 - val_loss: 1.0197e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5176e-04 - val_loss: 9.5951e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4881e-04 - val_loss: 9.8768e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4667e-04 - val_loss: 9.6514e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.4431e-04 - val_loss: 9.5088e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4271e-04 - val_loss: 9.5189e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4053e-04 - val_loss: 9.4338e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3819e-04 - val_loss: 9.9506e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3672e-04 - val_loss: 9.7980e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3532e-04 - val_loss: 9.4621e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3376e-04 - val_loss: 9.4136e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3267e-04 - val_loss: 9.6060e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3155e-04 - val_loss: 9.3828e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3017e-04 - val_loss: 9.6345e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2914e-04 - val_loss: 9.6006e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2750e-04 - val_loss: 9.3876e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2662e-04 - val_loss: 9.4512e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2569e-04 - val_loss: 9.1198e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2371e-04 - val_loss: 9.4897e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2332e-04 - val_loss: 9.0318e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.2225e-04 - val_loss: 9.4439e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2149e-04 - val_loss: 9.1530e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.2042e-04 - val_loss: 9.4777e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2007e-04 - val_loss: 9.1939e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1877e-04 - val_loss: 9.4469e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1743e-04 - val_loss: 9.2083e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1552e-04 - val_loss: 9.4539e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1409e-04 - val_loss: 9.6675e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1224e-04 - val_loss: 9.5204e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1161e-04 - val_loss: 9.9557e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.1110e-04 - val_loss: 9.3494e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1029e-04 - val_loss: 9.3707e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.0855e-04 - val_loss: 8.7982e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.0778e-04 - val_loss: 9.3920e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.0809e-04 - val_loss: 9.3141e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.0731e-04 - val_loss: 8.9009e-05\n",
      "Validation RMSE:  0.009434437 \n",
      "\n",
      "Model took 1330.65 seconds to train\n",
      "18 \t6     \t0.00905682\t0.000578152\t0.00847641\t0.0101131 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 33s 849us/step - loss: 0.0050 - val_loss: 0.0036\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 6.8110e-04 - val_loss: 8.1697e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 5.3457e-04 - val_loss: 4.0707e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 4.7417e-04 - val_loss: 4.0686e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 4.3259e-04 - val_loss: 4.0017e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.9733e-04 - val_loss: 4.2218e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.8149e-04 - val_loss: 3.1324e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.6101e-04 - val_loss: 2.9573e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.4249e-04 - val_loss: 2.8477e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.3273e-04 - val_loss: 2.7125e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.1979e-04 - val_loss: 2.5976e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.1328e-04 - val_loss: 2.6106e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.0658e-04 - val_loss: 2.4414e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.9800e-04 - val_loss: 2.3731e-04\n",
      "Epoch 16/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.8931e-04 - val_loss: 2.3768e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.8113e-04 - val_loss: 2.3552e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.7163e-04 - val_loss: 2.2422e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.6215e-04 - val_loss: 2.1417e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.5122e-04 - val_loss: 2.0311e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.3704e-04 - val_loss: 2.0549e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.2589e-04 - val_loss: 2.2211e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.2599e-04 - val_loss: 2.0375e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.1560e-04 - val_loss: 2.1092e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.0600e-04 - val_loss: 1.5857e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.9582e-04 - val_loss: 1.9168e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.8786e-04 - val_loss: 1.4095e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.7939e-04 - val_loss: 1.3449e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.7317e-04 - val_loss: 1.2907e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.6776e-04 - val_loss: 1.2592e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.6306e-04 - val_loss: 1.2692e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.5871e-04 - val_loss: 1.1473e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5628e-04 - val_loss: 1.1907e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5302e-04 - val_loss: 1.1725e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.5084e-04 - val_loss: 1.1670e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.4820e-04 - val_loss: 1.1958e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4747e-04 - val_loss: 1.1225e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.4314e-04 - val_loss: 1.1088e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4287e-04 - val_loss: 1.0536e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3995e-04 - val_loss: 9.7710e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3952e-04 - val_loss: 1.0130e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3708e-04 - val_loss: 1.0566e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3647e-04 - val_loss: 1.0188e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3428e-04 - val_loss: 1.0190e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3394e-04 - val_loss: 9.5973e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3318e-04 - val_loss: 9.3700e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3145e-04 - val_loss: 9.2808e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2984e-04 - val_loss: 9.0498e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2853e-04 - val_loss: 9.1558e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2753e-04 - val_loss: 9.1276e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2801e-04 - val_loss: 9.1669e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2723e-04 - val_loss: 8.9650e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2566e-04 - val_loss: 8.8468e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2521e-04 - val_loss: 8.5504e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2458e-04 - val_loss: 8.7840e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2324e-04 - val_loss: 8.6206e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2291e-04 - val_loss: 8.6667e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2241e-04 - val_loss: 8.6862e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2222e-04 - val_loss: 8.8483e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2056e-04 - val_loss: 8.7681e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1885e-04 - val_loss: 8.8387e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1833e-04 - val_loss: 8.9932e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1781e-04 - val_loss: 8.8222e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1710e-04 - val_loss: 9.0235e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1693e-04 - val_loss: 9.0975e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1551e-04 - val_loss: 9.2709e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1477e-04 - val_loss: 8.8831e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1462e-04 - val_loss: 8.8040e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1394e-04 - val_loss: 8.6318e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1403e-04 - val_loss: 8.5988e-05\n",
      "Validation RMSE:  0.009272991 \n",
      "\n",
      "Model took 1345.84 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 33s 851us/step - loss: 0.0075 - val_loss: 0.0045\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 7.1275e-04 - val_loss: 9.4187e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 5.3732e-04 - val_loss: 4.0292e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 4.6738e-04 - val_loss: 3.3700e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 4.1929e-04 - val_loss: 3.4406e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.8514e-04 - val_loss: 3.1480e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.5727e-04 - val_loss: 2.8381e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.3871e-04 - val_loss: 2.6599e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.2374e-04 - val_loss: 2.6933e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.1022e-04 - val_loss: 2.8023e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.9845e-04 - val_loss: 2.7571e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.9285e-04 - val_loss: 2.5917e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.8352e-04 - val_loss: 2.7353e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.7232e-04 - val_loss: 2.6732e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.6441e-04 - val_loss: 2.7390e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.5767e-04 - val_loss: 2.4165e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 2.4947e-04 - val_loss: 2.5382e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.4432e-04 - val_loss: 2.6090e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.3354e-04 - val_loss: 2.6803e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.2681e-04 - val_loss: 2.2876e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.2003e-04 - val_loss: 2.2577e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.1232e-04 - val_loss: 2.2993e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.0302e-04 - val_loss: 1.6857e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.9391e-04 - val_loss: 1.6186e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.8444e-04 - val_loss: 1.5059e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.7639e-04 - val_loss: 1.5519e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.7298e-04 - val_loss: 1.4534e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.6614e-04 - val_loss: 1.2454e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6485e-04 - val_loss: 1.3697e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.6336e-04 - val_loss: 1.3087e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.6061e-04 - val_loss: 1.3497e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5705e-04 - val_loss: 1.3329e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5590e-04 - val_loss: 1.3662e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5171e-04 - val_loss: 1.3380e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4928e-04 - val_loss: 1.1551e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4767e-04 - val_loss: 1.1197e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4655e-04 - val_loss: 1.0911e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4461e-04 - val_loss: 1.1608e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4255e-04 - val_loss: 1.1658e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4070e-04 - val_loss: 1.2228e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3825e-04 - val_loss: 1.1870e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3515e-04 - val_loss: 1.1197e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3408e-04 - val_loss: 1.0525e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3297e-04 - val_loss: 1.0271e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3236e-04 - val_loss: 1.0424e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3098e-04 - val_loss: 1.0459e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2963e-04 - val_loss: 9.9712e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2763e-04 - val_loss: 9.7196e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2662e-04 - val_loss: 9.7094e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2547e-04 - val_loss: 9.6311e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2329e-04 - val_loss: 9.5444e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2250e-04 - val_loss: 9.2923e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2033e-04 - val_loss: 9.1107e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1924e-04 - val_loss: 8.7645e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1867e-04 - val_loss: 8.5794e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1756e-04 - val_loss: 8.3739e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1575e-04 - val_loss: 8.0472e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1446e-04 - val_loss: 8.0348e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1297e-04 - val_loss: 7.7499e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1110e-04 - val_loss: 7.7415e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.0804e-04 - val_loss: 7.5173e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.0674e-04 - val_loss: 7.6210e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.0647e-04 - val_loss: 7.5993e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.0530e-04 - val_loss: 7.4390e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.0380e-04 - val_loss: 7.4442e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.0225e-04 - val_loss: 7.4616e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.0018e-04 - val_loss: 7.8068e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.0007e-04 - val_loss: 7.5055e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 9.9101e-05 - val_loss: 7.6274e-05\n",
      "Validation RMSE:  0.008733517 \n",
      "\n",
      "Model took 1335.00 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 34s 860us/step - loss: 0.0056 - val_loss: 0.0043\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 7.5528e-04 - val_loss: 7.8887e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 5.4925e-04 - val_loss: 4.4787e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 4.7613e-04 - val_loss: 3.7146e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 4.2423e-04 - val_loss: 3.9801e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.8247e-04 - val_loss: 2.9961e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.5340e-04 - val_loss: 2.5775e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.3495e-04 - val_loss: 2.3841e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.2104e-04 - val_loss: 2.4072e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.0993e-04 - val_loss: 2.3434e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.9938e-04 - val_loss: 2.2943e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.9113e-04 - val_loss: 2.3608e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.8115e-04 - val_loss: 2.2556e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.7088e-04 - val_loss: 2.2624e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.5707e-04 - val_loss: 2.0582e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.4776e-04 - val_loss: 2.7005e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.3947e-04 - val_loss: 2.0461e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.2856e-04 - val_loss: 1.7963e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.1848e-04 - val_loss: 1.7457e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.1023e-04 - val_loss: 1.5878e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.0083e-04 - val_loss: 1.3940e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.9496e-04 - val_loss: 1.3275e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.8755e-04 - val_loss: 1.1642e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.7840e-04 - val_loss: 1.0137e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.7240e-04 - val_loss: 9.7452e-05\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.6712e-04 - val_loss: 9.6395e-05\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.6200e-04 - val_loss: 9.4121e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.5828e-04 - val_loss: 9.8745e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.5539e-04 - val_loss: 9.7093e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5216e-04 - val_loss: 9.5654e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4903e-04 - val_loss: 9.5684e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4593e-04 - val_loss: 9.3746e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4304e-04 - val_loss: 9.6816e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.4193e-04 - val_loss: 9.7276e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3986e-04 - val_loss: 9.6902e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3898e-04 - val_loss: 9.4335e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3788e-04 - val_loss: 9.4308e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3694e-04 - val_loss: 9.3456e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3593e-04 - val_loss: 9.1297e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3474e-04 - val_loss: 9.0639e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3391e-04 - val_loss: 9.1720e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3185e-04 - val_loss: 9.0943e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3124e-04 - val_loss: 9.2227e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3020e-04 - val_loss: 9.2465e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2804e-04 - val_loss: 8.9794e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2771e-04 - val_loss: 9.2467e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2658e-04 - val_loss: 8.8674e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2523e-04 - val_loss: 8.7395e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2407e-04 - val_loss: 8.6665e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2321e-04 - val_loss: 8.8593e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2218e-04 - val_loss: 8.6966e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2177e-04 - val_loss: 8.6184e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2099e-04 - val_loss: 8.6283e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2028e-04 - val_loss: 8.4343e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1998e-04 - val_loss: 8.6191e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1907e-04 - val_loss: 8.5554e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1771e-04 - val_loss: 8.5613e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1462e-04 - val_loss: 9.4292e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1640e-04 - val_loss: 8.7982e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1596e-04 - val_loss: 8.5805e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1578e-04 - val_loss: 8.6780e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1512e-04 - val_loss: 8.8580e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1549e-04 - val_loss: 8.7650e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1419e-04 - val_loss: 8.5544e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1465e-04 - val_loss: 8.9205e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1440e-04 - val_loss: 9.1279e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1419e-04 - val_loss: 9.0024e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1362e-04 - val_loss: 8.6935e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1296e-04 - val_loss: 8.3997e-05\n",
      "Validation RMSE:  0.009165 \n",
      "\n",
      "Model took 1347.94 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 34s 860us/step - loss: 0.0051 - val_loss: 0.0032\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 0.0012 - val_loss: 9.7433e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 6.4550e-04 - val_loss: 5.1674e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 5.2937e-04 - val_loss: 3.7277e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 4.6865e-04 - val_loss: 3.4493e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 4.2592e-04 - val_loss: 3.4304e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.9357e-04 - val_loss: 3.2384e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.7099e-04 - val_loss: 3.0806e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.5699e-04 - val_loss: 2.6594e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 3.4180e-04 - val_loss: 2.5070e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.2930e-04 - val_loss: 2.3893e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 3.1903e-04 - val_loss: 2.3165e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.0945e-04 - val_loss: 2.4461e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.9804e-04 - val_loss: 2.5349e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.8323e-04 - val_loss: 2.3308e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.7204e-04 - val_loss: 2.3207e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.6498e-04 - val_loss: 2.3604e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.5095e-04 - val_loss: 2.2574e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.4075e-04 - val_loss: 1.6942e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.3734e-04 - val_loss: 1.4711e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.2861e-04 - val_loss: 1.4646e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.1806e-04 - val_loss: 1.6209e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.1121e-04 - val_loss: 1.3669e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.0555e-04 - val_loss: 1.1547e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.9971e-04 - val_loss: 1.0858e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.9400e-04 - val_loss: 1.1145e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.8696e-04 - val_loss: 1.0901e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.8292e-04 - val_loss: 1.0286e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.7936e-04 - val_loss: 1.0149e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.7563e-04 - val_loss: 1.0023e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.7227e-04 - val_loss: 1.0361e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.6931e-04 - val_loss: 1.0640e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.6581e-04 - val_loss: 1.2762e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.6401e-04 - val_loss: 1.1296e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6129e-04 - val_loss: 1.0056e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.5728e-04 - val_loss: 1.0710e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.5414e-04 - val_loss: 1.0769e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.5273e-04 - val_loss: 1.0901e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 1.5095e-04 - val_loss: 1.0981e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4880e-04 - val_loss: 1.0990e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.4646e-04 - val_loss: 1.0380e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4331e-04 - val_loss: 1.0467e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4178e-04 - val_loss: 1.0440e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4050e-04 - val_loss: 9.3847e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3906e-04 - val_loss: 8.7968e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3753e-04 - val_loss: 8.9504e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3617e-04 - val_loss: 8.9750e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3462e-04 - val_loss: 8.3303e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3341e-04 - val_loss: 7.9927e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3154e-04 - val_loss: 8.2630e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3079e-04 - val_loss: 9.6343e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3021e-04 - val_loss: 8.8012e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2991e-04 - val_loss: 8.9565e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2812e-04 - val_loss: 8.5539e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2735e-04 - val_loss: 8.3643e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2586e-04 - val_loss: 8.1278e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2582e-04 - val_loss: 8.3705e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.2562e-04 - val_loss: 8.3481e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2296e-04 - val_loss: 7.8910e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2250e-04 - val_loss: 8.5592e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2227e-04 - val_loss: 8.0726e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2110e-04 - val_loss: 8.5597e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1970e-04 - val_loss: 8.4819e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1836e-04 - val_loss: 8.1844e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1856e-04 - val_loss: 7.8173e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1736e-04 - val_loss: 8.2648e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1593e-04 - val_loss: 8.1110e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1409e-04 - val_loss: 8.2116e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1368e-04 - val_loss: 8.3845e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1266e-04 - val_loss: 8.2933e-05\n",
      "Validation RMSE:  0.00910676 \n",
      "\n",
      "Model took 1335.71 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 34s 871us/step - loss: 0.0053 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 6.5659e-04 - val_loss: 7.1132e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 5.2910e-04 - val_loss: 6.0606e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 4.5611e-04 - val_loss: 4.8310e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 473us/step - loss: 4.1570e-04 - val_loss: 4.2217e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 3.8199e-04 - val_loss: 3.7060e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.5773e-04 - val_loss: 3.5947e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.4293e-04 - val_loss: 3.1445e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.2813e-04 - val_loss: 3.3065e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.1700e-04 - val_loss: 3.2351e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.0746e-04 - val_loss: 3.1247e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.9901e-04 - val_loss: 3.0926e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.8678e-04 - val_loss: 2.9429e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.7569e-04 - val_loss: 3.0871e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.6266e-04 - val_loss: 2.9614e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.4433e-04 - val_loss: 2.8610e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.3329e-04 - val_loss: 2.4761e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.2421e-04 - val_loss: 2.2129e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.1462e-04 - val_loss: 2.1664e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 2.0753e-04 - val_loss: 1.8049e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.9790e-04 - val_loss: 1.8407e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.8787e-04 - val_loss: 1.7306e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.7958e-04 - val_loss: 1.6499e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.7395e-04 - val_loss: 1.6253e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.6975e-04 - val_loss: 1.6260e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.6701e-04 - val_loss: 1.5574e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.6473e-04 - val_loss: 1.4656e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.6250e-04 - val_loss: 1.4634e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.6028e-04 - val_loss: 1.3870e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5782e-04 - val_loss: 1.3551e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.5544e-04 - val_loss: 1.3118e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5308e-04 - val_loss: 1.2879e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.5089e-04 - val_loss: 1.2781e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4930e-04 - val_loss: 1.3203e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.4731e-04 - val_loss: 1.3274e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4519e-04 - val_loss: 1.2702e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.4403e-04 - val_loss: 1.2329e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4341e-04 - val_loss: 1.2261e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4214e-04 - val_loss: 1.2197e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4099e-04 - val_loss: 1.1663e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3967e-04 - val_loss: 1.1686e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.3829e-04 - val_loss: 1.1454e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3673e-04 - val_loss: 1.1358e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3521e-04 - val_loss: 1.1195e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3371e-04 - val_loss: 1.1132e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.3222e-04 - val_loss: 1.0706e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.3107e-04 - val_loss: 1.0437e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2993e-04 - val_loss: 1.0287e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2867e-04 - val_loss: 1.0268e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2742e-04 - val_loss: 1.0299e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2610e-04 - val_loss: 1.0168e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.2509e-04 - val_loss: 1.0140e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2388e-04 - val_loss: 1.0290e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2294e-04 - val_loss: 1.0142e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2179e-04 - val_loss: 9.7353e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2036e-04 - val_loss: 9.5182e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1943e-04 - val_loss: 9.4906e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1763e-04 - val_loss: 9.7172e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1628e-04 - val_loss: 9.3662e-05\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1553e-04 - val_loss: 9.1847e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1526e-04 - val_loss: 9.1614e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1381e-04 - val_loss: 8.8536e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.1315e-04 - val_loss: 8.6572e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1301e-04 - val_loss: 8.6266e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 474us/step - loss: 1.1181e-04 - val_loss: 8.2808e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1111e-04 - val_loss: 7.8932e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1075e-04 - val_loss: 8.1000e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.0883e-04 - val_loss: 8.3906e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.0905e-04 - val_loss: 8.2920e-05\n",
      "Validation RMSE:  0.009106063 \n",
      "\n",
      "Model took 1335.56 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 34s 879us/step - loss: 0.0060 - val_loss: 0.0048\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 7.4926e-04 - val_loss: 0.0012\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 5.2881e-04 - val_loss: 6.9410e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 4.6081e-04 - val_loss: 3.9216e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 4.1405e-04 - val_loss: 3.7170e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 3.7942e-04 - val_loss: 3.2418e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.5959e-04 - val_loss: 2.8267e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.4660e-04 - val_loss: 2.4935e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.3327e-04 - val_loss: 2.4144e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.2379e-04 - val_loss: 2.3883e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.1290e-04 - val_loss: 2.4780e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.0031e-04 - val_loss: 2.5210e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.9031e-04 - val_loss: 2.4760e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.7849e-04 - val_loss: 2.4508e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.6939e-04 - val_loss: 2.4167e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.6146e-04 - val_loss: 2.3761e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.5526e-04 - val_loss: 2.2146e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.4808e-04 - val_loss: 2.1788e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.3774e-04 - val_loss: 1.9996e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.2675e-04 - val_loss: 1.6258e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.1792e-04 - val_loss: 1.3526e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.1213e-04 - val_loss: 1.4172e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.0241e-04 - val_loss: 1.2552e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.9505e-04 - val_loss: 1.2390e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.8654e-04 - val_loss: 1.1548e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.8105e-04 - val_loss: 1.0492e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.7741e-04 - val_loss: 1.0458e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.7390e-04 - val_loss: 1.0354e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.7027e-04 - val_loss: 1.0566e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.6598e-04 - val_loss: 1.0448e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.6449e-04 - val_loss: 1.0332e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.6241e-04 - val_loss: 1.0514e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.5995e-04 - val_loss: 1.0547e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5748e-04 - val_loss: 1.0349e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5571e-04 - val_loss: 1.0308e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.5291e-04 - val_loss: 1.0429e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.5142e-04 - val_loss: 1.0746e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.5006e-04 - val_loss: 1.0564e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.4786e-04 - val_loss: 1.0451e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.4582e-04 - val_loss: 1.0559e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.4414e-04 - val_loss: 1.0556e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4212e-04 - val_loss: 1.0033e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4077e-04 - val_loss: 9.9139e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3870e-04 - val_loss: 9.6220e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3714e-04 - val_loss: 9.3252e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3528e-04 - val_loss: 9.4645e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3348e-04 - val_loss: 9.2783e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3159e-04 - val_loss: 8.9812e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2933e-04 - val_loss: 9.0975e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.2808e-04 - val_loss: 9.0779e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2700e-04 - val_loss: 9.1272e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2498e-04 - val_loss: 9.0866e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2322e-04 - val_loss: 9.0169e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2192e-04 - val_loss: 9.0417e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2011e-04 - val_loss: 9.0928e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1801e-04 - val_loss: 8.9351e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1716e-04 - val_loss: 9.0349e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1590e-04 - val_loss: 9.0342e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1543e-04 - val_loss: 9.0798e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1461e-04 - val_loss: 9.1773e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1456e-04 - val_loss: 9.3156e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1403e-04 - val_loss: 9.2079e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1285e-04 - val_loss: 9.3692e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1127e-04 - val_loss: 8.6869e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1247e-04 - val_loss: 8.9404e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1104e-04 - val_loss: 8.9981e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1114e-04 - val_loss: 8.7692e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1070e-04 - val_loss: 8.5460e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.0957e-04 - val_loss: 8.3282e-05\n",
      "Validation RMSE:  0.009125912 \n",
      "\n",
      "Model took 1357.45 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 35s 887us/step - loss: 0.0051 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 7.7878e-04 - val_loss: 5.7368e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 5.9708e-04 - val_loss: 4.8997e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 5.1227e-04 - val_loss: 3.2925e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 4.5026e-04 - val_loss: 4.0268e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 4.0695e-04 - val_loss: 4.0718e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.7394e-04 - val_loss: 3.6838e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.5235e-04 - val_loss: 3.2809e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.3609e-04 - val_loss: 2.9255e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.2357e-04 - val_loss: 2.7585e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.1385e-04 - val_loss: 2.6584e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.0408e-04 - val_loss: 2.6742e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.9482e-04 - val_loss: 2.6048e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.8800e-04 - val_loss: 2.3841e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.7846e-04 - val_loss: 2.4661e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.6802e-04 - val_loss: 2.6611e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.5948e-04 - val_loss: 2.7066e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.5039e-04 - val_loss: 2.6940e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.3826e-04 - val_loss: 2.5620e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.2787e-04 - val_loss: 2.1857e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.1874e-04 - val_loss: 1.8825e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.1061e-04 - val_loss: 1.7254e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.9995e-04 - val_loss: 2.0034e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.9100e-04 - val_loss: 1.4354e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.8254e-04 - val_loss: 1.2593e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.7301e-04 - val_loss: 1.1560e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.6977e-04 - val_loss: 1.2441e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.6494e-04 - val_loss: 1.2170e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.6048e-04 - val_loss: 1.1570e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5605e-04 - val_loss: 1.1432e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.5387e-04 - val_loss: 1.1765e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.5023e-04 - val_loss: 1.1183e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.4866e-04 - val_loss: 1.1759e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4386e-04 - val_loss: 1.1514e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4407e-04 - val_loss: 1.1752e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4279e-04 - val_loss: 1.1720e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4128e-04 - val_loss: 1.1292e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3967e-04 - val_loss: 1.1716e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3822e-04 - val_loss: 1.1093e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3683e-04 - val_loss: 1.1257e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3513e-04 - val_loss: 9.8307e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3360e-04 - val_loss: 9.8087e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3228e-04 - val_loss: 9.5555e-05\n",
      "Epoch 45/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2994e-04 - val_loss: 9.6899e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2885e-04 - val_loss: 9.1854e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.2817e-04 - val_loss: 9.5570e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2816e-04 - val_loss: 9.5842e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2677e-04 - val_loss: 9.1418e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2554e-04 - val_loss: 9.0340e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2413e-04 - val_loss: 9.2206e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2170e-04 - val_loss: 9.4458e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2237e-04 - val_loss: 9.2572e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2167e-04 - val_loss: 9.2137e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2094e-04 - val_loss: 9.3489e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2015e-04 - val_loss: 9.0320e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1921e-04 - val_loss: 9.2810e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1846e-04 - val_loss: 9.3908e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1725e-04 - val_loss: 9.6818e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1767e-04 - val_loss: 9.6952e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1613e-04 - val_loss: 9.2947e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1550e-04 - val_loss: 1.0040e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1496e-04 - val_loss: 1.0182e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1441e-04 - val_loss: 1.0015e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1260e-04 - val_loss: 1.0727e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1319e-04 - val_loss: 1.0116e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1219e-04 - val_loss: 1.0887e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1117e-04 - val_loss: 9.7296e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1013e-04 - val_loss: 1.0230e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1055e-04 - val_loss: 9.8832e-05\n",
      "Validation RMSE:  0.009941443 \n",
      "\n",
      "Model took 1353.14 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 35s 885us/step - loss: 0.0061 - val_loss: 0.0034\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 6.1772e-04 - val_loss: 7.3918e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 5.0074e-04 - val_loss: 4.3656e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 4.3511e-04 - val_loss: 3.9095e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.9944e-04 - val_loss: 3.9537e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.7856e-04 - val_loss: 3.3787e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.6073e-04 - val_loss: 3.1866e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.4387e-04 - val_loss: 3.1239e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.3137e-04 - val_loss: 2.9864e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 3.2064e-04 - val_loss: 2.8709e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 3.1128e-04 - val_loss: 2.7965e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.0179e-04 - val_loss: 2.7294e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.9377e-04 - val_loss: 2.9185e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.8382e-04 - val_loss: 2.7003e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.7301e-04 - val_loss: 2.6326e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.6471e-04 - val_loss: 2.3926e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 2.5395e-04 - val_loss: 2.4668e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.4775e-04 - val_loss: 2.4787e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.4036e-04 - val_loss: 2.5958e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.3084e-04 - val_loss: 2.4912e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.1817e-04 - val_loss: 2.4804e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.0964e-04 - val_loss: 1.9753e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.0187e-04 - val_loss: 1.9505e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.9414e-04 - val_loss: 1.5127e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.8710e-04 - val_loss: 1.4549e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.8113e-04 - val_loss: 1.4585e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.7588e-04 - val_loss: 1.4493e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.7131e-04 - val_loss: 1.4439e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.6812e-04 - val_loss: 1.4805e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.6380e-04 - val_loss: 1.5283e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.6102e-04 - val_loss: 1.5424e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.5792e-04 - val_loss: 1.4500e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 475us/step - loss: 1.5558e-04 - val_loss: 1.4656e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5266e-04 - val_loss: 1.4608e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.5165e-04 - val_loss: 1.5084e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4940e-04 - val_loss: 1.4523e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4716e-04 - val_loss: 1.3829e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.4645e-04 - val_loss: 1.4735e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4564e-04 - val_loss: 1.3467e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.4374e-04 - val_loss: 1.3007e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.4193e-04 - val_loss: 1.3674e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4066e-04 - val_loss: 1.4167e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3921e-04 - val_loss: 1.3128e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3746e-04 - val_loss: 1.2938e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3662e-04 - val_loss: 1.2242e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3554e-04 - val_loss: 1.2276e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3397e-04 - val_loss: 1.1752e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3248e-04 - val_loss: 1.1680e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3180e-04 - val_loss: 1.0865e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.3092e-04 - val_loss: 1.0389e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2928e-04 - val_loss: 9.8511e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2867e-04 - val_loss: 1.0017e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2711e-04 - val_loss: 8.9509e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2478e-04 - val_loss: 8.6637e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2501e-04 - val_loss: 9.4448e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2324e-04 - val_loss: 8.7995e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2409e-04 - val_loss: 9.2692e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2292e-04 - val_loss: 9.6530e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2113e-04 - val_loss: 9.1652e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.2019e-04 - val_loss: 8.9198e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 476us/step - loss: 1.1889e-04 - val_loss: 8.6261e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1890e-04 - val_loss: 9.1817e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 1.1726e-04 - val_loss: 9.0745e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.1520e-04 - val_loss: 9.7638e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1352e-04 - val_loss: 9.6009e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1225e-04 - val_loss: 9.3297e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1166e-04 - val_loss: 9.4629e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1094e-04 - val_loss: 9.4798e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.0998e-04 - val_loss: 9.3469e-05\n",
      "Validation RMSE:  0.009667945 \n",
      "\n",
      "Model took 1340.48 seconds to train\n",
      "19 \t8     \t0.00912587\t0.000419029\t0.00856956\t0.00994144\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 36\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 22 20 27 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1068\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 1, 1, 1, 1, 1, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1, 1, 1, 1, 1, 1, 1, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030]\n",
      "Lags length:\n",
      " 81\n",
      "Length Used:\n",
      " 27\n",
      "New Dataset shape after Lags: (48970, 82)\n",
      "Training and test data length 39176 9794\n",
      "trainX.shape[0]:  39176\n",
      "window_length:  27\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39176, 81)\n",
      "Train X shape after np.reshape (39176, 27, 3)\n",
      "Test X shape after np.reshape (9794, 27, 3)\n",
      "Train Y Shape (39176,)\n",
      "Test Y Shape (9794,)\n",
      "Train on 39176 samples, validate on 9794 samples\n",
      "Epoch 1/70\n",
      "39176/39176 [==============================] - 63s 2ms/step - loss: 0.0037 - val_loss: 0.0062\n",
      "Epoch 2/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 0.0016 - val_loss: 0.0052\n",
      "Epoch 3/70\n",
      "39176/39176 [==============================] - 47s 1ms/step - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 4/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 7.9909e-04 - val_loss: 0.0015\n",
      "Epoch 5/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 6.6982e-04 - val_loss: 0.0014\n",
      "Epoch 6/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 6.0880e-04 - val_loss: 0.0013\n",
      "Epoch 7/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 5.6389e-04 - val_loss: 0.0012\n",
      "Epoch 8/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 5.2918e-04 - val_loss: 0.0011\n",
      "Epoch 9/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 4.9820e-04 - val_loss: 0.0011\n",
      "Epoch 10/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 4.8300e-04 - val_loss: 9.8698e-04\n",
      "Epoch 11/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 4.5696e-04 - val_loss: 0.0010\n",
      "Epoch 12/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 4.4525e-04 - val_loss: 9.0655e-04\n",
      "Epoch 13/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 4.3422e-04 - val_loss: 8.2855e-04\n",
      "Epoch 14/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 4.2171e-04 - val_loss: 7.3007e-04\n",
      "Epoch 15/70\n",
      "39176/39176 [==============================] - 47s 1ms/step - loss: 4.1062e-04 - val_loss: 7.5591e-04\n",
      "Epoch 16/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.9970e-04 - val_loss: 7.7661e-04\n",
      "Epoch 17/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.9060e-04 - val_loss: 7.4705e-04\n",
      "Epoch 18/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.8095e-04 - val_loss: 8.7831e-04\n",
      "Epoch 19/70\n",
      "39176/39176 [==============================] - 47s 1ms/step - loss: 3.7379e-04 - val_loss: 9.4529e-04\n",
      "Epoch 20/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.6430e-04 - val_loss: 8.8812e-04\n",
      "Epoch 21/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.5310e-04 - val_loss: 7.4051e-04\n",
      "Epoch 22/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.4900e-04 - val_loss: 7.1093e-04\n",
      "Epoch 23/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.4203e-04 - val_loss: 7.0929e-04\n",
      "Epoch 24/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.3964e-04 - val_loss: 8.1929e-04\n",
      "Epoch 25/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.3211e-04 - val_loss: 7.4353e-04\n",
      "Epoch 26/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.2631e-04 - val_loss: 7.5240e-04\n",
      "Epoch 27/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.2387e-04 - val_loss: 8.6789e-04\n",
      "Epoch 28/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.1583e-04 - val_loss: 8.3021e-04\n",
      "Epoch 29/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 3.0973e-04 - val_loss: 8.7203e-04\n",
      "Epoch 30/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.9834e-04 - val_loss: 8.8661e-04\n",
      "Epoch 31/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.9524e-04 - val_loss: 8.3775e-04\n",
      "Epoch 32/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.8894e-04 - val_loss: 9.0490e-04\n",
      "Epoch 33/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.8956e-04 - val_loss: 8.5533e-04\n",
      "Epoch 34/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.8187e-04 - val_loss: 8.3747e-04\n",
      "Epoch 35/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.7720e-04 - val_loss: 7.8563e-04\n",
      "Epoch 36/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.7229e-04 - val_loss: 7.9252e-04\n",
      "Epoch 37/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.6646e-04 - val_loss: 7.2340e-04\n",
      "Epoch 38/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.6173e-04 - val_loss: 6.5888e-04\n",
      "Epoch 39/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.5644e-04 - val_loss: 6.9064e-04\n",
      "Epoch 40/70\n",
      "39176/39176 [==============================] - 47s 1ms/step - loss: 2.5184e-04 - val_loss: 7.3820e-04\n",
      "Epoch 41/70\n",
      "39176/39176 [==============================] - 47s 1ms/step - loss: 2.4716e-04 - val_loss: 6.4230e-04\n",
      "Epoch 42/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.4360e-04 - val_loss: 6.3770e-04\n",
      "Epoch 43/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.3972e-04 - val_loss: 6.1042e-04\n",
      "Epoch 44/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.3640e-04 - val_loss: 6.6911e-04\n",
      "Epoch 45/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.3281e-04 - val_loss: 5.8980e-04\n",
      "Epoch 46/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.2984e-04 - val_loss: 5.9015e-04\n",
      "Epoch 47/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.2659e-04 - val_loss: 6.2562e-04\n",
      "Epoch 48/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.2416e-04 - val_loss: 5.7880e-04\n",
      "Epoch 49/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.1987e-04 - val_loss: 5.1886e-04\n",
      "Epoch 50/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.1800e-04 - val_loss: 5.1414e-04\n",
      "Epoch 51/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.1530e-04 - val_loss: 4.6468e-04\n",
      "Epoch 52/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.1261e-04 - val_loss: 4.8448e-04\n",
      "Epoch 53/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.1014e-04 - val_loss: 5.6577e-04\n",
      "Epoch 54/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.0754e-04 - val_loss: 5.6150e-04\n",
      "Epoch 55/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.0666e-04 - val_loss: 5.3351e-04\n",
      "Epoch 56/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.0537e-04 - val_loss: 5.5926e-04\n",
      "Epoch 57/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.0342e-04 - val_loss: 5.8857e-04\n",
      "Epoch 58/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 2.0093e-04 - val_loss: 5.6507e-04\n",
      "Epoch 59/70\n",
      "39176/39176 [==============================] - 47s 1ms/step - loss: 1.9939e-04 - val_loss: 5.4447e-04\n",
      "Epoch 60/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 1.9743e-04 - val_loss: 6.1295e-04\n",
      "Epoch 61/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 1.9612e-04 - val_loss: 5.8021e-04\n",
      "Epoch 62/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 1.9561e-04 - val_loss: 5.1879e-04\n",
      "Epoch 63/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 1.9422e-04 - val_loss: 5.6529e-04\n",
      "Epoch 64/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 1.9370e-04 - val_loss: 5.7018e-04\n",
      "Epoch 65/70\n",
      "39176/39176 [==============================] - 47s 1ms/step - loss: 1.9148e-04 - val_loss: 5.9193e-04\n",
      "Epoch 66/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 1.9045e-04 - val_loss: 5.8709e-04\n",
      "Epoch 67/70\n",
      "39176/39176 [==============================] - 47s 1ms/step - loss: 1.8751e-04 - val_loss: 5.9112e-04\n",
      "Epoch 68/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 1.8590e-04 - val_loss: 5.4508e-04\n",
      "Epoch 69/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 1.8482e-04 - val_loss: 5.0590e-04\n",
      "Epoch 70/70\n",
      "39176/39176 [==============================] - 46s 1ms/step - loss: 1.8435e-04 - val_loss: 5.7252e-04\n",
      "Validation RMSE:  0.023927448 \n",
      "\n",
      "Model took 3271.95 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 35s 902us/step - loss: 0.0066 - val_loss: 0.0046\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 7.3085e-04 - val_loss: 6.3463e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 5.3975e-04 - val_loss: 4.1571e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 4.6623e-04 - val_loss: 3.2884e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 4.1556e-04 - val_loss: 3.1512e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.7888e-04 - val_loss: 3.2242e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.5684e-04 - val_loss: 3.3085e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.4100e-04 - val_loss: 2.9801e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 3.2577e-04 - val_loss: 2.6834e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.1296e-04 - val_loss: 2.8171e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.0392e-04 - val_loss: 2.7796e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 477us/step - loss: 2.9351e-04 - val_loss: 2.6240e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.8323e-04 - val_loss: 2.6129e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.7184e-04 - val_loss: 2.6009e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.5957e-04 - val_loss: 2.5441e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.5178e-04 - val_loss: 2.5985e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.4628e-04 - val_loss: 2.4740e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.4003e-04 - val_loss: 2.2766e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.3023e-04 - val_loss: 2.2408e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.1828e-04 - val_loss: 2.3461e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.1201e-04 - val_loss: 2.4439e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.0820e-04 - val_loss: 1.8056e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 2.0136e-04 - val_loss: 1.8494e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.9262e-04 - val_loss: 1.6210e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.8467e-04 - val_loss: 1.5196e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.7859e-04 - val_loss: 1.4352e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.7222e-04 - val_loss: 1.2687e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.6686e-04 - val_loss: 1.1787e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.6211e-04 - val_loss: 1.1555e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.5762e-04 - val_loss: 1.2211e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.5446e-04 - val_loss: 1.2614e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.5178e-04 - val_loss: 1.2763e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.4948e-04 - val_loss: 1.2599e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.4682e-04 - val_loss: 1.2501e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.4488e-04 - val_loss: 1.2394e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.4354e-04 - val_loss: 1.1960e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4156e-04 - val_loss: 1.1896e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3978e-04 - val_loss: 1.1978e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3877e-04 - val_loss: 1.1628e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.3740e-04 - val_loss: 1.1352e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3613e-04 - val_loss: 1.0877e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3496e-04 - val_loss: 1.1247e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3338e-04 - val_loss: 1.0966e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3189e-04 - val_loss: 1.0664e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.3114e-04 - val_loss: 9.9828e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3007e-04 - val_loss: 9.6938e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2908e-04 - val_loss: 9.7885e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2807e-04 - val_loss: 9.4296e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.2742e-04 - val_loss: 9.1763e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2598e-04 - val_loss: 9.5432e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2507e-04 - val_loss: 9.7631e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2399e-04 - val_loss: 1.0192e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2312e-04 - val_loss: 1.0050e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2213e-04 - val_loss: 1.0208e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2089e-04 - val_loss: 1.0742e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1923e-04 - val_loss: 1.0883e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1787e-04 - val_loss: 1.0996e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1674e-04 - val_loss: 1.1277e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1641e-04 - val_loss: 1.1408e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1489e-04 - val_loss: 1.2153e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1523e-04 - val_loss: 1.1522e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1476e-04 - val_loss: 1.2332e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1341e-04 - val_loss: 1.1393e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1288e-04 - val_loss: 1.1960e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1234e-04 - val_loss: 1.0787e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.1191e-04 - val_loss: 1.1695e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.1273e-04 - val_loss: 1.0102e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1171e-04 - val_loss: 1.1643e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.1129e-04 - val_loss: 1.1237e-04\n",
      "Validation RMSE:  0.010600249 \n",
      "\n",
      "Model took 1346.33 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 43\n",
      "2nd Window Start: 508\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 35s 903us/step - loss: 0.0056 - val_loss: 0.0041\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 7.5928e-04 - val_loss: 7.0112e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 6.1434e-04 - val_loss: 6.2217e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 5.5348e-04 - val_loss: 5.8351e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 5.1352e-04 - val_loss: 5.2652e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 4.7369e-04 - val_loss: 4.9299e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 4.3372e-04 - val_loss: 3.8727e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.9692e-04 - val_loss: 3.8177e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.7885e-04 - val_loss: 3.5898e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.6039e-04 - val_loss: 3.3955e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.4748e-04 - val_loss: 3.3813e-04\n",
      "Epoch 13/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.3150e-04 - val_loss: 3.1314e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.1260e-04 - val_loss: 2.6136e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.0240e-04 - val_loss: 2.4869e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.8038e-04 - val_loss: 2.8566e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.6523e-04 - val_loss: 2.6406e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.5113e-04 - val_loss: 2.7485e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.4263e-04 - val_loss: 2.3335e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.3624e-04 - val_loss: 2.1187e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.2991e-04 - val_loss: 1.9900e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.2118e-04 - val_loss: 1.7732e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.1770e-04 - val_loss: 1.7419e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 2.1466e-04 - val_loss: 1.6487e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.1266e-04 - val_loss: 1.6076e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.0802e-04 - val_loss: 1.6526e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.0512e-04 - val_loss: 1.5999e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.0197e-04 - val_loss: 1.6613e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.9809e-04 - val_loss: 1.5367e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.9598e-04 - val_loss: 1.4970e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.9561e-04 - val_loss: 1.4397e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.9514e-04 - val_loss: 1.5066e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.9201e-04 - val_loss: 1.4713e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.8780e-04 - val_loss: 1.5653e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.8319e-04 - val_loss: 1.4365e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.7862e-04 - val_loss: 1.6394e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.7386e-04 - val_loss: 1.3980e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.6915e-04 - val_loss: 1.4313e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.6590e-04 - val_loss: 1.4634e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.6127e-04 - val_loss: 1.4005e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5682e-04 - val_loss: 1.4963e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.5205e-04 - val_loss: 1.5708e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.5038e-04 - val_loss: 1.3766e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4674e-04 - val_loss: 1.6306e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.4560e-04 - val_loss: 1.5178e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4485e-04 - val_loss: 1.4244e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.4337e-04 - val_loss: 1.3936e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.4061e-04 - val_loss: 1.5091e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3995e-04 - val_loss: 1.4566e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3877e-04 - val_loss: 1.3960e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3848e-04 - val_loss: 1.3638e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3696e-04 - val_loss: 1.4035e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3628e-04 - val_loss: 1.5331e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3516e-04 - val_loss: 1.7855e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3387e-04 - val_loss: 1.9336e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3330e-04 - val_loss: 1.3205e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3129e-04 - val_loss: 1.2653e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3030e-04 - val_loss: 1.2442e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3070e-04 - val_loss: 1.2557e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.2958e-04 - val_loss: 1.2653e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2931e-04 - val_loss: 1.2520e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2864e-04 - val_loss: 1.3132e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2761e-04 - val_loss: 1.3212e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2737e-04 - val_loss: 1.2559e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2653e-04 - val_loss: 1.2342e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2530e-04 - val_loss: 1.2457e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2324e-04 - val_loss: 1.3225e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2229e-04 - val_loss: 1.3590e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2101e-04 - val_loss: 1.7234e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2093e-04 - val_loss: 1.3541e-04\n",
      "Validation RMSE:  0.011636717 \n",
      "\n",
      "Model took 1349.88 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 36s 924us/step - loss: 0.0068 - val_loss: 0.0053\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 8.6788e-04 - val_loss: 7.5044e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 5.6706e-04 - val_loss: 3.6300e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 4.8350e-04 - val_loss: 3.1600e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 4.3533e-04 - val_loss: 3.1090e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.9858e-04 - val_loss: 2.7336e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.7092e-04 - val_loss: 2.6186e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.5315e-04 - val_loss: 2.3480e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.3732e-04 - val_loss: 2.3003e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.2420e-04 - val_loss: 2.0554e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.1320e-04 - val_loss: 2.1833e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.0199e-04 - val_loss: 2.3453e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.9010e-04 - val_loss: 2.3787e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.7361e-04 - val_loss: 2.3872e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.6350e-04 - val_loss: 2.3215e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.5227e-04 - val_loss: 2.3831e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.3862e-04 - val_loss: 2.4335e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.3834e-04 - val_loss: 2.2620e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.2580e-04 - val_loss: 2.1055e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.1421e-04 - val_loss: 1.6966e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.0249e-04 - val_loss: 1.1970e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.9541e-04 - val_loss: 1.0729e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.8584e-04 - val_loss: 1.0848e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.7811e-04 - val_loss: 9.9139e-05\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.7127e-04 - val_loss: 9.9735e-05\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.6414e-04 - val_loss: 9.9178e-05\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5896e-04 - val_loss: 9.5791e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5503e-04 - val_loss: 8.9703e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5188e-04 - val_loss: 8.7258e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4924e-04 - val_loss: 8.8197e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4683e-04 - val_loss: 8.8704e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.4495e-04 - val_loss: 8.6761e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4370e-04 - val_loss: 8.6784e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.4160e-04 - val_loss: 8.7178e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3969e-04 - val_loss: 8.8153e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3754e-04 - val_loss: 8.9187e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3566e-04 - val_loss: 9.0534e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3392e-04 - val_loss: 9.0583e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3197e-04 - val_loss: 8.7274e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3083e-04 - val_loss: 8.9047e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2934e-04 - val_loss: 8.8934e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2801e-04 - val_loss: 8.7971e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2700e-04 - val_loss: 9.1476e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2533e-04 - val_loss: 8.9409e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2433e-04 - val_loss: 8.9958e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2280e-04 - val_loss: 9.3315e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2213e-04 - val_loss: 9.5306e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2040e-04 - val_loss: 9.3955e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.2021e-04 - val_loss: 8.8238e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1796e-04 - val_loss: 8.7135e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1721e-04 - val_loss: 9.0725e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.1688e-04 - val_loss: 8.8623e-05\n",
      "Validation RMSE:  0.009413964 \n",
      "\n",
      "Model took 1034.87 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 37s 945us/step - loss: 0.0057 - val_loss: 0.0031\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 6.7912e-04 - val_loss: 6.0626e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 5.2427e-04 - val_loss: 5.1052e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 4.5703e-04 - val_loss: 4.2233e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 4.0946e-04 - val_loss: 3.8990e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.7387e-04 - val_loss: 3.1514e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.5421e-04 - val_loss: 2.7870e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.3841e-04 - val_loss: 2.5554e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.2562e-04 - val_loss: 2.5501e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.1290e-04 - val_loss: 2.4057e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.0021e-04 - val_loss: 2.3734e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.8919e-04 - val_loss: 2.3870e-04\n",
      "Epoch 14/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.7846e-04 - val_loss: 2.0322e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.6780e-04 - val_loss: 2.1649e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.6029e-04 - val_loss: 2.1877e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.5393e-04 - val_loss: 2.0449e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.4639e-04 - val_loss: 1.8623e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.3961e-04 - val_loss: 1.8580e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.2467e-04 - val_loss: 1.7131e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.1912e-04 - val_loss: 1.6287e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.1350e-04 - val_loss: 1.5259e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.0254e-04 - val_loss: 1.4149e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.9338e-04 - val_loss: 1.2855e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.8296e-04 - val_loss: 1.1279e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.7838e-04 - val_loss: 1.0804e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.7354e-04 - val_loss: 1.0442e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.6756e-04 - val_loss: 1.0026e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.6309e-04 - val_loss: 9.8057e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.5953e-04 - val_loss: 9.8228e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.5624e-04 - val_loss: 9.6709e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.5335e-04 - val_loss: 9.6400e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4986e-04 - val_loss: 9.6576e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4715e-04 - val_loss: 1.0183e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.4532e-04 - val_loss: 1.0473e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.4328e-04 - val_loss: 1.0938e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.4148e-04 - val_loss: 1.0419e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3974e-04 - val_loss: 1.0553e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3821e-04 - val_loss: 1.1108e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3621e-04 - val_loss: 1.0553e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.3440e-04 - val_loss: 1.0379e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3258e-04 - val_loss: 9.9314e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.3131e-04 - val_loss: 9.4233e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3011e-04 - val_loss: 9.2759e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2849e-04 - val_loss: 9.7119e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2785e-04 - val_loss: 9.3358e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2662e-04 - val_loss: 9.3091e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.2559e-04 - val_loss: 1.0133e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2379e-04 - val_loss: 1.0261e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2288e-04 - val_loss: 1.1249e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2194e-04 - val_loss: 1.1172e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2064e-04 - val_loss: 1.1931e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1993e-04 - val_loss: 1.1545e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1798e-04 - val_loss: 1.2404e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1755e-04 - val_loss: 1.2482e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1653e-04 - val_loss: 1.1358e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1572e-04 - val_loss: 1.0784e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1579e-04 - val_loss: 1.1712e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1370e-04 - val_loss: 1.0551e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1481e-04 - val_loss: 8.9238e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.1278e-04 - val_loss: 8.7818e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1270e-04 - val_loss: 9.1330e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.1277e-04 - val_loss: 8.8074e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1161e-04 - val_loss: 8.9202e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1107e-04 - val_loss: 8.7916e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1032e-04 - val_loss: 1.0447e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.0969e-04 - val_loss: 1.0181e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1007e-04 - val_loss: 9.0843e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.0898e-04 - val_loss: 8.9191e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.0882e-04 - val_loss: 8.7998e-05\n",
      "Validation RMSE:  0.009380715 \n",
      "\n",
      "Model took 1365.00 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 36s 930us/step - loss: 0.0060 - val_loss: 0.0046\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 7.5305e-04 - val_loss: 5.8530e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 5.5525e-04 - val_loss: 4.3176e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 4.8172e-04 - val_loss: 4.7103e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 4.3032e-04 - val_loss: 4.7753e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.9510e-04 - val_loss: 4.1042e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 3.6826e-04 - val_loss: 3.6471e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.4754e-04 - val_loss: 3.3061e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.3153e-04 - val_loss: 3.2485e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.1848e-04 - val_loss: 3.1356e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.1089e-04 - val_loss: 3.0980e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.0196e-04 - val_loss: 3.0894e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.9471e-04 - val_loss: 3.0582e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.8627e-04 - val_loss: 2.9843e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.7569e-04 - val_loss: 2.9098e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.6758e-04 - val_loss: 2.5102e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.6006e-04 - val_loss: 2.5632e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.5460e-04 - val_loss: 2.4716e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.4696e-04 - val_loss: 2.5028e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.3415e-04 - val_loss: 2.2805e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.2196e-04 - val_loss: 1.6098e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.1171e-04 - val_loss: 1.6522e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.0090e-04 - val_loss: 1.3463e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.9237e-04 - val_loss: 1.1059e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.8577e-04 - val_loss: 1.1202e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.7930e-04 - val_loss: 1.1242e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.7446e-04 - val_loss: 1.0926e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.7096e-04 - val_loss: 1.0187e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.6724e-04 - val_loss: 1.0195e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.6480e-04 - val_loss: 1.0319e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.6163e-04 - val_loss: 1.0182e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.5907e-04 - val_loss: 1.0074e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.5538e-04 - val_loss: 9.2271e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.5382e-04 - val_loss: 9.3418e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.5021e-04 - val_loss: 9.4528e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.4867e-04 - val_loss: 9.4069e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4776e-04 - val_loss: 9.3457e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4590e-04 - val_loss: 9.3752e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.4416e-04 - val_loss: 9.3692e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.4208e-04 - val_loss: 9.4407e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4001e-04 - val_loss: 9.4551e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3931e-04 - val_loss: 9.3997e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3805e-04 - val_loss: 9.4669e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3733e-04 - val_loss: 9.5503e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3674e-04 - val_loss: 9.4840e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3458e-04 - val_loss: 9.4312e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.3353e-04 - val_loss: 9.4488e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3276e-04 - val_loss: 9.7523e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3228e-04 - val_loss: 1.0067e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3140e-04 - val_loss: 1.0118e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 478us/step - loss: 1.2969e-04 - val_loss: 1.0549e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.3010e-04 - val_loss: 1.0421e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2796e-04 - val_loss: 9.9063e-05\n",
      "Validation RMSE:  0.009953026 \n",
      "\n",
      "Model took 1050.23 seconds to train\n",
      "20 \t6     \t0.0109354 \t0.00443547 \t0.00856956\t0.0239274 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 36s 924us/step - loss: 0.0053 - val_loss: 0.0030\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 6.2029e-04 - val_loss: 5.6073e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 4.9574e-04 - val_loss: 5.9803e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 4.3943e-04 - val_loss: 4.3022e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 4.0259e-04 - val_loss: 3.5947e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.7654e-04 - val_loss: 2.9782e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.5690e-04 - val_loss: 2.6616e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.3929e-04 - val_loss: 2.5026e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.2698e-04 - val_loss: 2.5107e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.1664e-04 - val_loss: 2.4430e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.0802e-04 - val_loss: 2.5142e-04\n",
      "Epoch 13/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.0067e-04 - val_loss: 2.3883e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.9257e-04 - val_loss: 2.4333e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.8646e-04 - val_loss: 2.3819e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.7919e-04 - val_loss: 2.1544e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.7071e-04 - val_loss: 2.3292e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.6516e-04 - val_loss: 2.3347e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.5693e-04 - val_loss: 2.5226e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.4849e-04 - val_loss: 2.5217e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.4255e-04 - val_loss: 2.5815e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 2.3340e-04 - val_loss: 2.5280e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.2308e-04 - val_loss: 2.5032e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.1283e-04 - val_loss: 2.4639e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.0994e-04 - val_loss: 1.5359e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.0046e-04 - val_loss: 2.0773e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.9559e-04 - val_loss: 1.5167e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.8679e-04 - val_loss: 1.3072e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.8110e-04 - val_loss: 1.1872e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.7549e-04 - val_loss: 1.3224e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.7017e-04 - val_loss: 1.1370e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.6591e-04 - val_loss: 1.2058e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.6227e-04 - val_loss: 1.0571e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.5818e-04 - val_loss: 1.1524e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.5673e-04 - val_loss: 1.0945e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5371e-04 - val_loss: 1.1779e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.5065e-04 - val_loss: 1.1162e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 480us/step - loss: 1.4895e-04 - val_loss: 1.2897e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 479us/step - loss: 1.4697e-04 - val_loss: 1.1098e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4429e-04 - val_loss: 1.2286e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.4236e-04 - val_loss: 1.3350e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3991e-04 - val_loss: 1.3570e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3842e-04 - val_loss: 1.4386e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3632e-04 - val_loss: 1.3884e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3459e-04 - val_loss: 1.4150e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3224e-04 - val_loss: 1.4102e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3103e-04 - val_loss: 1.3275e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3052e-04 - val_loss: 1.3602e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2802e-04 - val_loss: 1.3255e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2781e-04 - val_loss: 1.3450e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.2655e-04 - val_loss: 1.3589e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2482e-04 - val_loss: 1.4376e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2329e-04 - val_loss: 1.3854e-04\n",
      "Validation RMSE:  0.011770332 \n",
      "\n",
      "Model took 1030.16 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 37s 944us/step - loss: 0.0047 - val_loss: 0.0033\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 6.6854e-04 - val_loss: 5.2508e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 5.5896e-04 - val_loss: 3.9987e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 4.8827e-04 - val_loss: 4.1526e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 4.3668e-04 - val_loss: 3.7649e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.9705e-04 - val_loss: 3.8737e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.7244e-04 - val_loss: 3.7128e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.5800e-04 - val_loss: 3.7768e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.4336e-04 - val_loss: 3.3906e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.3182e-04 - val_loss: 3.1530e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 3.2017e-04 - val_loss: 3.0279e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.0956e-04 - val_loss: 2.8873e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.0079e-04 - val_loss: 2.6683e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.9107e-04 - val_loss: 2.9132e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.8300e-04 - val_loss: 2.9420e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.7361e-04 - val_loss: 2.9379e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.6404e-04 - val_loss: 2.9650e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.5654e-04 - val_loss: 2.8817e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.4475e-04 - val_loss: 2.6266e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.2904e-04 - val_loss: 2.4972e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.2160e-04 - val_loss: 2.4626e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.1432e-04 - val_loss: 2.4261e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.0721e-04 - val_loss: 2.4026e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.0346e-04 - val_loss: 2.1457e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.8955e-04 - val_loss: 1.9029e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.8125e-04 - val_loss: 1.5970e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.7489e-04 - val_loss: 1.5435e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.6899e-04 - val_loss: 1.4632e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.6433e-04 - val_loss: 1.4220e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.6038e-04 - val_loss: 1.3806e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.5661e-04 - val_loss: 1.3414e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5423e-04 - val_loss: 1.2332e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.5196e-04 - val_loss: 1.1557e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.4921e-04 - val_loss: 1.2005e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4728e-04 - val_loss: 1.1823e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.4448e-04 - val_loss: 1.1655e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.4175e-04 - val_loss: 1.0949e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.4052e-04 - val_loss: 1.0803e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3940e-04 - val_loss: 1.0574e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.3798e-04 - val_loss: 1.0418e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3608e-04 - val_loss: 9.9433e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3488e-04 - val_loss: 9.6780e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.3301e-04 - val_loss: 9.4167e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3120e-04 - val_loss: 1.0035e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.3145e-04 - val_loss: 9.6467e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3003e-04 - val_loss: 9.4420e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2956e-04 - val_loss: 9.5768e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2798e-04 - val_loss: 9.2219e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2686e-04 - val_loss: 9.6904e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2651e-04 - val_loss: 9.4388e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.2484e-04 - val_loss: 8.5348e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2310e-04 - val_loss: 9.0091e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.2291e-04 - val_loss: 9.3204e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2339e-04 - val_loss: 9.1495e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2245e-04 - val_loss: 9.3708e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2171e-04 - val_loss: 8.9840e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2098e-04 - val_loss: 9.5403e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1890e-04 - val_loss: 9.6654e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1908e-04 - val_loss: 9.8585e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1992e-04 - val_loss: 9.6230e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1747e-04 - val_loss: 1.4983e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1873e-04 - val_loss: 1.2162e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.1740e-04 - val_loss: 1.3005e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1775e-04 - val_loss: 1.0632e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1696e-04 - val_loss: 9.9762e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1622e-04 - val_loss: 9.7811e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1742e-04 - val_loss: 8.9220e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1541e-04 - val_loss: 8.8200e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1593e-04 - val_loss: 1.0379e-04\n",
      "Validation RMSE:  0.010187639 \n",
      "\n",
      "Model took 1362.49 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 37s 944us/step - loss: 0.0063 - val_loss: 0.0045\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 6.9025e-04 - val_loss: 7.9174e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 5.0349e-04 - val_loss: 4.4116e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 4.2846e-04 - val_loss: 4.4463e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.8869e-04 - val_loss: 3.3190e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.5836e-04 - val_loss: 3.0629e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.3900e-04 - val_loss: 2.7942e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.2523e-04 - val_loss: 2.5887e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.1308e-04 - val_loss: 2.3714e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 3.0139e-04 - val_loss: 2.2085e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.9287e-04 - val_loss: 2.1695e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.8374e-04 - val_loss: 2.1161e-04\n",
      "Epoch 14/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.7400e-04 - val_loss: 2.1299e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.6383e-04 - val_loss: 2.2689e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.5576e-04 - val_loss: 2.5307e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.4749e-04 - val_loss: 2.4542e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.4130e-04 - val_loss: 2.5415e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.3174e-04 - val_loss: 2.4773e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.2383e-04 - val_loss: 1.7284e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.0776e-04 - val_loss: 1.6353e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.9983e-04 - val_loss: 1.5025e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.8961e-04 - val_loss: 1.4867e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.8239e-04 - val_loss: 1.3844e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.7642e-04 - val_loss: 1.2937e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.7042e-04 - val_loss: 1.1433e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.6691e-04 - val_loss: 1.1418e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.6301e-04 - val_loss: 1.1289e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.5940e-04 - val_loss: 1.0965e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5668e-04 - val_loss: 1.1113e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5439e-04 - val_loss: 1.1002e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5173e-04 - val_loss: 1.0860e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.4954e-04 - val_loss: 1.0289e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4779e-04 - val_loss: 1.0264e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 481us/step - loss: 1.4617e-04 - val_loss: 1.0059e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4432e-04 - val_loss: 9.6691e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4271e-04 - val_loss: 9.5580e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4087e-04 - val_loss: 9.5707e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3971e-04 - val_loss: 9.3411e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.3833e-04 - val_loss: 9.3172e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3678e-04 - val_loss: 9.1028e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3542e-04 - val_loss: 9.0081e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3398e-04 - val_loss: 8.9496e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3255e-04 - val_loss: 9.0250e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3170e-04 - val_loss: 9.0544e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3046e-04 - val_loss: 8.7535e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2917e-04 - val_loss: 8.6818e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2827e-04 - val_loss: 8.7035e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2741e-04 - val_loss: 8.3979e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2624e-04 - val_loss: 8.4498e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.2538e-04 - val_loss: 8.3163e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2420e-04 - val_loss: 8.3250e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2341e-04 - val_loss: 8.4481e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2260e-04 - val_loss: 8.1804e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2167e-04 - val_loss: 8.0158e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2081e-04 - val_loss: 7.9782e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1957e-04 - val_loss: 7.8397e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1894e-04 - val_loss: 7.7937e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1834e-04 - val_loss: 7.7959e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1783e-04 - val_loss: 7.9465e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1650e-04 - val_loss: 7.8258e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1572e-04 - val_loss: 7.8984e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1544e-04 - val_loss: 7.9411e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1420e-04 - val_loss: 8.1043e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1378e-04 - val_loss: 7.9440e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1269e-04 - val_loss: 7.8830e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.1175e-04 - val_loss: 7.8620e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.1154e-04 - val_loss: 8.1558e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1100e-04 - val_loss: 7.9964e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1039e-04 - val_loss: 7.8577e-05\n",
      "Validation RMSE:  0.008864394 \n",
      "\n",
      "Model took 1358.33 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 36s 925us/step - loss: 0.0063 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 0.0016 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 6.9902e-04 - val_loss: 6.9433e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 5.4056e-04 - val_loss: 5.5010e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 4.6880e-04 - val_loss: 3.7436e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 4.2589e-04 - val_loss: 2.9964e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.9543e-04 - val_loss: 2.7591e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 3.7177e-04 - val_loss: 2.6588e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 3.5327e-04 - val_loss: 2.6346e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 3.3730e-04 - val_loss: 2.7674e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 3.2442e-04 - val_loss: 2.6720e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.1425e-04 - val_loss: 2.4625e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 3.0527e-04 - val_loss: 2.4611e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.9561e-04 - val_loss: 2.3829e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 2.8884e-04 - val_loss: 2.2637e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.8082e-04 - val_loss: 2.2112e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.7294e-04 - val_loss: 2.1191e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.6634e-04 - val_loss: 2.2093e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.6222e-04 - val_loss: 2.0228e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.5443e-04 - val_loss: 1.9858e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.4384e-04 - val_loss: 2.0087e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 2.3404e-04 - val_loss: 1.8381e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.2466e-04 - val_loss: 1.6026e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.1962e-04 - val_loss: 1.8498e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.1355e-04 - val_loss: 1.4653e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.9981e-04 - val_loss: 1.4138e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.9106e-04 - val_loss: 1.4686e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.8093e-04 - val_loss: 1.3112e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.7779e-04 - val_loss: 1.2723e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.7338e-04 - val_loss: 1.2677e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.6910e-04 - val_loss: 1.2353e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.6654e-04 - val_loss: 1.1797e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.6364e-04 - val_loss: 1.1558e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.6115e-04 - val_loss: 1.1239e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.5825e-04 - val_loss: 1.1335e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.5482e-04 - val_loss: 1.0864e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.5245e-04 - val_loss: 1.0641e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.5034e-04 - val_loss: 1.0284e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.5002e-04 - val_loss: 1.0824e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.4791e-04 - val_loss: 1.1082e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4695e-04 - val_loss: 1.0006e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.4545e-04 - val_loss: 1.0534e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.4486e-04 - val_loss: 1.1014e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4297e-04 - val_loss: 1.0948e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.4199e-04 - val_loss: 1.1158e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.4129e-04 - val_loss: 9.8244e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.3897e-04 - val_loss: 9.7438e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.3824e-04 - val_loss: 9.8208e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.3618e-04 - val_loss: 9.6356e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.3446e-04 - val_loss: 1.0069e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.3296e-04 - val_loss: 1.0094e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.3167e-04 - val_loss: 9.9803e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.3027e-04 - val_loss: 9.6591e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2922e-04 - val_loss: 9.7992e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2841e-04 - val_loss: 9.5642e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2696e-04 - val_loss: 1.0039e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.2689e-04 - val_loss: 1.0244e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.2509e-04 - val_loss: 1.0602e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.2408e-04 - val_loss: 9.7865e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.2342e-04 - val_loss: 9.8662e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.2284e-04 - val_loss: 9.9843e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.2101e-04 - val_loss: 9.9244e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.2065e-04 - val_loss: 1.0076e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.2046e-04 - val_loss: 9.9079e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.1987e-04 - val_loss: 9.3568e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.1920e-04 - val_loss: 1.0302e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1745e-04 - val_loss: 1.0329e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.1664e-04 - val_loss: 1.0962e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1522e-04 - val_loss: 1.0481e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.1429e-04 - val_loss: 9.7924e-05\n",
      "Validation RMSE:  0.009895645 \n",
      "\n",
      "Model took 1380.55 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 37s 954us/step - loss: 0.0039 - val_loss: 0.0025\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 9.3469e-04 - val_loss: 9.3508e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 6.0527e-04 - val_loss: 4.6476e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 5.0081e-04 - val_loss: 4.5810e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 4.4383e-04 - val_loss: 3.8212e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 4.0743e-04 - val_loss: 3.5151e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.7898e-04 - val_loss: 3.4371e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 3.6382e-04 - val_loss: 3.4112e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 3.4713e-04 - val_loss: 3.3506e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.3524e-04 - val_loss: 3.1131e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.2473e-04 - val_loss: 3.0743e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.1285e-04 - val_loss: 3.4501e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.0477e-04 - val_loss: 3.2118e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.9820e-04 - val_loss: 3.2622e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.9079e-04 - val_loss: 3.1428e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.8226e-04 - val_loss: 2.9671e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.7113e-04 - val_loss: 3.4636e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.6295e-04 - val_loss: 3.8613e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.5435e-04 - val_loss: 3.5917e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 2.4831e-04 - val_loss: 2.7408e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.3484e-04 - val_loss: 2.5939e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.2577e-04 - val_loss: 2.6490e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.2089e-04 - val_loss: 2.2808e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.1096e-04 - val_loss: 2.0647e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.0447e-04 - val_loss: 1.7479e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.9876e-04 - val_loss: 1.4916e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.8880e-04 - val_loss: 1.5446e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.8140e-04 - val_loss: 1.3172e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.7564e-04 - val_loss: 1.2977e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.7040e-04 - val_loss: 1.1624e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.6801e-04 - val_loss: 1.1461e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.6509e-04 - val_loss: 1.2045e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.6158e-04 - val_loss: 1.2098e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5891e-04 - val_loss: 1.2340e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.5652e-04 - val_loss: 1.2289e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.5356e-04 - val_loss: 1.2552e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.5223e-04 - val_loss: 1.2129e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.5048e-04 - val_loss: 1.2097e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4862e-04 - val_loss: 1.1660e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.4662e-04 - val_loss: 1.1021e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.4393e-04 - val_loss: 1.1255e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.4164e-04 - val_loss: 1.1342e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.4004e-04 - val_loss: 1.0727e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3831e-04 - val_loss: 1.0364e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3634e-04 - val_loss: 1.0225e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3497e-04 - val_loss: 1.0073e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3345e-04 - val_loss: 9.6613e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3228e-04 - val_loss: 9.6034e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3155e-04 - val_loss: 1.0018e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3040e-04 - val_loss: 1.0054e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2937e-04 - val_loss: 1.0007e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2841e-04 - val_loss: 9.3073e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2768e-04 - val_loss: 9.4056e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2655e-04 - val_loss: 9.5817e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2516e-04 - val_loss: 1.0053e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2387e-04 - val_loss: 9.9414e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2282e-04 - val_loss: 1.0532e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2229e-04 - val_loss: 1.0245e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.2161e-04 - val_loss: 9.9817e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2072e-04 - val_loss: 9.7900e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.2014e-04 - val_loss: 9.9073e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1932e-04 - val_loss: 9.9850e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1781e-04 - val_loss: 1.0090e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1698e-04 - val_loss: 1.1057e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1675e-04 - val_loss: 1.0931e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1597e-04 - val_loss: 1.0578e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1502e-04 - val_loss: 1.0589e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1318e-04 - val_loss: 1.1533e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1268e-04 - val_loss: 1.0353e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1175e-04 - val_loss: 9.7362e-05\n",
      "Validation RMSE:  0.009867209 \n",
      "\n",
      "Model took 1359.94 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 36s 929us/step - loss: 0.0055 - val_loss: 0.0028\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 0.0010 - val_loss: 0.0010\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 6.3948e-04 - val_loss: 4.5411e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 5.2051e-04 - val_loss: 4.1182e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 4.5706e-04 - val_loss: 3.0564e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 4.1302e-04 - val_loss: 2.5348e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.8275e-04 - val_loss: 2.5230e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.6297e-04 - val_loss: 2.4538e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 3.4329e-04 - val_loss: 2.5491e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.2961e-04 - val_loss: 2.6273e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 3.1715e-04 - val_loss: 2.7811e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.0398e-04 - val_loss: 2.6802e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.8925e-04 - val_loss: 2.8858e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.7803e-04 - val_loss: 2.7408e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.6923e-04 - val_loss: 2.5806e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.6143e-04 - val_loss: 2.4756e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.4828e-04 - val_loss: 2.2405e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.3755e-04 - val_loss: 1.8877e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 2.3148e-04 - val_loss: 1.6485e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.2714e-04 - val_loss: 1.6313e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.2251e-04 - val_loss: 1.4326e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.1823e-04 - val_loss: 1.3634e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.1020e-04 - val_loss: 1.3180e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.0613e-04 - val_loss: 1.4621e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.9794e-04 - val_loss: 1.1819e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.9396e-04 - val_loss: 1.3914e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.8786e-04 - val_loss: 1.4441e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.8295e-04 - val_loss: 1.4904e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.7807e-04 - val_loss: 1.4108e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.7498e-04 - val_loss: 1.4378e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.7239e-04 - val_loss: 1.4283e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.6907e-04 - val_loss: 1.3774e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.6448e-04 - val_loss: 1.4200e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.6118e-04 - val_loss: 1.4126e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.5738e-04 - val_loss: 1.3645e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.5454e-04 - val_loss: 1.3299e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.4995e-04 - val_loss: 1.2464e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.4891e-04 - val_loss: 1.1798e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.4615e-04 - val_loss: 1.1676e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.4453e-04 - val_loss: 1.1348e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.4334e-04 - val_loss: 1.0610e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.4123e-04 - val_loss: 9.7716e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.3920e-04 - val_loss: 9.6532e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.3786e-04 - val_loss: 9.7757e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3620e-04 - val_loss: 9.6196e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3464e-04 - val_loss: 9.7962e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.3373e-04 - val_loss: 9.8254e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3247e-04 - val_loss: 9.7744e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3192e-04 - val_loss: 9.4069e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3058e-04 - val_loss: 9.2432e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2930e-04 - val_loss: 9.6632e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.2929e-04 - val_loss: 9.1328e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2791e-04 - val_loss: 9.5058e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2705e-04 - val_loss: 9.5369e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.2641e-04 - val_loss: 9.6218e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.2446e-04 - val_loss: 9.3334e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.2325e-04 - val_loss: 9.2145e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2273e-04 - val_loss: 9.1950e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2067e-04 - val_loss: 9.1424e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1894e-04 - val_loss: 9.0322e-05\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1822e-04 - val_loss: 8.8205e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1644e-04 - val_loss: 8.8157e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1661e-04 - val_loss: 9.2048e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1456e-04 - val_loss: 8.9881e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1248e-04 - val_loss: 8.6330e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1279e-04 - val_loss: 8.6502e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.1184e-04 - val_loss: 8.9389e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1173e-04 - val_loss: 8.9051e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.1118e-04 - val_loss: 8.8764e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.0962e-04 - val_loss: 8.9893e-05\n",
      "Validation RMSE:  0.009481192 \n",
      "\n",
      "Model took 1370.46 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 38s 970us/step - loss: 0.0047 - val_loss: 0.0029\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 0.0011 - val_loss: 9.3414e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 6.6181e-04 - val_loss: 5.7101e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 5.4360e-04 - val_loss: 3.3468e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 4.6734e-04 - val_loss: 3.5089e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 4.1479e-04 - val_loss: 3.4210e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 3.7901e-04 - val_loss: 3.2319e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 3.5508e-04 - val_loss: 3.3299e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.3750e-04 - val_loss: 2.8674e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.2546e-04 - val_loss: 2.6957e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 3.1476e-04 - val_loss: 2.5957e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 3.0552e-04 - val_loss: 2.5509e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.9757e-04 - val_loss: 2.5090e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.8856e-04 - val_loss: 2.5493e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.8025e-04 - val_loss: 2.5033e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 2.7022e-04 - val_loss: 2.2646e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 2.6128e-04 - val_loss: 2.3345e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 2.5325e-04 - val_loss: 2.3867e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.4058e-04 - val_loss: 2.6549e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 2.3390e-04 - val_loss: 1.6270e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.3206e-04 - val_loss: 1.4562e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.2173e-04 - val_loss: 1.4579e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 2.1162e-04 - val_loss: 1.3140e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 2.0065e-04 - val_loss: 1.5846e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.9480e-04 - val_loss: 1.2685e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.8662e-04 - val_loss: 1.0949e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.8127e-04 - val_loss: 1.0479e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.7543e-04 - val_loss: 1.0945e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.7124e-04 - val_loss: 1.0894e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.6710e-04 - val_loss: 1.0362e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.6452e-04 - val_loss: 1.0106e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.6045e-04 - val_loss: 9.9364e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.5792e-04 - val_loss: 1.0021e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.5576e-04 - val_loss: 1.0090e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.5385e-04 - val_loss: 1.0003e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.5200e-04 - val_loss: 9.8738e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.5011e-04 - val_loss: 1.0372e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.4820e-04 - val_loss: 1.0783e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.4661e-04 - val_loss: 1.1260e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4534e-04 - val_loss: 1.0818e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4386e-04 - val_loss: 1.0853e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.4268e-04 - val_loss: 1.1041e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.4146e-04 - val_loss: 1.1058e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4055e-04 - val_loss: 1.0820e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.3988e-04 - val_loss: 1.1126e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.3817e-04 - val_loss: 1.1206e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.3807e-04 - val_loss: 1.0648e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.3692e-04 - val_loss: 1.1313e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.3573e-04 - val_loss: 1.1184e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.3357e-04 - val_loss: 1.0229e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.3379e-04 - val_loss: 1.1562e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.3154e-04 - val_loss: 1.0972e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.3054e-04 - val_loss: 1.0997e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.3056e-04 - val_loss: 1.0874e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.2963e-04 - val_loss: 1.1259e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.2776e-04 - val_loss: 1.1068e-04\n",
      "Validation RMSE:  0.010520314 \n",
      "\n",
      "Model took 1111.87 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 37s 950us/step - loss: 0.0062 - val_loss: 0.0040\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 6.6425e-04 - val_loss: 7.0440e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 4.9704e-04 - val_loss: 7.3528e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 4.3278e-04 - val_loss: 4.6829e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.8949e-04 - val_loss: 3.6788e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 3.6279e-04 - val_loss: 3.1993e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.4579e-04 - val_loss: 2.8505e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 3.3169e-04 - val_loss: 2.7294e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 3.1982e-04 - val_loss: 2.7314e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.0842e-04 - val_loss: 2.7745e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.9789e-04 - val_loss: 2.6328e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.9133e-04 - val_loss: 2.4448e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 2.8385e-04 - val_loss: 2.4569e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.7606e-04 - val_loss: 2.4852e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.6844e-04 - val_loss: 2.5862e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.6070e-04 - val_loss: 2.4282e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.5002e-04 - val_loss: 2.3096e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.4066e-04 - val_loss: 2.2661e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 2.3258e-04 - val_loss: 2.1686e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.2064e-04 - val_loss: 3.2091e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.1665e-04 - val_loss: 2.0959e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 2.0704e-04 - val_loss: 2.1162e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.0130e-04 - val_loss: 1.6121e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 483us/step - loss: 1.9663e-04 - val_loss: 1.6139e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.9073e-04 - val_loss: 1.6757e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.8545e-04 - val_loss: 1.8442e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.7814e-04 - val_loss: 1.5814e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.7211e-04 - val_loss: 1.4760e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.6802e-04 - val_loss: 1.4372e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.6480e-04 - val_loss: 1.4540e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.6183e-04 - val_loss: 1.4449e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.5820e-04 - val_loss: 1.4653e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.5584e-04 - val_loss: 1.3768e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 482us/step - loss: 1.5367e-04 - val_loss: 1.4096e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.5211e-04 - val_loss: 1.4717e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.4992e-04 - val_loss: 1.6373e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.4850e-04 - val_loss: 1.5094e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.4670e-04 - val_loss: 1.6298e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.4498e-04 - val_loss: 1.5108e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.4284e-04 - val_loss: 1.3413e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.4227e-04 - val_loss: 1.4540e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.4083e-04 - val_loss: 1.4402e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3984e-04 - val_loss: 1.5256e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3912e-04 - val_loss: 1.3782e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.3781e-04 - val_loss: 1.1980e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3567e-04 - val_loss: 1.2189e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.3408e-04 - val_loss: 1.1716e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.3230e-04 - val_loss: 1.1243e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3205e-04 - val_loss: 1.1853e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.3154e-04 - val_loss: 1.1380e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.3163e-04 - val_loss: 1.1356e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2860e-04 - val_loss: 1.1366e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2729e-04 - val_loss: 1.0713e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2699e-04 - val_loss: 1.1204e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.2548e-04 - val_loss: 1.0961e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2451e-04 - val_loss: 1.0752e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2334e-04 - val_loss: 1.1692e-04\n",
      "Epoch 59/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.2242e-04 - val_loss: 1.1196e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2207e-04 - val_loss: 1.0940e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2111e-04 - val_loss: 1.0469e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.2022e-04 - val_loss: 1.0322e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1910e-04 - val_loss: 1.0026e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1864e-04 - val_loss: 1.0292e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 485us/step - loss: 1.1769e-04 - val_loss: 1.0114e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1664e-04 - val_loss: 9.3643e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 484us/step - loss: 1.1581e-04 - val_loss: 9.4659e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 1.1528e-04 - val_loss: 9.3380e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1448e-04 - val_loss: 9.0476e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1373e-04 - val_loss: 9.2303e-05\n",
      "Validation RMSE:  0.009607452 \n",
      "\n",
      "Model took 1362.65 seconds to train\n",
      "21 \t8     \t0.00973333\t0.000925343\t0.00856956\t0.0117703 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 38s 976us/step - loss: 0.0066 - val_loss: 0.0045\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 6.8418e-04 - val_loss: 0.0013\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 5.2826e-04 - val_loss: 5.1724e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 4.5529e-04 - val_loss: 3.4766e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 4.0599e-04 - val_loss: 2.9423e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 3.7331e-04 - val_loss: 2.7877e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 3.5024e-04 - val_loss: 2.6105e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 3.3387e-04 - val_loss: 2.4571e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 3.1989e-04 - val_loss: 2.3667e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 3.0944e-04 - val_loss: 2.3009e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 2.9879e-04 - val_loss: 2.3142e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.9056e-04 - val_loss: 2.2568e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.7947e-04 - val_loss: 2.5316e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 2.7138e-04 - val_loss: 2.5063e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.6181e-04 - val_loss: 2.3475e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 2.5062e-04 - val_loss: 2.2267e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 2.3838e-04 - val_loss: 2.1828e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 2.3048e-04 - val_loss: 1.5245e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.2748e-04 - val_loss: 1.3339e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 2.1406e-04 - val_loss: 1.2762e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 2.0142e-04 - val_loss: 1.7586e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.9517e-04 - val_loss: 1.1906e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.8834e-04 - val_loss: 1.1059e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.8344e-04 - val_loss: 1.0356e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.7665e-04 - val_loss: 1.0161e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.7323e-04 - val_loss: 1.0396e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.7005e-04 - val_loss: 1.0066e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.6422e-04 - val_loss: 1.0073e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.6399e-04 - val_loss: 9.8689e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.6003e-04 - val_loss: 1.0148e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.5533e-04 - val_loss: 9.8677e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.5542e-04 - val_loss: 9.3971e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.5348e-04 - val_loss: 9.6370e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.5064e-04 - val_loss: 1.0315e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.4871e-04 - val_loss: 1.0314e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.4547e-04 - val_loss: 1.0126e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.4473e-04 - val_loss: 9.7890e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.4204e-04 - val_loss: 9.9431e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.4044e-04 - val_loss: 9.3417e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.3858e-04 - val_loss: 9.2581e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.3484e-04 - val_loss: 9.1233e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3456e-04 - val_loss: 9.1520e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.3212e-04 - val_loss: 8.6749e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.3113e-04 - val_loss: 8.5646e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.2995e-04 - val_loss: 8.8979e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.2929e-04 - val_loss: 8.8922e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.2795e-04 - val_loss: 8.9787e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2801e-04 - val_loss: 8.8782e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2694e-04 - val_loss: 9.1144e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.2579e-04 - val_loss: 9.2615e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.2475e-04 - val_loss: 9.1562e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.2382e-04 - val_loss: 9.1983e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.2289e-04 - val_loss: 9.2652e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.2119e-04 - val_loss: 9.1523e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.2034e-04 - val_loss: 9.9403e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.1942e-04 - val_loss: 9.7293e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.1896e-04 - val_loss: 9.7858e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.1678e-04 - val_loss: 9.6398e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.1527e-04 - val_loss: 9.5182e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1392e-04 - val_loss: 1.0103e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.1327e-04 - val_loss: 1.0247e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.1130e-04 - val_loss: 9.9747e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.1159e-04 - val_loss: 1.0436e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.0995e-04 - val_loss: 1.1007e-04\n",
      "Validation RMSE:  0.0104914 \n",
      "\n",
      "Model took 1297.48 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 38s 981us/step - loss: 0.0060 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 0.0015 - val_loss: 0.0021\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 6.8866e-04 - val_loss: 7.5612e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 5.1938e-04 - val_loss: 3.0553e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 4.4272e-04 - val_loss: 2.5662e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 3.9809e-04 - val_loss: 2.5990e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 3.6904e-04 - val_loss: 2.4525e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 3.4511e-04 - val_loss: 2.3342e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 3.2687e-04 - val_loss: 2.3615e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 3.1170e-04 - val_loss: 2.3984e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.9949e-04 - val_loss: 2.3191e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.8825e-04 - val_loss: 2.2150e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.7571e-04 - val_loss: 2.1184e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.6314e-04 - val_loss: 2.0557e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.5283e-04 - val_loss: 2.0836e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.3641e-04 - val_loss: 2.5399e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.2605e-04 - val_loss: 1.8291e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.1514e-04 - val_loss: 1.9746e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.0480e-04 - val_loss: 1.5743e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.9244e-04 - val_loss: 1.3148e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.8421e-04 - val_loss: 1.1653e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.7862e-04 - val_loss: 1.0917e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.7386e-04 - val_loss: 1.0798e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.6943e-04 - val_loss: 1.0874e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.6528e-04 - val_loss: 1.1123e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.6155e-04 - val_loss: 1.1429e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.5826e-04 - val_loss: 1.1367e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.5458e-04 - val_loss: 1.0905e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.5325e-04 - val_loss: 1.1384e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.5110e-04 - val_loss: 1.1861e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4896e-04 - val_loss: 1.1552e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.4618e-04 - val_loss: 1.2008e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.4498e-04 - val_loss: 1.2016e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.4296e-04 - val_loss: 1.1757e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4026e-04 - val_loss: 1.1508e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.3895e-04 - val_loss: 1.1275e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.3811e-04 - val_loss: 1.0851e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.3713e-04 - val_loss: 1.0850e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.3609e-04 - val_loss: 1.0945e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.3515e-04 - val_loss: 1.1409e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.3339e-04 - val_loss: 1.1267e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.3244e-04 - val_loss: 1.1084e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.3126e-04 - val_loss: 1.0693e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.3028e-04 - val_loss: 1.0930e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2980e-04 - val_loss: 1.0694e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.2873e-04 - val_loss: 1.1026e-04\n",
      "Epoch 47/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.2780e-04 - val_loss: 1.0804e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.2679e-04 - val_loss: 1.1384e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2598e-04 - val_loss: 1.1111e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.2462e-04 - val_loss: 1.0760e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.2397e-04 - val_loss: 1.0992e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2421e-04 - val_loss: 1.1412e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.2288e-04 - val_loss: 1.1540e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2224e-04 - val_loss: 1.1576e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2169e-04 - val_loss: 1.1169e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.2041e-04 - val_loss: 1.1020e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1999e-04 - val_loss: 1.0817e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.1910e-04 - val_loss: 1.0561e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.1842e-04 - val_loss: 1.0332e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1798e-04 - val_loss: 1.0449e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.1754e-04 - val_loss: 1.0506e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.1634e-04 - val_loss: 1.0403e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.1543e-04 - val_loss: 1.0374e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.1277e-04 - val_loss: 9.8714e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1188e-04 - val_loss: 9.0105e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1275e-04 - val_loss: 9.9531e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1130e-04 - val_loss: 9.7500e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.1210e-04 - val_loss: 9.5144e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1109e-04 - val_loss: 9.6943e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1026e-04 - val_loss: 9.5557e-05\n",
      "Validation RMSE:  0.0097753275 \n",
      "\n",
      "Model took 1382.73 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 39s 1ms/step - loss: 0.0080 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 7.4941e-04 - val_loss: 7.6722e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 5.5581e-04 - val_loss: 3.4091e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 4.7771e-04 - val_loss: 3.2572e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 4.2166e-04 - val_loss: 3.2696e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 3.7827e-04 - val_loss: 3.0310e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 3.5028e-04 - val_loss: 2.8235e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 3.3014e-04 - val_loss: 2.7060e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 3.1593e-04 - val_loss: 2.5923e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 2.9722e-04 - val_loss: 2.6763e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 2.8315e-04 - val_loss: 2.7796e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.7060e-04 - val_loss: 2.6366e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.6000e-04 - val_loss: 2.5298e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 2.5259e-04 - val_loss: 2.4537e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 2.4701e-04 - val_loss: 2.3792e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 2.4166e-04 - val_loss: 2.2687e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 2.3693e-04 - val_loss: 2.2490e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.3136e-04 - val_loss: 1.9317e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 2.2283e-04 - val_loss: 1.7752e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 2.1098e-04 - val_loss: 2.0696e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 2.0628e-04 - val_loss: 1.5080e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.9843e-04 - val_loss: 1.4199e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.9099e-04 - val_loss: 1.2766e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.8357e-04 - val_loss: 1.1586e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.7729e-04 - val_loss: 1.0930e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.7224e-04 - val_loss: 1.0359e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.6671e-04 - val_loss: 9.9024e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.6221e-04 - val_loss: 1.0147e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.5925e-04 - val_loss: 1.0035e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.5587e-04 - val_loss: 9.7144e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.5336e-04 - val_loss: 9.4984e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.5074e-04 - val_loss: 9.3711e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.4790e-04 - val_loss: 9.1732e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.4672e-04 - val_loss: 9.2219e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.4528e-04 - val_loss: 9.2087e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.4391e-04 - val_loss: 8.9675e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.4194e-04 - val_loss: 9.1247e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.4111e-04 - val_loss: 9.0551e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.3959e-04 - val_loss: 8.6775e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.3892e-04 - val_loss: 8.8530e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.3679e-04 - val_loss: 8.7972e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.3615e-04 - val_loss: 8.5094e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.3457e-04 - val_loss: 8.7290e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.3427e-04 - val_loss: 8.4466e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.3304e-04 - val_loss: 8.3219e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.3189e-04 - val_loss: 8.1732e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.3010e-04 - val_loss: 8.1941e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.2917e-04 - val_loss: 8.2263e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.2829e-04 - val_loss: 8.1569e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.2619e-04 - val_loss: 8.2464e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.2754e-04 - val_loss: 7.9649e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.2616e-04 - val_loss: 7.9386e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2491e-04 - val_loss: 7.9363e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.2290e-04 - val_loss: 7.9877e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.2183e-04 - val_loss: 7.8981e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.2015e-04 - val_loss: 8.2850e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.1947e-04 - val_loss: 8.6281e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.1896e-04 - val_loss: 8.1649e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1847e-04 - val_loss: 8.1789e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.1790e-04 - val_loss: 8.0506e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.1640e-04 - val_loss: 8.1722e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.1639e-04 - val_loss: 8.4390e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.1543e-04 - val_loss: 8.1606e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1529e-04 - val_loss: 8.2360e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.1406e-04 - val_loss: 8.5047e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1353e-04 - val_loss: 8.8093e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.1360e-04 - val_loss: 8.5726e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.1210e-04 - val_loss: 8.6417e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.1207e-04 - val_loss: 8.4667e-05\n",
      "Validation RMSE:  0.0092014605 \n",
      "\n",
      "Model took 1422.99 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 40s 1ms/step - loss: 0.0044 - val_loss: 0.0032\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 6.8434e-04 - val_loss: 5.8093e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 5.6870e-04 - val_loss: 4.0412e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 4.9582e-04 - val_loss: 4.2326e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 4.4739e-04 - val_loss: 3.6032e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 4.0873e-04 - val_loss: 2.7650e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 3.8160e-04 - val_loss: 2.7341e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 3.5867e-04 - val_loss: 2.6217e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 3.4298e-04 - val_loss: 2.5779e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 3.2833e-04 - val_loss: 2.6388e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 3.1817e-04 - val_loss: 2.4519e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 3.0771e-04 - val_loss: 2.4177e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.9587e-04 - val_loss: 2.3473e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.8248e-04 - val_loss: 2.3293e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 2.7381e-04 - val_loss: 2.4739e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.6417e-04 - val_loss: 2.3230e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.5623e-04 - val_loss: 2.2887e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.4275e-04 - val_loss: 2.1143e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.3566e-04 - val_loss: 2.3155e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 2.3491e-04 - val_loss: 1.8534e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 2.2441e-04 - val_loss: 1.6664e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 2.1290e-04 - val_loss: 1.8177e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 2.0425e-04 - val_loss: 1.5312e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.9415e-04 - val_loss: 1.5082e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.8589e-04 - val_loss: 1.4141e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.8079e-04 - val_loss: 1.2485e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.7453e-04 - val_loss: 1.1533e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.7026e-04 - val_loss: 1.0969e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.6649e-04 - val_loss: 1.0354e-04\n",
      "Epoch 31/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.6391e-04 - val_loss: 1.0029e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.6110e-04 - val_loss: 9.7747e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.5890e-04 - val_loss: 9.5854e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.5657e-04 - val_loss: 9.4620e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.5320e-04 - val_loss: 9.1673e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.5006e-04 - val_loss: 9.1875e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4753e-04 - val_loss: 9.2863e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.4514e-04 - val_loss: 9.4774e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.4301e-04 - val_loss: 9.5623e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.4000e-04 - val_loss: 9.9259e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.3891e-04 - val_loss: 1.0234e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.3777e-04 - val_loss: 9.8711e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.3613e-04 - val_loss: 9.9191e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.3521e-04 - val_loss: 1.0014e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.3345e-04 - val_loss: 9.4918e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.3256e-04 - val_loss: 9.6108e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.3145e-04 - val_loss: 9.8982e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.3042e-04 - val_loss: 9.6549e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.2937e-04 - val_loss: 9.4262e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.2823e-04 - val_loss: 9.5451e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.2762e-04 - val_loss: 9.2287e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2679e-04 - val_loss: 9.3181e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2595e-04 - val_loss: 8.9656e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2508e-04 - val_loss: 8.6229e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2414e-04 - val_loss: 8.7577e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.2319e-04 - val_loss: 8.5420e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2195e-04 - val_loss: 8.4413e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.2111e-04 - val_loss: 8.5138e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.1999e-04 - val_loss: 8.3412e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.1829e-04 - val_loss: 8.4099e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.1754e-04 - val_loss: 8.6377e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.1607e-04 - val_loss: 8.8734e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.1554e-04 - val_loss: 8.5620e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 19s 494us/step - loss: 1.1526e-04 - val_loss: 8.8141e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.1404e-04 - val_loss: 8.6275e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.1418e-04 - val_loss: 8.7434e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1323e-04 - val_loss: 9.2570e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1281e-04 - val_loss: 8.0386e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1367e-04 - val_loss: 8.7190e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.1318e-04 - val_loss: 8.6372e-05\n",
      "Validation RMSE:  0.009293637 \n",
      "\n",
      "Model took 1387.99 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 40s 1ms/step - loss: 0.0068 - val_loss: 0.0048\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 7.3772e-04 - val_loss: 7.3020e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 5.5801e-04 - val_loss: 3.9483e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 4.8614e-04 - val_loss: 3.4019e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 4.3409e-04 - val_loss: 3.2888e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.9647e-04 - val_loss: 2.9379e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 3.7446e-04 - val_loss: 2.7014e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.5666e-04 - val_loss: 2.7993e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 3.4354e-04 - val_loss: 2.7917e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 3.3021e-04 - val_loss: 2.7857e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 3.1804e-04 - val_loss: 2.7520e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 3.0198e-04 - val_loss: 2.6095e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.9004e-04 - val_loss: 2.6054e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.8034e-04 - val_loss: 2.3205e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 2.6648e-04 - val_loss: 2.1370e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.5489e-04 - val_loss: 2.1641e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.4629e-04 - val_loss: 1.7225e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 486us/step - loss: 2.3597e-04 - val_loss: 1.5754e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 2.2626e-04 - val_loss: 1.4477e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 2.1421e-04 - val_loss: 1.4103e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 2.0554e-04 - val_loss: 1.2551e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.9632e-04 - val_loss: 1.0599e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.8964e-04 - val_loss: 1.0486e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.8420e-04 - val_loss: 1.0085e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.7950e-04 - val_loss: 1.0683e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.7394e-04 - val_loss: 9.8280e-05\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.6838e-04 - val_loss: 1.0012e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.6441e-04 - val_loss: 1.0750e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.6004e-04 - val_loss: 1.0344e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.5678e-04 - val_loss: 9.7807e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.5330e-04 - val_loss: 9.7123e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.5040e-04 - val_loss: 1.0029e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.4816e-04 - val_loss: 1.0133e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.4492e-04 - val_loss: 9.8487e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.4171e-04 - val_loss: 9.3433e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.4008e-04 - val_loss: 8.8755e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.3887e-04 - val_loss: 8.7205e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.3722e-04 - val_loss: 8.5315e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3514e-04 - val_loss: 8.6133e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.3435e-04 - val_loss: 8.7674e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.3303e-04 - val_loss: 9.1769e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.3160e-04 - val_loss: 9.0082e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.2973e-04 - val_loss: 8.8922e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2795e-04 - val_loss: 8.9054e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2612e-04 - val_loss: 9.0861e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 19s 492us/step - loss: 1.2544e-04 - val_loss: 9.4674e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.2375e-04 - val_loss: 9.4459e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.2267e-04 - val_loss: 9.6287e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 493us/step - loss: 1.2128e-04 - val_loss: 9.3868e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2167e-04 - val_loss: 9.8974e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.2048e-04 - val_loss: 9.5955e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 488us/step - loss: 1.1992e-04 - val_loss: 9.3643e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 491us/step - loss: 1.1881e-04 - val_loss: 9.4684e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1779e-04 - val_loss: 9.3199e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 489us/step - loss: 1.1673e-04 - val_loss: 9.3110e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1557e-04 - val_loss: 9.3545e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 19s 490us/step - loss: 1.1526e-04 - val_loss: 9.1286e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 19s 487us/step - loss: 1.1415e-04 - val_loss: 9.1050e-05\n",
      "Validation RMSE:  0.009542007 \n",
      "\n",
      "Model took 1163.99 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 40s 1ms/step - loss: 0.0068 - val_loss: 0.0050\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 0.0018 - val_loss: 0.0022\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 8.3118e-04 - val_loss: 0.0013\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 5.6252e-04 - val_loss: 6.1948e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 4.7934e-04 - val_loss: 4.2764e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 4.2429e-04 - val_loss: 3.8446e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 3.7938e-04 - val_loss: 3.6539e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 3.5242e-04 - val_loss: 3.2727e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 3.3241e-04 - val_loss: 2.7969e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 3.1974e-04 - val_loss: 2.7684e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 3.0690e-04 - val_loss: 3.0526e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 2.9512e-04 - val_loss: 2.8389e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 2.8355e-04 - val_loss: 2.7066e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.7203e-04 - val_loss: 2.6177e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.6263e-04 - val_loss: 2.4959e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 2.4628e-04 - val_loss: 2.4165e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.3571e-04 - val_loss: 2.0045e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.2930e-04 - val_loss: 1.6787e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 2.2769e-04 - val_loss: 1.6045e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 2.1629e-04 - val_loss: 1.6023e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.0352e-04 - val_loss: 1.5759e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.9758e-04 - val_loss: 1.3519e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.8902e-04 - val_loss: 1.1995e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.8340e-04 - val_loss: 9.8739e-05\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.7695e-04 - val_loss: 1.0459e-04\n",
      "Epoch 26/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.7087e-04 - val_loss: 1.0496e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.6642e-04 - val_loss: 1.1206e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.6235e-04 - val_loss: 1.0043e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.5939e-04 - val_loss: 1.0265e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.5658e-04 - val_loss: 9.9963e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.5415e-04 - val_loss: 9.2924e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.5195e-04 - val_loss: 9.7440e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.4924e-04 - val_loss: 9.1063e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.4764e-04 - val_loss: 9.5152e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.4616e-04 - val_loss: 9.2177e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.4483e-04 - val_loss: 8.9234e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.4275e-04 - val_loss: 8.7248e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.4162e-04 - val_loss: 8.7621e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.3947e-04 - val_loss: 8.8320e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.3861e-04 - val_loss: 8.7845e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3754e-04 - val_loss: 9.1725e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.3547e-04 - val_loss: 9.1957e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.3368e-04 - val_loss: 9.3702e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.3195e-04 - val_loss: 9.7745e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.3120e-04 - val_loss: 9.6262e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.2948e-04 - val_loss: 9.2840e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.2795e-04 - val_loss: 9.5422e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2554e-04 - val_loss: 9.8936e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2350e-04 - val_loss: 1.1539e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.2213e-04 - val_loss: 1.1236e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.2145e-04 - val_loss: 1.1306e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.1941e-04 - val_loss: 9.2650e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.1880e-04 - val_loss: 1.1412e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.1823e-04 - val_loss: 1.1443e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1781e-04 - val_loss: 1.1860e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1741e-04 - val_loss: 1.2427e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 19s 496us/step - loss: 1.1680e-04 - val_loss: 1.2984e-04\n",
      "Validation RMSE:  0.011394635 \n",
      "\n",
      "Model took 1145.98 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 41s 1ms/step - loss: 0.0056 - val_loss: 0.0044\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 7.1021e-04 - val_loss: 8.7352e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 5.4791e-04 - val_loss: 3.4603e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 4.7283e-04 - val_loss: 3.7765e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 4.2180e-04 - val_loss: 3.1864e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 3.8288e-04 - val_loss: 3.1237e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 3.5992e-04 - val_loss: 3.0429e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 3.3810e-04 - val_loss: 2.6257e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 3.2447e-04 - val_loss: 2.6204e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 3.1270e-04 - val_loss: 2.8444e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 3.0209e-04 - val_loss: 3.0296e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.9102e-04 - val_loss: 2.9863e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 2.7667e-04 - val_loss: 2.9881e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 2.6791e-04 - val_loss: 2.7965e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.5392e-04 - val_loss: 2.4401e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.4066e-04 - val_loss: 2.3758e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 2.3621e-04 - val_loss: 1.7756e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 2.2793e-04 - val_loss: 2.0346e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 2.2298e-04 - val_loss: 1.6931e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 2.1171e-04 - val_loss: 1.5544e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 2.0385e-04 - val_loss: 1.4027e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.9695e-04 - val_loss: 1.1782e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.8887e-04 - val_loss: 1.1168e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.8197e-04 - val_loss: 1.1524e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.7656e-04 - val_loss: 1.2424e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.7147e-04 - val_loss: 1.2168e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 19s 495us/step - loss: 1.6701e-04 - val_loss: 1.2899e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.6440e-04 - val_loss: 1.1919e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.5945e-04 - val_loss: 1.2617e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.5579e-04 - val_loss: 1.1816e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.5267e-04 - val_loss: 1.0927e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.5036e-04 - val_loss: 1.0566e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.4861e-04 - val_loss: 1.0462e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.4701e-04 - val_loss: 1.0165e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.4372e-04 - val_loss: 9.6093e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.4234e-04 - val_loss: 1.0194e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.4021e-04 - val_loss: 9.9560e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.3867e-04 - val_loss: 9.4948e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3863e-04 - val_loss: 9.1307e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.3761e-04 - val_loss: 9.0901e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3614e-04 - val_loss: 9.1314e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.3563e-04 - val_loss: 9.5683e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.3436e-04 - val_loss: 9.2496e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3331e-04 - val_loss: 9.3167e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.3085e-04 - val_loss: 9.3175e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.3022e-04 - val_loss: 9.3681e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2917e-04 - val_loss: 9.4394e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.2872e-04 - val_loss: 9.1693e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2680e-04 - val_loss: 9.0491e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2622e-04 - val_loss: 8.8053e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.2577e-04 - val_loss: 8.5827e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.2525e-04 - val_loss: 8.4475e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.2465e-04 - val_loss: 8.3484e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.2369e-04 - val_loss: 8.5666e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.2266e-04 - val_loss: 8.6665e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2225e-04 - val_loss: 8.6659e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2150e-04 - val_loss: 8.4020e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2144e-04 - val_loss: 8.7844e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2027e-04 - val_loss: 8.4813e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 19s 497us/step - loss: 1.1965e-04 - val_loss: 8.5872e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1937e-04 - val_loss: 9.0900e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.1891e-04 - val_loss: 9.2570e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1817e-04 - val_loss: 9.4033e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.1708e-04 - val_loss: 9.7328e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1522e-04 - val_loss: 9.5719e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.1406e-04 - val_loss: 9.4699e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1359e-04 - val_loss: 1.0148e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.1329e-04 - val_loss: 1.0254e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1249e-04 - val_loss: 1.0261e-04\n",
      "Validation RMSE:  0.010129619 \n",
      "\n",
      "Model took 1402.78 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 40s 1ms/step - loss: 0.0082 - val_loss: 0.0058\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 0.0022 - val_loss: 0.0025\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 9.8224e-04 - val_loss: 0.0012\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 5.7673e-04 - val_loss: 4.1463e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 4.6777e-04 - val_loss: 3.1748e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 4.1069e-04 - val_loss: 2.8521e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 3.7147e-04 - val_loss: 2.7623e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 3.4129e-04 - val_loss: 2.5099e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 3.2239e-04 - val_loss: 2.4123e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 3.1290e-04 - val_loss: 2.2550e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 3.0223e-04 - val_loss: 2.3740e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 2.9483e-04 - val_loss: 2.0707e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.8682e-04 - val_loss: 2.1174e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 2.7819e-04 - val_loss: 2.1238e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 2.6975e-04 - val_loss: 2.3333e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.5881e-04 - val_loss: 2.5646e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.5073e-04 - val_loss: 2.5531e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 2.4359e-04 - val_loss: 2.5436e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 2.3186e-04 - val_loss: 2.3880e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.1899e-04 - val_loss: 2.2112e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.1452e-04 - val_loss: 1.6093e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 2.0681e-04 - val_loss: 1.5903e-04\n",
      "Epoch 23/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.9665e-04 - val_loss: 1.3036e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.8965e-04 - val_loss: 1.2242e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.8355e-04 - val_loss: 1.0842e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.7712e-04 - val_loss: 1.0082e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.7057e-04 - val_loss: 9.9485e-05\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.6650e-04 - val_loss: 9.6542e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.6351e-04 - val_loss: 9.3161e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.5958e-04 - val_loss: 9.3804e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.5673e-04 - val_loss: 9.3916e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.5424e-04 - val_loss: 9.1796e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.5208e-04 - val_loss: 8.9834e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.4917e-04 - val_loss: 9.1392e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.4732e-04 - val_loss: 9.1510e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.4392e-04 - val_loss: 9.0678e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.4241e-04 - val_loss: 9.2567e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.4076e-04 - val_loss: 9.1201e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.3915e-04 - val_loss: 8.5159e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.3868e-04 - val_loss: 8.4756e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.3716e-04 - val_loss: 8.5437e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.3588e-04 - val_loss: 8.2099e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.3514e-04 - val_loss: 8.4880e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.3327e-04 - val_loss: 8.4283e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3442e-04 - val_loss: 8.1377e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.3310e-04 - val_loss: 8.1459e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3162e-04 - val_loss: 8.3557e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.3029e-04 - val_loss: 8.5093e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2928e-04 - val_loss: 8.3179e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2790e-04 - val_loss: 8.3326e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.2674e-04 - val_loss: 8.4345e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.2558e-04 - val_loss: 8.4566e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.2464e-04 - val_loss: 8.5144e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2344e-04 - val_loss: 8.6021e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.2235e-04 - val_loss: 8.7137e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.2098e-04 - val_loss: 8.6961e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.1933e-04 - val_loss: 8.9838e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.1805e-04 - val_loss: 8.6139e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1592e-04 - val_loss: 8.7509e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 19s 498us/step - loss: 1.1608e-04 - val_loss: 8.6703e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.1530e-04 - val_loss: 8.4878e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1236e-04 - val_loss: 8.9220e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.1282e-04 - val_loss: 8.5714e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1224e-04 - val_loss: 8.4456e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.1206e-04 - val_loss: 8.5580e-05\n",
      "Validation RMSE:  0.009250946 \n",
      "\n",
      "Model took 1307.28 seconds to train\n",
      "22 \t8     \t0.00962181\t0.000826033\t0.00856956\t0.0113946 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 3 11\n",
      "1st Window Start: 42\n",
      "2nd Window Start: 502\n",
      "3rd Window Start: 1020\n",
      "4th Window Start: 1068\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 1020, 1021, 1022, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 41s 1ms/step - loss: 0.0023 - val_loss: 8.9329e-04\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 7.5185e-04 - val_loss: 5.2952e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 6.7085e-04 - val_loss: 4.9445e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 6.0634e-04 - val_loss: 5.7835e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 5.6321e-04 - val_loss: 4.6296e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 5.4074e-04 - val_loss: 4.2908e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 5.1670e-04 - val_loss: 3.5103e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 5.0246e-04 - val_loss: 3.0347e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 4.8345e-04 - val_loss: 3.1262e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 4.6746e-04 - val_loss: 3.1496e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 4.4939e-04 - val_loss: 3.9307e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 4.4169e-04 - val_loss: 3.7970e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 4.2882e-04 - val_loss: 4.4725e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 4.1985e-04 - val_loss: 4.6617e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 4.1107e-04 - val_loss: 5.7173e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 4.0346e-04 - val_loss: 5.0271e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 3.9321e-04 - val_loss: 5.8454e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 3.8545e-04 - val_loss: 6.9847e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 3.7966e-04 - val_loss: 6.3374e-04\n",
      "Epoch 20/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 505us/step - loss: 3.7630e-04 - val_loss: 5.6122e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 3.7007e-04 - val_loss: 8.2065e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 3.6735e-04 - val_loss: 5.8368e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 3.6298e-04 - val_loss: 6.1112e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 3.6331e-04 - val_loss: 5.7894e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 3.5175e-04 - val_loss: 6.2959e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 3.4748e-04 - val_loss: 5.1469e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 3.4153e-04 - val_loss: 4.2539e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 3.3800e-04 - val_loss: 3.8571e-04\n",
      "Validation RMSE:  0.019639598 \n",
      "\n",
      "Model took 587.59 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 41s 1ms/step - loss: 0.0082 - val_loss: 0.0052\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 9.4011e-04 - val_loss: 9.4582e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 5.7432e-04 - val_loss: 4.5001e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 4.7082e-04 - val_loss: 3.3290e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 4.1720e-04 - val_loss: 3.0760e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 3.7784e-04 - val_loss: 2.9478e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 3.5388e-04 - val_loss: 2.9227e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 3.3387e-04 - val_loss: 2.7694e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 3.1869e-04 - val_loss: 2.6369e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 3.0858e-04 - val_loss: 2.5561e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.9676e-04 - val_loss: 2.6164e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 2.8665e-04 - val_loss: 2.6522e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.7757e-04 - val_loss: 2.8045e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.6942e-04 - val_loss: 2.6601e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.6014e-04 - val_loss: 2.4747e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.5380e-04 - val_loss: 2.4433e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.4684e-04 - val_loss: 2.4960e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 2.3978e-04 - val_loss: 2.3349e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 2.2588e-04 - val_loss: 2.1494e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.1717e-04 - val_loss: 1.7668e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 2.1649e-04 - val_loss: 1.6149e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.0598e-04 - val_loss: 1.7385e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.9739e-04 - val_loss: 1.4830e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.8875e-04 - val_loss: 1.1833e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.8244e-04 - val_loss: 1.2034e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.7539e-04 - val_loss: 1.1550e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.7040e-04 - val_loss: 1.1512e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.6619e-04 - val_loss: 1.0824e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.6349e-04 - val_loss: 1.0601e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.6075e-04 - val_loss: 1.0082e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.5836e-04 - val_loss: 9.5666e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.5682e-04 - val_loss: 9.2570e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.5347e-04 - val_loss: 8.8204e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.5165e-04 - val_loss: 9.3064e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.4951e-04 - val_loss: 9.1524e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.4798e-04 - val_loss: 9.1764e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.4665e-04 - val_loss: 9.0635e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.4482e-04 - val_loss: 9.0107e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.4283e-04 - val_loss: 8.7194e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.4090e-04 - val_loss: 8.3417e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.3913e-04 - val_loss: 8.8820e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.3778e-04 - val_loss: 8.5975e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3667e-04 - val_loss: 8.5700e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.3394e-04 - val_loss: 8.6031e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.3284e-04 - val_loss: 8.4188e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.3245e-04 - val_loss: 8.2581e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2981e-04 - val_loss: 8.4565e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.3076e-04 - val_loss: 8.2268e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.2975e-04 - val_loss: 8.0833e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.2838e-04 - val_loss: 7.9669e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.2805e-04 - val_loss: 8.0089e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.2672e-04 - val_loss: 7.9239e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.2520e-04 - val_loss: 7.9824e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2461e-04 - val_loss: 7.9413e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2361e-04 - val_loss: 7.9930e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2304e-04 - val_loss: 8.0823e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2178e-04 - val_loss: 8.0557e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2086e-04 - val_loss: 8.0187e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1965e-04 - val_loss: 7.9582e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.1855e-04 - val_loss: 8.0188e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.1778e-04 - val_loss: 7.9476e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.1673e-04 - val_loss: 7.9496e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.1518e-04 - val_loss: 8.0054e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1418e-04 - val_loss: 8.0138e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1313e-04 - val_loss: 8.0918e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1143e-04 - val_loss: 7.9762e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.1104e-04 - val_loss: 8.0306e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.0991e-04 - val_loss: 8.6612e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.0828e-04 - val_loss: 8.5679e-05\n",
      "Validation RMSE:  0.009256285 \n",
      "\n",
      "Model took 1408.21 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 41s 1ms/step - loss: 0.0051 - val_loss: 0.0036\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 6.7201e-04 - val_loss: 5.7050e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 5.3183e-04 - val_loss: 3.8237e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 4.6548e-04 - val_loss: 5.0150e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 4.2394e-04 - val_loss: 4.8449e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 3.9401e-04 - val_loss: 3.5585e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 3.6691e-04 - val_loss: 2.9083e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 3.4768e-04 - val_loss: 3.2563e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 3.3360e-04 - val_loss: 3.2065e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 3.2151e-04 - val_loss: 2.7462e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 3.1275e-04 - val_loss: 2.6893e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 3.0108e-04 - val_loss: 2.6254e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 2.9116e-04 - val_loss: 2.4238e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 2.7979e-04 - val_loss: 2.3330e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 2.6794e-04 - val_loss: 2.1298e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.5960e-04 - val_loss: 1.9797e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.4645e-04 - val_loss: 1.8850e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.3595e-04 - val_loss: 1.8736e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.3037e-04 - val_loss: 1.5007e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 2.2176e-04 - val_loss: 1.7793e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 2.1793e-04 - val_loss: 1.6723e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.0839e-04 - val_loss: 1.4019e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 2.0148e-04 - val_loss: 1.5294e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.9437e-04 - val_loss: 1.2670e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.8613e-04 - val_loss: 1.0807e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.8115e-04 - val_loss: 1.0523e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.7615e-04 - val_loss: 1.0966e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 498us/step - loss: 1.7187e-04 - val_loss: 1.0720e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.6888e-04 - val_loss: 1.1105e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.6581e-04 - val_loss: 1.1808e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.6193e-04 - val_loss: 1.2137e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.5863e-04 - val_loss: 1.1432e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.5676e-04 - val_loss: 1.1389e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.5447e-04 - val_loss: 1.1159e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.5108e-04 - val_loss: 1.1055e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.4865e-04 - val_loss: 1.1121e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.4619e-04 - val_loss: 1.0884e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.4370e-04 - val_loss: 1.0632e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.4190e-04 - val_loss: 1.0171e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.3980e-04 - val_loss: 1.0204e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.3551e-04 - val_loss: 1.2857e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.3659e-04 - val_loss: 1.0779e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.3536e-04 - val_loss: 9.0734e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.3208e-04 - val_loss: 8.6680e-05\n",
      "Epoch 46/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.3097e-04 - val_loss: 8.9199e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3109e-04 - val_loss: 8.1776e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.2801e-04 - val_loss: 8.8489e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.2491e-04 - val_loss: 1.0650e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.2528e-04 - val_loss: 8.5297e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2432e-04 - val_loss: 7.9663e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.2265e-04 - val_loss: 1.0340e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2261e-04 - val_loss: 8.5584e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.2104e-04 - val_loss: 8.9385e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2067e-04 - val_loss: 9.0915e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.1909e-04 - val_loss: 9.1852e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1885e-04 - val_loss: 8.6274e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1824e-04 - val_loss: 8.8354e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1775e-04 - val_loss: 8.7109e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.1583e-04 - val_loss: 8.2743e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.1434e-04 - val_loss: 8.1417e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1298e-04 - val_loss: 8.3548e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.1283e-04 - val_loss: 8.2489e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 499us/step - loss: 1.1174e-04 - val_loss: 8.2162e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1082e-04 - val_loss: 8.1596e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.1094e-04 - val_loss: 7.9867e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.0952e-04 - val_loss: 8.0885e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1050e-04 - val_loss: 8.0754e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.0917e-04 - val_loss: 8.3062e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.0812e-04 - val_loss: 8.3648e-05\n",
      "Validation RMSE:  0.009145911 \n",
      "\n",
      "Model took 1408.22 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 41s 1ms/step - loss: 0.0044 - val_loss: 0.0034\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 0.0010 - val_loss: 5.1263e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 6.0990e-04 - val_loss: 5.3425e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 4.9897e-04 - val_loss: 4.1092e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 4.4234e-04 - val_loss: 3.7777e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 4.0783e-04 - val_loss: 3.2220e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 3.8361e-04 - val_loss: 2.7923e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 3.6556e-04 - val_loss: 2.5677e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 3.4722e-04 - val_loss: 2.8123e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 3.3246e-04 - val_loss: 2.7730e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 3.2386e-04 - val_loss: 2.7281e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 3.1391e-04 - val_loss: 2.6277e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 3.0414e-04 - val_loss: 2.5578e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.9627e-04 - val_loss: 2.4170e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.8738e-04 - val_loss: 2.7039e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.7907e-04 - val_loss: 2.3606e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.7103e-04 - val_loss: 2.4023e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.6398e-04 - val_loss: 2.2087e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.5774e-04 - val_loss: 2.1265e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.5057e-04 - val_loss: 2.1337e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.4022e-04 - val_loss: 1.8089e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.3606e-04 - val_loss: 2.6171e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.2927e-04 - val_loss: 2.1166e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 2.2778e-04 - val_loss: 2.4421e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 2.2999e-04 - val_loss: 2.3691e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 2.1588e-04 - val_loss: 1.5505e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.0471e-04 - val_loss: 1.3567e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.9493e-04 - val_loss: 1.0749e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.8588e-04 - val_loss: 1.0598e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.7753e-04 - val_loss: 1.0040e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.7267e-04 - val_loss: 1.0806e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.6682e-04 - val_loss: 1.0873e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.6363e-04 - val_loss: 1.1051e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.5995e-04 - val_loss: 1.0804e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.5698e-04 - val_loss: 1.1122e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.5445e-04 - val_loss: 1.0685e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.5233e-04 - val_loss: 1.0791e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.5041e-04 - val_loss: 1.0388e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.4888e-04 - val_loss: 1.0632e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.4746e-04 - val_loss: 1.0468e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.4577e-04 - val_loss: 1.0433e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.4482e-04 - val_loss: 1.0044e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.3968e-04 - val_loss: 1.0522e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.3622e-04 - val_loss: 1.0575e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.3523e-04 - val_loss: 1.0550e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.3361e-04 - val_loss: 1.0358e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.3115e-04 - val_loss: 1.0253e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.2865e-04 - val_loss: 1.0022e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.2822e-04 - val_loss: 9.6174e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2687e-04 - val_loss: 9.7297e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.2597e-04 - val_loss: 9.3358e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.2501e-04 - val_loss: 9.1312e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.2360e-04 - val_loss: 9.0220e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.2259e-04 - val_loss: 8.9292e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.2209e-04 - val_loss: 8.9924e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.2097e-04 - val_loss: 8.8973e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.2027e-04 - val_loss: 8.9801e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.1940e-04 - val_loss: 8.7149e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.1881e-04 - val_loss: 8.8125e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.1816e-04 - val_loss: 8.7123e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1773e-04 - val_loss: 8.7253e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.1690e-04 - val_loss: 8.8028e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.1604e-04 - val_loss: 8.9308e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 500us/step - loss: 1.1538e-04 - val_loss: 9.0771e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1536e-04 - val_loss: 8.9351e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.1349e-04 - val_loss: 9.0426e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.1316e-04 - val_loss: 8.9942e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1214e-04 - val_loss: 8.9772e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1129e-04 - val_loss: 9.0049e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.1001e-04 - val_loss: 8.9832e-05\n",
      "Validation RMSE:  0.009477952 \n",
      "\n",
      "Model took 1414.02 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 43s 1ms/step - loss: 0.0068 - val_loss: 0.0041\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 0.0015 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 7.1569e-04 - val_loss: 7.2465e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 5.4108e-04 - val_loss: 3.9143e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 4.7005e-04 - val_loss: 3.2547e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 4.1926e-04 - val_loss: 3.7339e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.8562e-04 - val_loss: 3.3012e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 3.6351e-04 - val_loss: 2.5535e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.5079e-04 - val_loss: 2.4697e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.3478e-04 - val_loss: 2.4962e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 3.2248e-04 - val_loss: 2.4782e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 3.1105e-04 - val_loss: 2.6099e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.9797e-04 - val_loss: 2.4721e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.8420e-04 - val_loss: 2.4911e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.7298e-04 - val_loss: 2.5640e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.5752e-04 - val_loss: 2.5865e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.4948e-04 - val_loss: 2.7296e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.4559e-04 - val_loss: 2.0886e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.4678e-04 - val_loss: 1.5150e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.3160e-04 - val_loss: 1.5098e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.1949e-04 - val_loss: 1.5051e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.1095e-04 - val_loss: 1.4022e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.9974e-04 - val_loss: 1.1125e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.9282e-04 - val_loss: 1.1171e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.8774e-04 - val_loss: 1.1896e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.8233e-04 - val_loss: 1.1170e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.7680e-04 - val_loss: 1.0765e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.7331e-04 - val_loss: 1.2665e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.7015e-04 - val_loss: 1.2185e-04\n",
      "Epoch 30/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.6602e-04 - val_loss: 1.1997e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.6338e-04 - val_loss: 1.2685e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.6084e-04 - val_loss: 1.2319e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.5721e-04 - val_loss: 1.1665e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.5514e-04 - val_loss: 1.1010e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.5347e-04 - val_loss: 1.0895e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.5167e-04 - val_loss: 1.0807e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.5021e-04 - val_loss: 1.1012e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4850e-04 - val_loss: 1.1077e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.4715e-04 - val_loss: 1.0984e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.4596e-04 - val_loss: 1.1023e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.4432e-04 - val_loss: 1.0655e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4282e-04 - val_loss: 1.0856e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.4145e-04 - val_loss: 1.0466e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.4018e-04 - val_loss: 1.0281e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3890e-04 - val_loss: 1.0303e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.3730e-04 - val_loss: 9.9625e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.3517e-04 - val_loss: 1.0484e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3416e-04 - val_loss: 1.0156e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.3270e-04 - val_loss: 1.0056e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.3116e-04 - val_loss: 1.0302e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.3037e-04 - val_loss: 1.0201e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.2862e-04 - val_loss: 1.0206e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2701e-04 - val_loss: 9.7523e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.2587e-04 - val_loss: 9.5779e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2525e-04 - val_loss: 9.5075e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2428e-04 - val_loss: 9.3290e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2299e-04 - val_loss: 9.6511e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2197e-04 - val_loss: 9.1242e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2154e-04 - val_loss: 9.0471e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2103e-04 - val_loss: 9.0261e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.1948e-04 - val_loss: 8.9707e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1828e-04 - val_loss: 8.7251e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1753e-04 - val_loss: 8.8850e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1727e-04 - val_loss: 8.6659e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1629e-04 - val_loss: 8.5974e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.1548e-04 - val_loss: 8.5434e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.1537e-04 - val_loss: 8.6455e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.1433e-04 - val_loss: 8.7786e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.1287e-04 - val_loss: 8.6190e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1020e-04 - val_loss: 9.1103e-05\n",
      "Validation RMSE:  0.009544784 \n",
      "\n",
      "Model took 1441.91 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 42s 1ms/step - loss: 0.0087 - val_loss: 0.0056\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 0.0024 - val_loss: 0.0033\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 6.2058e-04 - val_loss: 8.5162e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 4.9114e-04 - val_loss: 6.3514e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 4.1276e-04 - val_loss: 4.3150e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 3.6520e-04 - val_loss: 3.3805e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 3.4000e-04 - val_loss: 2.7371e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 3.2215e-04 - val_loss: 2.6490e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 3.0594e-04 - val_loss: 2.6053e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 2.9637e-04 - val_loss: 2.7734e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.8998e-04 - val_loss: 2.5984e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 2.8009e-04 - val_loss: 2.4892e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 2.7311e-04 - val_loss: 2.5279e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 2.6703e-04 - val_loss: 2.5657e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.5971e-04 - val_loss: 2.5036e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.5227e-04 - val_loss: 2.7338e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 2.4657e-04 - val_loss: 2.5356e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 2.4075e-04 - val_loss: 2.4783e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.3305e-04 - val_loss: 2.5313e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 2.2893e-04 - val_loss: 2.3060e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.2307e-04 - val_loss: 2.3470e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.1553e-04 - val_loss: 2.2450e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 2.0905e-04 - val_loss: 2.4218e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.9743e-04 - val_loss: 2.1743e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.9343e-04 - val_loss: 1.8047e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.8859e-04 - val_loss: 1.6516e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.8171e-04 - val_loss: 1.7131e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.7623e-04 - val_loss: 1.7536e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.7064e-04 - val_loss: 1.5455e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.6464e-04 - val_loss: 1.3304e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.5946e-04 - val_loss: 1.2199e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.5678e-04 - val_loss: 1.1649e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.5351e-04 - val_loss: 1.1160e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.5015e-04 - val_loss: 1.1396e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.4708e-04 - val_loss: 1.1094e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.4509e-04 - val_loss: 1.1139e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.4308e-04 - val_loss: 1.1254e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.4013e-04 - val_loss: 1.1142e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.3846e-04 - val_loss: 1.0423e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.3661e-04 - val_loss: 1.1000e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.3472e-04 - val_loss: 1.1140e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.3275e-04 - val_loss: 1.1884e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.3149e-04 - val_loss: 1.2012e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.2962e-04 - val_loss: 1.2350e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.2811e-04 - val_loss: 1.2421e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.2704e-04 - val_loss: 1.1997e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.2571e-04 - val_loss: 1.2061e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2473e-04 - val_loss: 1.1450e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.2359e-04 - val_loss: 1.1626e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.2291e-04 - val_loss: 1.1435e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.2210e-04 - val_loss: 1.2012e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.2133e-04 - val_loss: 1.1764e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.2041e-04 - val_loss: 1.1841e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1926e-04 - val_loss: 1.1850e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1860e-04 - val_loss: 1.1882e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1769e-04 - val_loss: 1.2191e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.1645e-04 - val_loss: 1.1906e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1540e-04 - val_loss: 1.1076e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.1324e-04 - val_loss: 1.1013e-04\n",
      "Validation RMSE:  0.010494294 \n",
      "\n",
      "Model took 1224.89 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 43s 1ms/step - loss: 0.0055 - val_loss: 0.0040\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 8.0203e-04 - val_loss: 6.4812e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 6.0031e-04 - val_loss: 4.3547e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 5.1589e-04 - val_loss: 3.8107e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 4.5353e-04 - val_loss: 3.4009e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 4.1062e-04 - val_loss: 3.3002e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 3.8369e-04 - val_loss: 2.7224e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 3.6131e-04 - val_loss: 2.7478e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 3.4646e-04 - val_loss: 2.3156e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 3.3096e-04 - val_loss: 2.1245e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 3.1646e-04 - val_loss: 2.2428e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 3.0455e-04 - val_loss: 2.2974e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 2.8767e-04 - val_loss: 2.3459e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.7331e-04 - val_loss: 2.3023e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 2.6220e-04 - val_loss: 2.2477e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.5378e-04 - val_loss: 1.8606e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.5306e-04 - val_loss: 2.3136e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.4999e-04 - val_loss: 1.9321e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.3851e-04 - val_loss: 1.7219e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 2.2448e-04 - val_loss: 1.7980e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.1057e-04 - val_loss: 1.9085e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 2.0264e-04 - val_loss: 1.4155e-04\n",
      "Epoch 24/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.9115e-04 - val_loss: 1.2377e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.8244e-04 - val_loss: 1.1608e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.7390e-04 - val_loss: 1.1440e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.6551e-04 - val_loss: 1.1213e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.6165e-04 - val_loss: 1.1285e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.5894e-04 - val_loss: 1.1840e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.5652e-04 - val_loss: 1.1357e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.5450e-04 - val_loss: 1.1275e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.5244e-04 - val_loss: 1.0441e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.5024e-04 - val_loss: 1.0130e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.4909e-04 - val_loss: 9.9308e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.4728e-04 - val_loss: 9.8434e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.4565e-04 - val_loss: 9.4580e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.4387e-04 - val_loss: 9.6535e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.4228e-04 - val_loss: 9.3416e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.4059e-04 - val_loss: 8.9623e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 501us/step - loss: 1.3923e-04 - val_loss: 8.8893e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.3719e-04 - val_loss: 8.8187e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.3605e-04 - val_loss: 8.6617e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.3490e-04 - val_loss: 8.5033e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.3418e-04 - val_loss: 8.5514e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.3276e-04 - val_loss: 8.9205e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.3065e-04 - val_loss: 8.7866e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.2916e-04 - val_loss: 8.4265e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.2812e-04 - val_loss: 8.2090e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.2688e-04 - val_loss: 7.8569e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.2558e-04 - val_loss: 7.7531e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.2442e-04 - val_loss: 7.6983e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.2316e-04 - val_loss: 7.6685e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.2228e-04 - val_loss: 7.5632e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.2125e-04 - val_loss: 7.8821e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.2036e-04 - val_loss: 7.7130e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.1946e-04 - val_loss: 7.5748e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.1882e-04 - val_loss: 7.7170e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.1800e-04 - val_loss: 7.4103e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.1694e-04 - val_loss: 7.5053e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 502us/step - loss: 1.1636e-04 - val_loss: 7.6707e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 503us/step - loss: 1.1501e-04 - val_loss: 7.7197e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.1455e-04 - val_loss: 7.8021e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1334e-04 - val_loss: 7.7476e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.1290e-04 - val_loss: 8.1446e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.1214e-04 - val_loss: 8.7205e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.1106e-04 - val_loss: 8.8735e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.1039e-04 - val_loss: 8.5576e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 504us/step - loss: 1.0975e-04 - val_loss: 8.8257e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.0928e-04 - val_loss: 8.9993e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.0867e-04 - val_loss: 8.9055e-05\n",
      "Validation RMSE:  0.009436912 \n",
      "\n",
      "Model took 1422.08 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 43s 1ms/step - loss: 0.0059 - val_loss: 0.0046\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 7.5041e-04 - val_loss: 8.9252e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 5.5239e-04 - val_loss: 4.6095e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 4.7361e-04 - val_loss: 4.0589e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 4.2466e-04 - val_loss: 3.3289e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 3.8878e-04 - val_loss: 2.9429e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 3.6352e-04 - val_loss: 3.0686e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 3.4635e-04 - val_loss: 2.8259e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.3233e-04 - val_loss: 2.7572e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.2069e-04 - val_loss: 2.6330e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 3.0977e-04 - val_loss: 2.4691e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.9817e-04 - val_loss: 2.3963e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.8780e-04 - val_loss: 2.2822e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.7683e-04 - val_loss: 2.3015e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.6702e-04 - val_loss: 2.1175e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.5926e-04 - val_loss: 2.0669e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.4838e-04 - val_loss: 2.1698e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.4246e-04 - val_loss: 2.2907e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.3999e-04 - val_loss: 2.1782e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.3539e-04 - val_loss: 1.8374e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.3181e-04 - val_loss: 1.4108e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.2023e-04 - val_loss: 1.6660e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.0840e-04 - val_loss: 1.6491e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.9924e-04 - val_loss: 1.5476e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.9252e-04 - val_loss: 1.2102e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.8547e-04 - val_loss: 1.2361e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.7934e-04 - val_loss: 1.2345e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.7387e-04 - val_loss: 1.1815e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.7074e-04 - val_loss: 1.2001e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.6831e-04 - val_loss: 1.2078e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.6394e-04 - val_loss: 1.0400e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.6183e-04 - val_loss: 1.1084e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.5903e-04 - val_loss: 1.0712e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.5621e-04 - val_loss: 1.0639e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.5447e-04 - val_loss: 1.0497e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.5197e-04 - val_loss: 1.0585e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.5020e-04 - val_loss: 1.0572e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.4826e-04 - val_loss: 1.0240e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.4652e-04 - val_loss: 1.0092e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.4441e-04 - val_loss: 1.0144e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.4253e-04 - val_loss: 9.8584e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.4038e-04 - val_loss: 9.8139e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.3829e-04 - val_loss: 9.6677e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.3696e-04 - val_loss: 9.5704e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.3500e-04 - val_loss: 9.8040e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.3303e-04 - val_loss: 9.6125e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.3291e-04 - val_loss: 1.0484e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.3129e-04 - val_loss: 1.0279e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2903e-04 - val_loss: 9.2588e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.2680e-04 - val_loss: 9.5301e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.2482e-04 - val_loss: 1.0905e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.2442e-04 - val_loss: 9.7789e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.2311e-04 - val_loss: 9.9753e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2204e-04 - val_loss: 9.0959e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.2033e-04 - val_loss: 9.2352e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.1846e-04 - val_loss: 8.9400e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1798e-04 - val_loss: 8.8185e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1608e-04 - val_loss: 9.1885e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.1565e-04 - val_loss: 9.0717e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1505e-04 - val_loss: 9.0612e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1432e-04 - val_loss: 9.3849e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.1303e-04 - val_loss: 9.4413e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1203e-04 - val_loss: 9.4448e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.1053e-04 - val_loss: 8.7723e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.0958e-04 - val_loss: 8.8528e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.0958e-04 - val_loss: 9.0384e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.0848e-04 - val_loss: 8.4539e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.0788e-04 - val_loss: 8.6392e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.0700e-04 - val_loss: 8.5239e-05\n",
      "Validation RMSE:  0.009232521 \n",
      "\n",
      "Model took 1451.90 seconds to train\n",
      "23 \t8     \t0.0103367 \t0.00314314 \t0.00856956\t0.0196396 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 44s 1ms/step - loss: 0.0056 - val_loss: 0.0046\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 7.8459e-04 - val_loss: 0.0011\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 5.5612e-04 - val_loss: 5.0521e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 4.7343e-04 - val_loss: 3.2282e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 4.2186e-04 - val_loss: 2.9188e-04\n",
      "Epoch 7/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 510us/step - loss: 3.8283e-04 - val_loss: 2.9315e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 3.5910e-04 - val_loss: 2.6263e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.3998e-04 - val_loss: 2.3474e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.2865e-04 - val_loss: 2.2410e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 3.1618e-04 - val_loss: 2.3413e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 3.0652e-04 - val_loss: 2.5524e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.9695e-04 - val_loss: 2.5873e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 2.8933e-04 - val_loss: 2.7077e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.8262e-04 - val_loss: 2.8113e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.7490e-04 - val_loss: 2.7922e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.6700e-04 - val_loss: 2.8575e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.6087e-04 - val_loss: 2.7996e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.4937e-04 - val_loss: 2.7621e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.4576e-04 - val_loss: 2.3332e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 2.3280e-04 - val_loss: 2.7531e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 2.2477e-04 - val_loss: 1.7099e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.1525e-04 - val_loss: 2.1174e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.0113e-04 - val_loss: 1.3975e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.8768e-04 - val_loss: 1.1771e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.8086e-04 - val_loss: 1.0822e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.7461e-04 - val_loss: 1.0501e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.7017e-04 - val_loss: 1.0722e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.6723e-04 - val_loss: 1.0467e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.6390e-04 - val_loss: 1.0245e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.6078e-04 - val_loss: 9.9882e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.5861e-04 - val_loss: 9.8870e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.5460e-04 - val_loss: 9.5571e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.5285e-04 - val_loss: 9.6632e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.5069e-04 - val_loss: 9.5133e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.4927e-04 - val_loss: 9.3340e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4795e-04 - val_loss: 9.4089e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.4628e-04 - val_loss: 9.4817e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.4441e-04 - val_loss: 8.9538e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4341e-04 - val_loss: 8.9196e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.4174e-04 - val_loss: 8.8980e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.4058e-04 - val_loss: 8.7777e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3945e-04 - val_loss: 8.7495e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3850e-04 - val_loss: 8.6789e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.3619e-04 - val_loss: 8.6174e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.3613e-04 - val_loss: 8.4575e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.3483e-04 - val_loss: 8.3818e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.3303e-04 - val_loss: 8.3555e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.3244e-04 - val_loss: 8.7252e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.3089e-04 - val_loss: 8.3901e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.3043e-04 - val_loss: 8.9509e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2909e-04 - val_loss: 8.8291e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.2763e-04 - val_loss: 8.7066e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2718e-04 - val_loss: 8.4824e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2620e-04 - val_loss: 8.2475e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2527e-04 - val_loss: 8.3400e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2423e-04 - val_loss: 8.2815e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2363e-04 - val_loss: 8.3869e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2293e-04 - val_loss: 8.6551e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2203e-04 - val_loss: 8.3967e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.2154e-04 - val_loss: 8.3296e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.2076e-04 - val_loss: 8.0878e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2009e-04 - val_loss: 7.9998e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.1842e-04 - val_loss: 7.7841e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1839e-04 - val_loss: 8.0435e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1786e-04 - val_loss: 8.0609e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.1754e-04 - val_loss: 8.1387e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1594e-04 - val_loss: 8.5951e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1681e-04 - val_loss: 8.0320e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1527e-04 - val_loss: 8.5621e-05\n",
      "Validation RMSE:  0.009253181 \n",
      "\n",
      "Model took 1443.19 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 44s 1ms/step - loss: 0.0050 - val_loss: 0.0036\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 0.0012 - val_loss: 9.3348e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 6.8761e-04 - val_loss: 6.3366e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 5.5547e-04 - val_loss: 3.9043e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 4.8738e-04 - val_loss: 3.6363e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 4.3777e-04 - val_loss: 3.3370e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 4.0489e-04 - val_loss: 3.0484e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.8303e-04 - val_loss: 2.9223e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 3.6362e-04 - val_loss: 2.9923e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 3.4684e-04 - val_loss: 2.8516e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 3.3261e-04 - val_loss: 2.7578e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.2070e-04 - val_loss: 2.5860e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.0962e-04 - val_loss: 2.6114e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 3.0092e-04 - val_loss: 2.6476e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.9135e-04 - val_loss: 2.6386e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.8224e-04 - val_loss: 2.5897e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.7348e-04 - val_loss: 2.6930e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.6332e-04 - val_loss: 2.7370e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.5825e-04 - val_loss: 2.6461e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.5218e-04 - val_loss: 2.6217e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.4544e-04 - val_loss: 2.9056e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.3589e-04 - val_loss: 3.1791e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.2570e-04 - val_loss: 3.4725e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.1879e-04 - val_loss: 2.0405e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 2.1718e-04 - val_loss: 2.2822e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.1107e-04 - val_loss: 1.9651e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.0213e-04 - val_loss: 2.2119e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.9141e-04 - val_loss: 1.9621e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.8127e-04 - val_loss: 1.9065e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.7774e-04 - val_loss: 1.8776e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.7243e-04 - val_loss: 1.6849e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.6795e-04 - val_loss: 1.4741e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.6388e-04 - val_loss: 1.4188e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.6014e-04 - val_loss: 1.3901e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.5827e-04 - val_loss: 1.3295e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.5498e-04 - val_loss: 1.2266e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.5206e-04 - val_loss: 1.1803e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.4851e-04 - val_loss: 1.1369e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.4530e-04 - val_loss: 1.0982e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.4331e-04 - val_loss: 1.0634e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.4024e-04 - val_loss: 1.0775e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3892e-04 - val_loss: 1.0413e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.3713e-04 - val_loss: 1.0400e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.3537e-04 - val_loss: 1.0298e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.3401e-04 - val_loss: 1.0145e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.3212e-04 - val_loss: 9.9752e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.3110e-04 - val_loss: 9.9188e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2902e-04 - val_loss: 1.0078e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2723e-04 - val_loss: 1.0573e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2694e-04 - val_loss: 1.0946e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.2507e-04 - val_loss: 1.2431e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2400e-04 - val_loss: 1.1876e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2333e-04 - val_loss: 1.1599e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.2286e-04 - val_loss: 1.1271e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2192e-04 - val_loss: 1.1385e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2083e-04 - val_loss: 1.2287e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2009e-04 - val_loss: 1.2278e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1905e-04 - val_loss: 1.2114e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1854e-04 - val_loss: 1.1513e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1716e-04 - val_loss: 1.1976e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1659e-04 - val_loss: 1.2129e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1571e-04 - val_loss: 1.1506e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.1551e-04 - val_loss: 1.1858e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1436e-04 - val_loss: 1.0862e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.1444e-04 - val_loss: 1.0655e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.1331e-04 - val_loss: 9.7355e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.1133e-04 - val_loss: 9.7713e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.1273e-04 - val_loss: 1.0501e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1182e-04 - val_loss: 1.0256e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.1111e-04 - val_loss: 9.3919e-05\n",
      "Validation RMSE:  0.009691181 \n",
      "\n",
      "Model took 1460.43 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 100\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 21 20 3 8\n",
      "1st Window Start: 43\n",
      "2nd Window Start: 506\n",
      "3rd Window Start: 1016\n",
      "4th Window Start: 1133\n",
      "Activation Function is: tanh\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 1, 1016, 1017, 1018, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Lags length:\n",
      " 63\n",
      "Length Used:\n",
      " 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48982, 64)\n",
      "Training and test data length 39185 9797\n",
      "trainX.shape[0]:  39185\n",
      "window_length:  21\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39185, 63)\n",
      "Train X shape after np.reshape (39185, 21, 3)\n",
      "Test X shape after np.reshape (9797, 21, 3)\n",
      "Train Y Shape (39185,)\n",
      "Test Y Shape (9797,)\n",
      "Train on 39185 samples, validate on 9797 samples\n",
      "Epoch 1/70\n",
      "39185/39185 [==============================] - 39s 997us/step - loss: 0.0050 - val_loss: 9.2977e-04\n",
      "Epoch 2/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 0.0010 - val_loss: 6.6262e-04\n",
      "Epoch 3/70\n",
      "39185/39185 [==============================] - 15s 373us/step - loss: 8.3420e-04 - val_loss: 5.4350e-04\n",
      "Epoch 4/70\n",
      "39185/39185 [==============================] - 15s 373us/step - loss: 7.4006e-04 - val_loss: 5.2822e-04\n",
      "Epoch 5/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 6.7057e-04 - val_loss: 4.2471e-04\n",
      "Epoch 6/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 6.3246e-04 - val_loss: 3.8337e-04\n",
      "Epoch 7/70\n",
      "39185/39185 [==============================] - 15s 370us/step - loss: 6.0216e-04 - val_loss: 3.5602e-04\n",
      "Epoch 8/70\n",
      "39185/39185 [==============================] - 15s 371us/step - loss: 5.7633e-04 - val_loss: 3.2326e-04\n",
      "Epoch 9/70\n",
      "39185/39185 [==============================] - 15s 373us/step - loss: 5.5654e-04 - val_loss: 3.0785e-04\n",
      "Epoch 10/70\n",
      "39185/39185 [==============================] - 15s 374us/step - loss: 5.4038e-04 - val_loss: 3.0026e-04\n",
      "Epoch 11/70\n",
      "39185/39185 [==============================] - 15s 371us/step - loss: 5.2313e-04 - val_loss: 2.9578e-04\n",
      "Epoch 12/70\n",
      "39185/39185 [==============================] - 15s 374us/step - loss: 5.0838e-04 - val_loss: 3.9826e-04\n",
      "Epoch 13/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 4.9339e-04 - val_loss: 6.5860e-04\n",
      "Epoch 14/70\n",
      "39185/39185 [==============================] - 15s 373us/step - loss: 4.8796e-04 - val_loss: 3.9042e-04\n",
      "Epoch 15/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 4.7952e-04 - val_loss: 4.5817e-04\n",
      "Epoch 16/70\n",
      "39185/39185 [==============================] - 15s 373us/step - loss: 4.7123e-04 - val_loss: 4.1658e-04\n",
      "Epoch 17/70\n",
      "39185/39185 [==============================] - 15s 374us/step - loss: 4.6047e-04 - val_loss: 9.9588e-04\n",
      "Epoch 18/70\n",
      "39185/39185 [==============================] - 15s 374us/step - loss: 4.5747e-04 - val_loss: 3.4457e-04\n",
      "Epoch 19/70\n",
      "39185/39185 [==============================] - 15s 374us/step - loss: 4.4921e-04 - val_loss: 3.5427e-04\n",
      "Epoch 20/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 4.4200e-04 - val_loss: 3.3391e-04\n",
      "Epoch 21/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 4.3547e-04 - val_loss: 3.3815e-04\n",
      "Epoch 22/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 4.2760e-04 - val_loss: 3.5277e-04\n",
      "Epoch 23/70\n",
      "39185/39185 [==============================] - 15s 374us/step - loss: 4.2306e-04 - val_loss: 3.7406e-04 loss: 4.2568e\n",
      "Epoch 24/70\n",
      "39185/39185 [==============================] - 15s 374us/step - loss: 4.1475e-04 - val_loss: 3.6493e-04\n",
      "Epoch 25/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 4.0990e-04 - val_loss: 3.6993e-04\n",
      "Epoch 26/70\n",
      "39185/39185 [==============================] - 15s 373us/step - loss: 4.0433e-04 - val_loss: 3.8321e-04\n",
      "Epoch 27/70\n",
      "39185/39185 [==============================] - 15s 371us/step - loss: 3.9939e-04 - val_loss: 3.9293e-04\n",
      "Epoch 28/70\n",
      "39185/39185 [==============================] - 15s 373us/step - loss: 3.9401e-04 - val_loss: 3.9844e-04\n",
      "Epoch 29/70\n",
      "39185/39185 [==============================] - 15s 373us/step - loss: 3.8872e-04 - val_loss: 4.0134e-04\n",
      "Epoch 30/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 3.8334e-04 - val_loss: 4.0146e-04\n",
      "Epoch 31/70\n",
      "39185/39185 [==============================] - 15s 372us/step - loss: 3.7745e-04 - val_loss: 4.4170e-04\n",
      "Validation RMSE:  0.021016672 \n",
      "\n",
      "Model took 489.73 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 44s 1ms/step - loss: 0.0064 - val_loss: 0.0051\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 0.0018 - val_loss: 0.0020\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 7.5731e-04 - val_loss: 6.8221e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 5.4499e-04 - val_loss: 3.8817e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 4.5818e-04 - val_loss: 3.5309e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 4.1166e-04 - val_loss: 3.0971e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 3.8298e-04 - val_loss: 2.7267e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.5868e-04 - val_loss: 2.4582e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 3.4576e-04 - val_loss: 2.2556e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 3.3125e-04 - val_loss: 2.2937e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 3.2010e-04 - val_loss: 2.2505e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 3.1270e-04 - val_loss: 2.2274e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 3.0477e-04 - val_loss: 2.1353e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 2.9571e-04 - val_loss: 2.0595e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.8499e-04 - val_loss: 2.1253e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.7465e-04 - val_loss: 2.0623e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.6649e-04 - val_loss: 2.0126e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.5830e-04 - val_loss: 1.9893e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.4981e-04 - val_loss: 2.0601e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.4608e-04 - val_loss: 2.0890e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.4042e-04 - val_loss: 2.0514e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.3185e-04 - val_loss: 1.8626e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 2.1649e-04 - val_loss: 1.8007e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.0975e-04 - val_loss: 1.7105e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.0012e-04 - val_loss: 1.3286e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.9228e-04 - val_loss: 1.2228e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.8549e-04 - val_loss: 1.1498e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.7864e-04 - val_loss: 1.0831e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.7281e-04 - val_loss: 1.0238e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.7005e-04 - val_loss: 1.0226e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.6740e-04 - val_loss: 1.0363e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.6385e-04 - val_loss: 1.0488e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.5968e-04 - val_loss: 1.0857e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.5670e-04 - val_loss: 1.1593e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.5343e-04 - val_loss: 1.1387e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.5088e-04 - val_loss: 1.1665e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.4984e-04 - val_loss: 1.0385e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.4814e-04 - val_loss: 1.0052e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.4591e-04 - val_loss: 1.0219e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4401e-04 - val_loss: 1.0082e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.4187e-04 - val_loss: 9.9171e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.4028e-04 - val_loss: 9.7807e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.3918e-04 - val_loss: 9.7764e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.3692e-04 - val_loss: 9.8133e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.3579e-04 - val_loss: 1.0504e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.3504e-04 - val_loss: 1.0263e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.3424e-04 - val_loss: 1.0422e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.3223e-04 - val_loss: 1.0196e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.3158e-04 - val_loss: 1.0309e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.3069e-04 - val_loss: 1.0355e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2942e-04 - val_loss: 1.0138e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2617e-04 - val_loss: 9.6141e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2472e-04 - val_loss: 9.4672e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2276e-04 - val_loss: 9.3777e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2245e-04 - val_loss: 9.8121e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2113e-04 - val_loss: 9.5762e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2104e-04 - val_loss: 9.3348e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.1998e-04 - val_loss: 9.2928e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1876e-04 - val_loss: 9.7612e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.1726e-04 - val_loss: 9.7752e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1584e-04 - val_loss: 9.6414e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1534e-04 - val_loss: 9.5886e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1466e-04 - val_loss: 9.7748e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.1397e-04 - val_loss: 9.9088e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1344e-04 - val_loss: 9.8685e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.1237e-04 - val_loss: 9.9326e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1175e-04 - val_loss: 9.6002e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1108e-04 - val_loss: 9.7727e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1094e-04 - val_loss: 9.6923e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1018e-04 - val_loss: 9.7717e-05\n",
      "Validation RMSE:  0.009885204 \n",
      "\n",
      "Model took 1440.73 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 45s 1ms/step - loss: 0.0048 - val_loss: 0.0033\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 6.5050e-04 - val_loss: 6.3833e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 5.2344e-04 - val_loss: 4.0997e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 4.5102e-04 - val_loss: 4.4321e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 4.0903e-04 - val_loss: 3.6509e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 3.8212e-04 - val_loss: 3.1196e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.6200e-04 - val_loss: 2.8751e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.5368e-04 - val_loss: 2.8407e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 3.4213e-04 - val_loss: 2.6825e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.3142e-04 - val_loss: 2.8574e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.2179e-04 - val_loss: 2.6386e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.0929e-04 - val_loss: 2.4422e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.9738e-04 - val_loss: 2.2181e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.8894e-04 - val_loss: 2.1319e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.8106e-04 - val_loss: 1.9637e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.7107e-04 - val_loss: 2.0359e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.5285e-04 - val_loss: 2.0406e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.4549e-04 - val_loss: 1.7021e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.3452e-04 - val_loss: 2.2161e-04\n",
      "Epoch 21/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 509us/step - loss: 2.2430e-04 - val_loss: 2.2884e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.1114e-04 - val_loss: 1.8700e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.0199e-04 - val_loss: 1.6469e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.9525e-04 - val_loss: 1.3763e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.8874e-04 - val_loss: 1.2859e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.8313e-04 - val_loss: 1.1800e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.7893e-04 - val_loss: 1.0965e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.7484e-04 - val_loss: 1.0810e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.7147e-04 - val_loss: 1.0875e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.6797e-04 - val_loss: 1.0479e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.6532e-04 - val_loss: 1.1091e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.6273e-04 - val_loss: 1.0185e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.6013e-04 - val_loss: 9.8122e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.5758e-04 - val_loss: 9.6315e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.5413e-04 - val_loss: 9.8820e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.5163e-04 - val_loss: 9.9352e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4992e-04 - val_loss: 9.0742e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.4838e-04 - val_loss: 9.9766e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4602e-04 - val_loss: 9.5882e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.4382e-04 - val_loss: 9.6064e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.4244e-04 - val_loss: 9.7053e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4124e-04 - val_loss: 1.0278e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.3935e-04 - val_loss: 9.9367e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.3823e-04 - val_loss: 1.0330e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.3669e-04 - val_loss: 1.0102e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.3423e-04 - val_loss: 1.0620e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.3319e-04 - val_loss: 1.0288e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.3253e-04 - val_loss: 1.0308e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.3009e-04 - val_loss: 1.0447e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2769e-04 - val_loss: 1.1427e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2444e-04 - val_loss: 1.0463e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2451e-04 - val_loss: 1.0833e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2315e-04 - val_loss: 1.0852e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2358e-04 - val_loss: 1.0915e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.2213e-04 - val_loss: 1.1205e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2233e-04 - val_loss: 1.1149e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.2154e-04 - val_loss: 1.1097e-04\n",
      "Validation RMSE:  0.010534176 \n",
      "\n",
      "Model took 1181.26 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 46s 1ms/step - loss: 0.0065 - val_loss: 0.0036\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 6.7987e-04 - val_loss: 5.0634e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 5.5071e-04 - val_loss: 4.1897e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 4.7954e-04 - val_loss: 3.9306e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 4.2913e-04 - val_loss: 3.4029e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 3.9506e-04 - val_loss: 3.0246e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 3.7066e-04 - val_loss: 3.0019e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 3.5257e-04 - val_loss: 3.2376e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 3.3892e-04 - val_loss: 3.0114e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.2580e-04 - val_loss: 2.8465e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 3.1363e-04 - val_loss: 2.8104e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 3.0282e-04 - val_loss: 2.7328e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 2.8818e-04 - val_loss: 2.8753e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 2.7739e-04 - val_loss: 2.9175e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 2.6770e-04 - val_loss: 2.6067e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 2.5896e-04 - val_loss: 2.5200e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.4336e-04 - val_loss: 2.2545e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.3000e-04 - val_loss: 1.6095e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.2019e-04 - val_loss: 1.3793e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.0833e-04 - val_loss: 1.3529e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.0014e-04 - val_loss: 1.2724e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.9443e-04 - val_loss: 1.0976e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.8696e-04 - val_loss: 1.0856e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.8239e-04 - val_loss: 1.0479e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.7780e-04 - val_loss: 1.0538e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.7410e-04 - val_loss: 1.0328e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.7024e-04 - val_loss: 1.0569e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.6840e-04 - val_loss: 1.0021e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.6490e-04 - val_loss: 9.7290e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.6079e-04 - val_loss: 9.6650e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.5813e-04 - val_loss: 9.7250e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.5499e-04 - val_loss: 9.5866e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.5276e-04 - val_loss: 9.4043e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.5106e-04 - val_loss: 9.2719e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.4932e-04 - val_loss: 9.7303e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.4717e-04 - val_loss: 9.7375e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.4514e-04 - val_loss: 9.8246e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.4344e-04 - val_loss: 9.9899e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.4047e-04 - val_loss: 9.7609e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.3934e-04 - val_loss: 9.5215e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.3789e-04 - val_loss: 9.5229e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.3654e-04 - val_loss: 9.8442e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3512e-04 - val_loss: 9.9989e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.3405e-04 - val_loss: 1.0030e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3266e-04 - val_loss: 1.0109e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.3125e-04 - val_loss: 9.9835e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.3029e-04 - val_loss: 9.9485e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.2890e-04 - val_loss: 9.8353e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 505us/step - loss: 1.2768e-04 - val_loss: 9.4871e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.2615e-04 - val_loss: 9.4184e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.2416e-04 - val_loss: 9.2587e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 506us/step - loss: 1.2271e-04 - val_loss: 9.1072e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.2143e-04 - val_loss: 9.1552e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1993e-04 - val_loss: 9.0152e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1908e-04 - val_loss: 9.1420e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.1793e-04 - val_loss: 9.0542e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1707e-04 - val_loss: 9.2918e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1644e-04 - val_loss: 9.7254e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1564e-04 - val_loss: 9.3759e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.1520e-04 - val_loss: 1.0220e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.1427e-04 - val_loss: 1.0422e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.1384e-04 - val_loss: 1.0868e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1271e-04 - val_loss: 1.0310e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1213e-04 - val_loss: 1.0674e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.1156e-04 - val_loss: 9.4663e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1047e-04 - val_loss: 9.2985e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.1013e-04 - val_loss: 9.0218e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.0906e-04 - val_loss: 8.1763e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.0834e-04 - val_loss: 8.1609e-05\n",
      "Validation RMSE:  0.00903377 \n",
      "\n",
      "Model took 1438.10 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 46s 1ms/step - loss: 0.0046 - val_loss: 0.0032\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 6.5350e-04 - val_loss: 4.4343e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 5.3246e-04 - val_loss: 4.2405e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 4.7499e-04 - val_loss: 3.5666e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 4.1508e-04 - val_loss: 3.8968e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 3.7789e-04 - val_loss: 3.6852e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 3.5279e-04 - val_loss: 3.9391e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 3.3458e-04 - val_loss: 3.5385e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 3.2090e-04 - val_loss: 3.3395e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.0777e-04 - val_loss: 3.2014e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.9639e-04 - val_loss: 3.0409e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.8402e-04 - val_loss: 3.2211e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.7141e-04 - val_loss: 2.9795e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.5823e-04 - val_loss: 2.9337e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.5024e-04 - val_loss: 2.6950e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.4314e-04 - val_loss: 2.6626e-04\n",
      "Epoch 18/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.3576e-04 - val_loss: 2.5520e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.2415e-04 - val_loss: 2.5209e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.1547e-04 - val_loss: 2.0577e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.1194e-04 - val_loss: 1.8365e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.0285e-04 - val_loss: 1.7569e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.9369e-04 - val_loss: 1.6923e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.8716e-04 - val_loss: 1.5696e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.8015e-04 - val_loss: 1.2426e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.7379e-04 - val_loss: 1.1202e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.6711e-04 - val_loss: 1.1676e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.6382e-04 - val_loss: 1.1811e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.5880e-04 - val_loss: 1.1968e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.5592e-04 - val_loss: 1.1968e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.5314e-04 - val_loss: 1.1665e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.5085e-04 - val_loss: 1.2009e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.4819e-04 - val_loss: 1.2048e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.4688e-04 - val_loss: 1.1741e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.4455e-04 - val_loss: 1.1582e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.4294e-04 - val_loss: 1.1464e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.4081e-04 - val_loss: 1.0719e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3844e-04 - val_loss: 1.1011e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3672e-04 - val_loss: 1.0824e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3540e-04 - val_loss: 1.0982e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3352e-04 - val_loss: 1.0458e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3213e-04 - val_loss: 1.0884e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.3144e-04 - val_loss: 1.0679e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.3015e-04 - val_loss: 1.0391e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2888e-04 - val_loss: 1.0858e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2703e-04 - val_loss: 1.1069e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2522e-04 - val_loss: 1.0960e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.2353e-04 - val_loss: 1.0950e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.2223e-04 - val_loss: 1.2752e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2177e-04 - val_loss: 1.1912e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1982e-04 - val_loss: 1.2379e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1819e-04 - val_loss: 1.1989e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1817e-04 - val_loss: 1.1388e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1802e-04 - val_loss: 1.1707e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1666e-04 - val_loss: 1.1757e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.1593e-04 - val_loss: 1.1911e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.1475e-04 - val_loss: 1.1641e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1391e-04 - val_loss: 1.2037e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1289e-04 - val_loss: 1.1934e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.1117e-04 - val_loss: 1.1023e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1150e-04 - val_loss: 1.1734e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.0943e-04 - val_loss: 1.2560e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.0904e-04 - val_loss: 1.2570e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.0806e-04 - val_loss: 1.1977e-04\n",
      "Validation RMSE:  0.010944105 \n",
      "\n",
      "Model took 1344.43 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 46s 1ms/step - loss: 0.0056 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 6.6989e-04 - val_loss: 5.4027e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 5.4169e-04 - val_loss: 3.7327e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 4.6642e-04 - val_loss: 3.8732e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 4.1805e-04 - val_loss: 3.4881e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 3.8590e-04 - val_loss: 3.0313e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.6533e-04 - val_loss: 2.9960e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 3.4836e-04 - val_loss: 3.1711e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 3.3344e-04 - val_loss: 2.8220e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 3.2191e-04 - val_loss: 2.6682e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 3.1053e-04 - val_loss: 2.5344e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 2.9948e-04 - val_loss: 2.4721e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.8982e-04 - val_loss: 2.4493e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 2.8098e-04 - val_loss: 2.5742e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.7290e-04 - val_loss: 2.6640e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 2.6101e-04 - val_loss: 2.8250e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.5185e-04 - val_loss: 2.6986e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.4030e-04 - val_loss: 2.7445e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.2771e-04 - val_loss: 1.7891e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.1802e-04 - val_loss: 2.2425e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.1405e-04 - val_loss: 1.8068e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.0544e-04 - val_loss: 1.3900e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.9668e-04 - val_loss: 1.2922e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.8886e-04 - val_loss: 1.3215e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.8160e-04 - val_loss: 1.6860e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.7648e-04 - val_loss: 1.3414e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.7083e-04 - val_loss: 1.3219e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.6667e-04 - val_loss: 1.2332e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.6181e-04 - val_loss: 1.2208e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.5868e-04 - val_loss: 1.1970e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.5569e-04 - val_loss: 1.2096e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.5321e-04 - val_loss: 1.2298e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.5110e-04 - val_loss: 1.2171e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.4837e-04 - val_loss: 1.2129e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.4692e-04 - val_loss: 1.2248e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.4289e-04 - val_loss: 1.1115e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.4243e-04 - val_loss: 1.1122e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.4153e-04 - val_loss: 1.1543e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3835e-04 - val_loss: 1.1285e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3772e-04 - val_loss: 1.1340e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.3609e-04 - val_loss: 1.1321e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3411e-04 - val_loss: 1.0771e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3117e-04 - val_loss: 1.1517e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3143e-04 - val_loss: 1.0496e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2995e-04 - val_loss: 1.0473e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2895e-04 - val_loss: 1.0408e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2657e-04 - val_loss: 1.1359e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2692e-04 - val_loss: 1.0942e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2595e-04 - val_loss: 1.0919e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2512e-04 - val_loss: 1.1151e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.2401e-04 - val_loss: 1.1552e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.2349e-04 - val_loss: 1.1548e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2291e-04 - val_loss: 1.1649e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2180e-04 - val_loss: 1.1428e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2086e-04 - val_loss: 1.1118e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.1974e-04 - val_loss: 1.0722e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.1900e-04 - val_loss: 1.0083e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1614e-04 - val_loss: 1.0045e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1549e-04 - val_loss: 1.0025e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1422e-04 - val_loss: 9.5923e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.1308e-04 - val_loss: 1.0002e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1335e-04 - val_loss: 1.1464e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1173e-04 - val_loss: 1.1084e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1121e-04 - val_loss: 9.7828e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.1073e-04 - val_loss: 1.0336e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.0979e-04 - val_loss: 1.0542e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.0947e-04 - val_loss: 1.0266e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.0798e-04 - val_loss: 1.0344e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.0853e-04 - val_loss: 9.0030e-05\n",
      "Validation RMSE:  0.009488422 \n",
      "\n",
      "Model took 1470.48 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 47s 1ms/step - loss: 0.0055 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 6.2904e-04 - val_loss: 7.7118e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 5.1156e-04 - val_loss: 4.2900e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 4.4719e-04 - val_loss: 4.2726e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 4.0903e-04 - val_loss: 3.3411e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.7984e-04 - val_loss: 3.2232e-04\n",
      "Epoch 8/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 509us/step - loss: 3.5597e-04 - val_loss: 2.9226e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 3.3899e-04 - val_loss: 2.5004e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 3.2190e-04 - val_loss: 2.6138e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.1142e-04 - val_loss: 2.5731e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.0064e-04 - val_loss: 2.4949e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.9293e-04 - val_loss: 2.5227e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 2.8532e-04 - val_loss: 2.6122e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.7780e-04 - val_loss: 2.8962e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.6936e-04 - val_loss: 2.9076e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 2.5952e-04 - val_loss: 2.7523e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 2.5124e-04 - val_loss: 2.7068e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 2.4346e-04 - val_loss: 2.4503e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 2.3633e-04 - val_loss: 2.3561e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 2.2836e-04 - val_loss: 2.4832e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.2155e-04 - val_loss: 1.9932e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 2.1214e-04 - val_loss: 1.9525e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.0778e-04 - val_loss: 1.4963e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 2.0051e-04 - val_loss: 1.3444e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.9021e-04 - val_loss: 1.3767e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.8502e-04 - val_loss: 1.1496e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.7708e-04 - val_loss: 1.0969e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.7365e-04 - val_loss: 1.0167e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.6958e-04 - val_loss: 1.0029e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.6644e-04 - val_loss: 9.7094e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.6295e-04 - val_loss: 9.3313e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.5895e-04 - val_loss: 9.5587e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.5597e-04 - val_loss: 9.1936e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.5352e-04 - val_loss: 9.4984e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.5134e-04 - val_loss: 9.1236e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.4873e-04 - val_loss: 9.2983e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.4764e-04 - val_loss: 9.5134e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.4640e-04 - val_loss: 9.0882e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.4342e-04 - val_loss: 9.4736e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.4411e-04 - val_loss: 8.6623e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.4269e-04 - val_loss: 8.8954e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4070e-04 - val_loss: 8.6965e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.4017e-04 - val_loss: 8.6748e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3899e-04 - val_loss: 8.5891e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.3801e-04 - val_loss: 8.7233e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.3682e-04 - val_loss: 8.5987e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.3555e-04 - val_loss: 8.6637e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.3428e-04 - val_loss: 8.5564e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.3281e-04 - val_loss: 8.4647e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.3092e-04 - val_loss: 8.4258e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2885e-04 - val_loss: 8.4633e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.2752e-04 - val_loss: 8.2974e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.2592e-04 - val_loss: 8.1762e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.2391e-04 - val_loss: 8.0121e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.2196e-04 - val_loss: 8.0167e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2055e-04 - val_loss: 7.9191e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1815e-04 - val_loss: 7.8852e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 508us/step - loss: 1.1578e-04 - val_loss: 8.2604e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.1510e-04 - val_loss: 7.9380e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.1381e-04 - val_loss: 8.2766e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 507us/step - loss: 1.1234e-04 - val_loss: 8.6434e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1207e-04 - val_loss: 8.3221e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.1154e-04 - val_loss: 8.2060e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 509us/step - loss: 1.1099e-04 - val_loss: 8.3205e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 510us/step - loss: 1.0981e-04 - val_loss: 8.3534e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.1018e-04 - val_loss: 8.2085e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.0915e-04 - val_loss: 8.1515e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.0838e-04 - val_loss: 8.4750e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.0826e-04 - val_loss: 8.2759e-05\n",
      "Validation RMSE:  0.00909718 \n",
      "\n",
      "Model took 1440.45 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 47s 1ms/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 8.8083e-04 - val_loss: 7.8622e-04\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 6.0573e-04 - val_loss: 5.5061e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 5.2527e-04 - val_loss: 4.6067e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 4.6501e-04 - val_loss: 4.5483e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 4.2507e-04 - val_loss: 3.9340e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.9551e-04 - val_loss: 3.2316e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 3.7621e-04 - val_loss: 3.1870e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.5736e-04 - val_loss: 3.0811e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 3.4414e-04 - val_loss: 3.1337e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 3.3309e-04 - val_loss: 2.8293e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 3.2300e-04 - val_loss: 2.9200e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 3.1383e-04 - val_loss: 3.0417e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.0347e-04 - val_loss: 3.3215e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.9395e-04 - val_loss: 2.7932e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.8528e-04 - val_loss: 3.0021e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.7534e-04 - val_loss: 2.9404e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.6445e-04 - val_loss: 2.4892e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.5548e-04 - val_loss: 2.6481e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.4941e-04 - val_loss: 2.4843e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.3752e-04 - val_loss: 2.5225e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.2865e-04 - val_loss: 1.9378e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.1866e-04 - val_loss: 1.6657e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.0930e-04 - val_loss: 1.9723e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.0199e-04 - val_loss: 1.7068e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.9560e-04 - val_loss: 1.5616e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.8888e-04 - val_loss: 1.4206e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.8266e-04 - val_loss: 1.2498e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.7474e-04 - val_loss: 1.1943e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.7013e-04 - val_loss: 1.1162e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.6596e-04 - val_loss: 1.0706e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.6181e-04 - val_loss: 1.0339e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.5744e-04 - val_loss: 1.0187e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.5407e-04 - val_loss: 1.0212e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.5258e-04 - val_loss: 1.0002e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.5061e-04 - val_loss: 1.0510e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.4778e-04 - val_loss: 1.0174e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.4620e-04 - val_loss: 1.0614e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.4488e-04 - val_loss: 1.0156e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.4293e-04 - val_loss: 1.0992e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.4160e-04 - val_loss: 1.0130e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3950e-04 - val_loss: 9.9294e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.3796e-04 - val_loss: 9.6862e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3773e-04 - val_loss: 9.4802e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.3532e-04 - val_loss: 9.4533e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.3473e-04 - val_loss: 9.4194e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.3462e-04 - val_loss: 9.7538e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.3319e-04 - val_loss: 9.7397e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.3237e-04 - val_loss: 9.7534e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.3029e-04 - val_loss: 9.2685e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.3008e-04 - val_loss: 8.7532e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2869e-04 - val_loss: 8.8667e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2699e-04 - val_loss: 9.0162e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2498e-04 - val_loss: 1.0125e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.2509e-04 - val_loss: 9.1699e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2377e-04 - val_loss: 8.9761e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.2225e-04 - val_loss: 8.5611e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.2070e-04 - val_loss: 8.5713e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.2016e-04 - val_loss: 8.8511e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1943e-04 - val_loss: 8.4329e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1800e-04 - val_loss: 9.2037e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1697e-04 - val_loss: 9.6747e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1464e-04 - val_loss: 9.3074e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1625e-04 - val_loss: 9.0974e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.1394e-04 - val_loss: 1.0014e-04\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1425e-04 - val_loss: 9.7968e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1314e-04 - val_loss: 9.6983e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.1287e-04 - val_loss: 9.1208e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1130e-04 - val_loss: 9.5060e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1095e-04 - val_loss: 8.8116e-05\n",
      "Validation RMSE:  0.009387032 \n",
      "\n",
      "Model took 1460.92 seconds to train\n",
      "24 \t10    \t0.0108331 \t0.00344464 \t0.00903377\t0.0210167 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 48s 1ms/step - loss: 0.0081 - val_loss: 0.0040\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 6.9186e-04 - val_loss: 4.9099e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 5.1771e-04 - val_loss: 3.2451e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 4.4647e-04 - val_loss: 3.4474e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 4.0301e-04 - val_loss: 3.0350e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 3.7631e-04 - val_loss: 2.5682e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.5262e-04 - val_loss: 2.5249e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.3970e-04 - val_loss: 2.4305e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 3.2666e-04 - val_loss: 2.3134e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 3.1581e-04 - val_loss: 2.2599e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 3.0431e-04 - val_loss: 2.2212e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.9750e-04 - val_loss: 2.2264e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 2.8588e-04 - val_loss: 2.2616e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.7934e-04 - val_loss: 2.2619e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.6788e-04 - val_loss: 2.3735e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.5737e-04 - val_loss: 2.3170e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 2.4974e-04 - val_loss: 2.1675e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.4182e-04 - val_loss: 2.1890e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.3631e-04 - val_loss: 2.2867e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.2844e-04 - val_loss: 2.6747e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.1982e-04 - val_loss: 2.3686e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.1234e-04 - val_loss: 1.9918e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.0244e-04 - val_loss: 1.8706e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.9678e-04 - val_loss: 1.7498e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.9049e-04 - val_loss: 1.6486e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.8442e-04 - val_loss: 1.6094e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.7939e-04 - val_loss: 1.5279e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.7163e-04 - val_loss: 1.4888e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.6664e-04 - val_loss: 1.3897e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.6178e-04 - val_loss: 1.3023e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.5773e-04 - val_loss: 1.2641e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.5493e-04 - val_loss: 1.2083e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.5115e-04 - val_loss: 1.2302e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.4829e-04 - val_loss: 1.2290e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.4553e-04 - val_loss: 1.2071e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.4203e-04 - val_loss: 1.2522e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.4082e-04 - val_loss: 1.1903e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.3840e-04 - val_loss: 1.1502e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.3672e-04 - val_loss: 1.1348e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.3498e-04 - val_loss: 1.0797e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3353e-04 - val_loss: 1.0581e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3193e-04 - val_loss: 1.0293e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.3054e-04 - val_loss: 1.0215e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2849e-04 - val_loss: 9.9018e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.2690e-04 - val_loss: 9.5842e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.2495e-04 - val_loss: 9.0889e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.2409e-04 - val_loss: 8.8828e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2312e-04 - val_loss: 8.8088e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2223e-04 - val_loss: 8.4820e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2004e-04 - val_loss: 8.4578e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1846e-04 - val_loss: 8.5510e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.1830e-04 - val_loss: 8.5902e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1719e-04 - val_loss: 8.5303e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.1615e-04 - val_loss: 8.6760e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.1532e-04 - val_loss: 8.6295e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.1416e-04 - val_loss: 8.8856e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1233e-04 - val_loss: 8.8643e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1260e-04 - val_loss: 8.7473e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.1126e-04 - val_loss: 8.9334e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.1114e-04 - val_loss: 9.5304e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.1089e-04 - val_loss: 9.7308e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.0976e-04 - val_loss: 9.5424e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.0765e-04 - val_loss: 9.6266e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 511us/step - loss: 1.0757e-04 - val_loss: 9.8641e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.0799e-04 - val_loss: 9.6693e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.0608e-04 - val_loss: 9.1655e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 512us/step - loss: 1.0693e-04 - val_loss: 9.3994e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.0627e-04 - val_loss: 9.3014e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.0581e-04 - val_loss: 9.8260e-05\n",
      "Validation RMSE:  0.009912627 \n",
      "\n",
      "Model took 1450.62 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 48s 1ms/step - loss: 0.0049 - val_loss: 0.0039\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 6.8858e-04 - val_loss: 5.0087e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 5.5232e-04 - val_loss: 2.9697e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 4.9379e-04 - val_loss: 2.6374e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 4.4330e-04 - val_loss: 3.1994e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 4.0660e-04 - val_loss: 2.9505e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.8150e-04 - val_loss: 2.9782e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 3.6572e-04 - val_loss: 2.7368e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.4748e-04 - val_loss: 3.0815e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 3.3730e-04 - val_loss: 3.0005e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 3.2346e-04 - val_loss: 2.8198e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 3.1479e-04 - val_loss: 2.8729e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.0327e-04 - val_loss: 2.7553e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 2.9202e-04 - val_loss: 2.5005e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.8189e-04 - val_loss: 2.4629e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.7304e-04 - val_loss: 2.4282e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 2.6599e-04 - val_loss: 2.3273e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.5640e-04 - val_loss: 2.3146e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 2.4463e-04 - val_loss: 1.9162e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.3297e-04 - val_loss: 1.8311e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 2.2626e-04 - val_loss: 1.6514e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.1333e-04 - val_loss: 1.6500e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.0356e-04 - val_loss: 1.6559e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.9445e-04 - val_loss: 1.5943e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.8975e-04 - val_loss: 1.4233e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.8455e-04 - val_loss: 1.3623e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.7989e-04 - val_loss: 1.3382e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.7621e-04 - val_loss: 1.3846e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.7286e-04 - val_loss: 1.3735e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.7051e-04 - val_loss: 1.3300e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.6798e-04 - val_loss: 1.4079e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.6517e-04 - val_loss: 1.4062e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.6208e-04 - val_loss: 1.3502e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.5943e-04 - val_loss: 1.2996e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.5701e-04 - val_loss: 1.2955e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.5409e-04 - val_loss: 1.2288e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5167e-04 - val_loss: 1.2967e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.4988e-04 - val_loss: 1.2762e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.4779e-04 - val_loss: 1.2252e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4614e-04 - val_loss: 1.2057e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.4417e-04 - val_loss: 1.1212e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.4284e-04 - val_loss: 1.1326e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.4169e-04 - val_loss: 1.1176e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.4023e-04 - val_loss: 1.1136e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.3922e-04 - val_loss: 1.1003e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.3818e-04 - val_loss: 1.0948e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3725e-04 - val_loss: 1.1223e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.3668e-04 - val_loss: 1.0771e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.3574e-04 - val_loss: 1.0848e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.3500e-04 - val_loss: 1.0386e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.3391e-04 - val_loss: 1.0407e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.3314e-04 - val_loss: 1.0170e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3176e-04 - val_loss: 1.0344e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.3089e-04 - val_loss: 1.0283e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2875e-04 - val_loss: 1.0502e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2810e-04 - val_loss: 9.4381e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.2668e-04 - val_loss: 9.0230e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.2599e-04 - val_loss: 8.7295e-05\n",
      "Epoch 60/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2394e-04 - val_loss: 8.6739e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2310e-04 - val_loss: 1.0767e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2323e-04 - val_loss: 9.7893e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2102e-04 - val_loss: 8.5086e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.2102e-04 - val_loss: 8.9010e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1975e-04 - val_loss: 8.5703e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1798e-04 - val_loss: 8.5621e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.1784e-04 - val_loss: 8.5040e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.1686e-04 - val_loss: 8.2790e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1511e-04 - val_loss: 8.3026e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1433e-04 - val_loss: 8.1251e-05\n",
      "Validation RMSE:  0.009013949 \n",
      "\n",
      "Model took 1482.54 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 48s 1ms/step - loss: 0.0065 - val_loss: 0.0041\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 6.8026e-04 - val_loss: 4.8161e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 5.3350e-04 - val_loss: 4.0884e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 4.6677e-04 - val_loss: 3.4738e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 4.1829e-04 - val_loss: 2.6940e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 3.8665e-04 - val_loss: 2.6873e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 3.6562e-04 - val_loss: 2.7870e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 3.4794e-04 - val_loss: 2.8545e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 3.3492e-04 - val_loss: 2.7492e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 3.2349e-04 - val_loss: 2.6705e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 3.1222e-04 - val_loss: 2.5381e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 3.0076e-04 - val_loss: 2.4205e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.8704e-04 - val_loss: 2.5775e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.7957e-04 - val_loss: 2.6108e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.7209e-04 - val_loss: 2.5167e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.5918e-04 - val_loss: 2.2957e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.4569e-04 - val_loss: 2.6647e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.4176e-04 - val_loss: 1.7566e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.3560e-04 - val_loss: 1.5176e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.2439e-04 - val_loss: 1.9078e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.1259e-04 - val_loss: 1.8781e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.0570e-04 - val_loss: 1.6519e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.9809e-04 - val_loss: 1.3795e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.8910e-04 - val_loss: 1.3438e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.8164e-04 - val_loss: 1.1975e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.7607e-04 - val_loss: 1.1733e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.7140e-04 - val_loss: 1.1741e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.6840e-04 - val_loss: 1.0732e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.6485e-04 - val_loss: 1.0922e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.6211e-04 - val_loss: 1.0448e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.5888e-04 - val_loss: 1.0456e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.5631e-04 - val_loss: 1.0292e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.5367e-04 - val_loss: 1.0114e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.5340e-04 - val_loss: 1.0235e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.5024e-04 - val_loss: 1.0283e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.4936e-04 - val_loss: 1.0349e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.4797e-04 - val_loss: 1.0496e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.4613e-04 - val_loss: 1.0246e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.4454e-04 - val_loss: 9.6760e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.4245e-04 - val_loss: 1.0404e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.4071e-04 - val_loss: 1.0058e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.3970e-04 - val_loss: 9.8920e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3832e-04 - val_loss: 9.6619e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.3673e-04 - val_loss: 9.6442e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.3475e-04 - val_loss: 9.3318e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3459e-04 - val_loss: 9.8053e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3325e-04 - val_loss: 9.2375e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.3068e-04 - val_loss: 9.0647e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.3024e-04 - val_loss: 9.0479e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2972e-04 - val_loss: 8.8835e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2782e-04 - val_loss: 8.9047e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.2699e-04 - val_loss: 8.7076e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2594e-04 - val_loss: 8.6305e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2520e-04 - val_loss: 8.6282e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2399e-04 - val_loss: 8.7083e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.2277e-04 - val_loss: 8.6342e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2138e-04 - val_loss: 8.6975e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2107e-04 - val_loss: 9.0467e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1975e-04 - val_loss: 8.8287e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1925e-04 - val_loss: 9.2029e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1811e-04 - val_loss: 9.5151e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1646e-04 - val_loss: 9.7716e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.1530e-04 - val_loss: 1.0037e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1456e-04 - val_loss: 9.4990e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1392e-04 - val_loss: 9.6182e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1279e-04 - val_loss: 1.1112e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1243e-04 - val_loss: 9.8162e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1211e-04 - val_loss: 1.0994e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1016e-04 - val_loss: 1.1876e-04\n",
      "Validation RMSE:  0.010897815 \n",
      "\n",
      "Model took 1470.88 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 48s 1ms/step - loss: 0.0054 - val_loss: 0.0037\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 7.0961e-04 - val_loss: 6.3187e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 5.4301e-04 - val_loss: 4.2078e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 4.6646e-04 - val_loss: 4.1574e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 4.2099e-04 - val_loss: 3.5158e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.8605e-04 - val_loss: 3.1308e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.6194e-04 - val_loss: 2.8366e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 3.4771e-04 - val_loss: 3.1313e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 3.3474e-04 - val_loss: 3.0131e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 3.2352e-04 - val_loss: 2.9948e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.1198e-04 - val_loss: 3.0050e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.0142e-04 - val_loss: 3.0107e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.8860e-04 - val_loss: 2.7802e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.7635e-04 - val_loss: 2.7435e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.6769e-04 - val_loss: 2.4410e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.5647e-04 - val_loss: 2.0683e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.4275e-04 - val_loss: 2.1146e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.3934e-04 - val_loss: 1.6437e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.3174e-04 - val_loss: 1.6220e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.2228e-04 - val_loss: 1.7427e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.1432e-04 - val_loss: 1.8284e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.0930e-04 - val_loss: 1.5298e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.0429e-04 - val_loss: 1.3301e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.9884e-04 - val_loss: 1.2878e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.9335e-04 - val_loss: 1.2781e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.8907e-04 - val_loss: 1.3010e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.8472e-04 - val_loss: 1.3042e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.8055e-04 - val_loss: 1.1762e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.7586e-04 - val_loss: 1.1274e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.7271e-04 - val_loss: 1.1064e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.7010e-04 - val_loss: 1.1003e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.6582e-04 - val_loss: 1.0755e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.6362e-04 - val_loss: 1.0477e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.6089e-04 - val_loss: 9.8423e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5985e-04 - val_loss: 9.9074e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5857e-04 - val_loss: 9.9436e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.5534e-04 - val_loss: 9.8088e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.5562e-04 - val_loss: 9.5016e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.5418e-04 - val_loss: 9.5979e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5181e-04 - val_loss: 9.1716e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.5005e-04 - val_loss: 9.1371e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4829e-04 - val_loss: 8.9794e-05\n",
      "Epoch 44/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4603e-04 - val_loss: 8.8582e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.4419e-04 - val_loss: 8.9166e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.4280e-04 - val_loss: 8.7882e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.4114e-04 - val_loss: 8.6219e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3928e-04 - val_loss: 8.9678e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.3705e-04 - val_loss: 8.7503e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3509e-04 - val_loss: 8.7208e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3361e-04 - val_loss: 8.5907e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.3242e-04 - val_loss: 8.4986e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.3069e-04 - val_loss: 8.4929e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2880e-04 - val_loss: 8.6481e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2667e-04 - val_loss: 8.6762e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2387e-04 - val_loss: 8.4439e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2281e-04 - val_loss: 8.1203e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2149e-04 - val_loss: 8.0097e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2053e-04 - val_loss: 8.1440e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1946e-04 - val_loss: 7.9892e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1814e-04 - val_loss: 8.0123e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.1677e-04 - val_loss: 7.9840e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1525e-04 - val_loss: 8.0839e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.1483e-04 - val_loss: 8.0396e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1413e-04 - val_loss: 8.1878e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1417e-04 - val_loss: 8.0110e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1336e-04 - val_loss: 7.8405e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1265e-04 - val_loss: 7.6789e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.1161e-04 - val_loss: 7.4151e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.1026e-04 - val_loss: 7.4413e-05\n",
      "Validation RMSE:  0.008626317 \n",
      "\n",
      "Model took 1486.44 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 4\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 9\n",
      "2nd Window Start: 502\n",
      "3rd Window Start: 997\n",
      "4th Window Start: 1148\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Four Lags are\n",
      " [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Lags length:\n",
      " 80\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48845, 81)\n",
      "Training and test data length 39076 9769\n",
      "trainX.shape[0]:  39076\n",
      "window_length:  20\n",
      "no_wins:  4\n",
      "trainX shape:\n",
      " (39076, 80)\n",
      "Train X shape after np.reshape (39076, 20, 4)\n",
      "Test X shape after np.reshape (9769, 20, 4)\n",
      "Train Y Shape (39076,)\n",
      "Test Y Shape (9769,)\n",
      "Train on 39076 samples, validate on 9769 samples\n",
      "Epoch 1/70\n",
      "39076/39076 [==============================] - 47s 1ms/step - loss: 0.0042 - val_loss: 0.0016\n",
      "Epoch 2/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 6.9064e-04 - val_loss: 5.7863e-04\n",
      "Epoch 3/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 5.7345e-04 - val_loss: 3.6703e-04\n",
      "Epoch 4/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 5.1154e-04 - val_loss: 3.3354e-04\n",
      "Epoch 5/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 4.7247e-04 - val_loss: 3.0272e-04\n",
      "Epoch 6/70\n",
      "39076/39076 [==============================] - 21s 525us/step - loss: 4.3634e-04 - val_loss: 2.7439e-04\n",
      "Epoch 7/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 4.0713e-04 - val_loss: 2.6152e-04\n",
      "Epoch 8/70\n",
      "39076/39076 [==============================] - 21s 539us/step - loss: 3.8800e-04 - val_loss: 2.4649e-04\n",
      "Epoch 9/70\n",
      "39076/39076 [==============================] - 21s 530us/step - loss: 3.7117e-04 - val_loss: 2.3336e-04\n",
      "Epoch 10/70\n",
      "39076/39076 [==============================] - 21s 531us/step - loss: 3.5340e-04 - val_loss: 2.2330e-04\n",
      "Epoch 11/70\n",
      "39076/39076 [==============================] - 21s 530us/step - loss: 3.3962e-04 - val_loss: 2.2577e-04\n",
      "Epoch 12/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 3.2282e-04 - val_loss: 2.1121e-04\n",
      "Epoch 13/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 3.1115e-04 - val_loss: 2.2050e-04\n",
      "Epoch 14/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 3.0342e-04 - val_loss: 2.2491e-04\n",
      "Epoch 15/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 2.9506e-04 - val_loss: 2.4636e-04\n",
      "Epoch 16/70\n",
      "39076/39076 [==============================] - 21s 530us/step - loss: 2.8625e-04 - val_loss: 2.5277e-04\n",
      "Epoch 17/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 2.7956e-04 - val_loss: 2.4604e-04\n",
      "Epoch 18/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 2.6981e-04 - val_loss: 2.7450e-04\n",
      "Epoch 19/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 2.6205e-04 - val_loss: 2.1220e-04\n",
      "Epoch 20/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 2.5216e-04 - val_loss: 2.2093e-04\n",
      "Epoch 21/70\n",
      "39076/39076 [==============================] - 21s 530us/step - loss: 2.4612e-04 - val_loss: 2.3115e-04\n",
      "Epoch 22/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 2.4023e-04 - val_loss: 2.1029e-04\n",
      "Epoch 23/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 2.3343e-04 - val_loss: 2.5297e-04\n",
      "Epoch 24/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 2.2430e-04 - val_loss: 2.3497e-04\n",
      "Epoch 25/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 2.1877e-04 - val_loss: 2.1772e-04\n",
      "Epoch 26/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 2.1330e-04 - val_loss: 2.0426e-04\n",
      "Epoch 27/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 2.0889e-04 - val_loss: 1.8330e-04\n",
      "Epoch 28/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 2.0430e-04 - val_loss: 1.7615e-04\n",
      "Epoch 29/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 1.9846e-04 - val_loss: 1.6953e-04\n",
      "Epoch 30/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.9636e-04 - val_loss: 1.7141e-04\n",
      "Epoch 31/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.9274e-04 - val_loss: 1.7120e-04\n",
      "Epoch 32/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.8903e-04 - val_loss: 1.8007e-04\n",
      "Epoch 33/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.8476e-04 - val_loss: 1.7925e-04\n",
      "Epoch 34/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.8102e-04 - val_loss: 1.8984e-04\n",
      "Epoch 35/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 1.7853e-04 - val_loss: 1.8287e-04\n",
      "Epoch 36/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 1.7728e-04 - val_loss: 1.8877e-04\n",
      "Epoch 37/70\n",
      "39076/39076 [==============================] - 21s 530us/step - loss: 1.7343e-04 - val_loss: 1.8775e-04\n",
      "Epoch 38/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 1.7287e-04 - val_loss: 1.8673e-04\n",
      "Epoch 39/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 1.7006e-04 - val_loss: 1.8596e-04\n",
      "Epoch 40/70\n",
      "39076/39076 [==============================] - 21s 530us/step - loss: 1.6897e-04 - val_loss: 1.8750e-04\n",
      "Epoch 41/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.6819e-04 - val_loss: 1.5904e-04\n",
      "Epoch 42/70\n",
      "39076/39076 [==============================] - 20s 524us/step - loss: 1.6686e-04 - val_loss: 1.6232e-04\n",
      "Epoch 43/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.6472e-04 - val_loss: 1.7365e-04\n",
      "Epoch 44/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.6353e-04 - val_loss: 1.8449e-04\n",
      "Epoch 45/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 1.6304e-04 - val_loss: 1.6807e-04\n",
      "Epoch 46/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.6089e-04 - val_loss: 1.6808e-04\n",
      "Epoch 47/70\n",
      "39076/39076 [==============================] - 20s 524us/step - loss: 1.6140e-04 - val_loss: 1.6610e-04\n",
      "Epoch 48/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.6048e-04 - val_loss: 1.7358e-04\n",
      "Epoch 49/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.5836e-04 - val_loss: 1.6749e-04\n",
      "Epoch 50/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 1.5851e-04 - val_loss: 1.6794e-04\n",
      "Epoch 51/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 1.5472e-04 - val_loss: 1.6513e-04\n",
      "Epoch 52/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 1.5444e-04 - val_loss: 1.6806e-04\n",
      "Epoch 53/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.5592e-04 - val_loss: 1.5805e-04\n",
      "Epoch 54/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 1.5517e-04 - val_loss: 1.6483e-04\n",
      "Epoch 55/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.5322e-04 - val_loss: 1.5546e-04\n",
      "Epoch 56/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.5290e-04 - val_loss: 1.4558e-04\n",
      "Epoch 57/70\n",
      "39076/39076 [==============================] - 21s 525us/step - loss: 1.5096e-04 - val_loss: 1.4158e-04\n",
      "Epoch 58/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.5162e-04 - val_loss: 1.4025e-04\n",
      "Epoch 59/70\n",
      "39076/39076 [==============================] - 21s 530us/step - loss: 1.5120e-04 - val_loss: 1.4091e-04\n",
      "Epoch 60/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.5069e-04 - val_loss: 1.4066e-04\n",
      "Epoch 61/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.4858e-04 - val_loss: 1.4999e-04\n",
      "Epoch 62/70\n",
      "39076/39076 [==============================] - 21s 528us/step - loss: 1.4857e-04 - val_loss: 1.3502e-04\n",
      "Epoch 63/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.4641e-04 - val_loss: 1.5188e-04\n",
      "Epoch 64/70\n",
      "39076/39076 [==============================] - 21s 530us/step - loss: 1.4793e-04 - val_loss: 1.2784e-04\n",
      "Epoch 65/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.4760e-04 - val_loss: 1.3427e-04\n",
      "Epoch 66/70\n",
      "39076/39076 [==============================] - 21s 529us/step - loss: 1.4578e-04 - val_loss: 1.3152e-04\n",
      "Epoch 67/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 1.4415e-04 - val_loss: 1.3279e-04\n",
      "Epoch 68/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 1.4430e-04 - val_loss: 1.2777e-04\n",
      "Epoch 69/70\n",
      "39076/39076 [==============================] - 21s 527us/step - loss: 1.4195e-04 - val_loss: 1.4912e-04\n",
      "Epoch 70/70\n",
      "39076/39076 [==============================] - 21s 526us/step - loss: 1.4217e-04 - val_loss: 1.2083e-04\n",
      "Validation RMSE:  0.010992333 \n",
      "\n",
      "Model took 1483.98 seconds to train\n",
      "25 \t5     \t0.00957572\t0.000752208\t0.00862632\t0.0109923 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 49s 1ms/step - loss: 0.0077 - val_loss: 0.0040\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 6.7638e-04 - val_loss: 9.7004e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 5.0070e-04 - val_loss: 5.2071e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 4.2628e-04 - val_loss: 4.4059e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 3.8158e-04 - val_loss: 3.7798e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 3.6013e-04 - val_loss: 3.3853e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.3867e-04 - val_loss: 2.8909e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 3.1717e-04 - val_loss: 2.7548e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.0353e-04 - val_loss: 2.6713e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.9127e-04 - val_loss: 2.5600e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.8319e-04 - val_loss: 2.3553e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.7472e-04 - val_loss: 2.2177e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.6656e-04 - val_loss: 2.1870e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.5848e-04 - val_loss: 2.3514e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.5035e-04 - val_loss: 2.2952e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.4160e-04 - val_loss: 2.3319e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.3717e-04 - val_loss: 2.3168e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.3166e-04 - val_loss: 1.9643e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.2280e-04 - val_loss: 1.9929e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.1246e-04 - val_loss: 1.8573e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.0541e-04 - val_loss: 1.7312e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.9825e-04 - val_loss: 1.4944e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.9102e-04 - val_loss: 1.3707e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.8626e-04 - val_loss: 1.3934e-04\n",
      "Epoch 26/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.7946e-04 - val_loss: 1.4058e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.7143e-04 - val_loss: 1.3157e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.6558e-04 - val_loss: 1.2149e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.5944e-04 - val_loss: 1.1946e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5577e-04 - val_loss: 1.1602e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5261e-04 - val_loss: 1.0382e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.4813e-04 - val_loss: 1.0292e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.4661e-04 - val_loss: 9.8723e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.4426e-04 - val_loss: 9.5754e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.4207e-04 - val_loss: 9.5964e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.4111e-04 - val_loss: 9.0666e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.3938e-04 - val_loss: 9.4134e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3809e-04 - val_loss: 9.5841e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3653e-04 - val_loss: 9.7571e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3559e-04 - val_loss: 1.0036e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3325e-04 - val_loss: 8.7303e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.3290e-04 - val_loss: 9.0212e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3123e-04 - val_loss: 9.2348e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.3107e-04 - val_loss: 9.0776e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2978e-04 - val_loss: 9.1960e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2897e-04 - val_loss: 9.0020e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2824e-04 - val_loss: 8.9253e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2737e-04 - val_loss: 8.8296e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2654e-04 - val_loss: 8.8793e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2533e-04 - val_loss: 9.1747e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2492e-04 - val_loss: 9.1006e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2432e-04 - val_loss: 8.6039e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2310e-04 - val_loss: 8.8741e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2148e-04 - val_loss: 8.8233e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2139e-04 - val_loss: 8.3858e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2061e-04 - val_loss: 8.5630e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1832e-04 - val_loss: 8.1977e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1670e-04 - val_loss: 7.8526e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1753e-04 - val_loss: 7.4435e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1616e-04 - val_loss: 7.5551e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1581e-04 - val_loss: 7.5297e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1385e-04 - val_loss: 7.5505e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1190e-04 - val_loss: 7.9116e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1230e-04 - val_loss: 7.7881e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1102e-04 - val_loss: 7.7606e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.0928e-04 - val_loss: 7.8130e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.0987e-04 - val_loss: 7.7479e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.0767e-04 - val_loss: 7.7390e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.0768e-04 - val_loss: 8.0231e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.0632e-04 - val_loss: 8.0626e-05\n",
      "Validation RMSE:  0.008979197 \n",
      "\n",
      "Model took 1496.35 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 48s 1ms/step - loss: 0.0049 - val_loss: 0.0038\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 6.7686e-04 - val_loss: 5.0287e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 5.2230e-04 - val_loss: 3.3710e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 4.4816e-04 - val_loss: 3.0430e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 4.0559e-04 - val_loss: 2.8250e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 3.8039e-04 - val_loss: 2.6322e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 3.6192e-04 - val_loss: 2.4521e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 3.4576e-04 - val_loss: 2.3398e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 3.3281e-04 - val_loss: 2.3001e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 3.2090e-04 - val_loss: 2.4353e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.1015e-04 - val_loss: 2.4993e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.0157e-04 - val_loss: 2.5057e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.9362e-04 - val_loss: 2.4710e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.8533e-04 - val_loss: 2.4515e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.7626e-04 - val_loss: 2.3252e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.6639e-04 - val_loss: 2.2264e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.5731e-04 - val_loss: 2.2087e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.5003e-04 - val_loss: 2.3091e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.4343e-04 - val_loss: 2.1067e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.3236e-04 - val_loss: 1.9564e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.2316e-04 - val_loss: 1.8991e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.1767e-04 - val_loss: 1.7525e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.0980e-04 - val_loss: 1.6638e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.0276e-04 - val_loss: 1.5472e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.9753e-04 - val_loss: 1.4865e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.8980e-04 - val_loss: 1.3915e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.8278e-04 - val_loss: 1.2454e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.7568e-04 - val_loss: 1.1548e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.7006e-04 - val_loss: 1.1095e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.6424e-04 - val_loss: 1.0495e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.5859e-04 - val_loss: 1.1315e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.5600e-04 - val_loss: 9.6353e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.5199e-04 - val_loss: 9.6672e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.5022e-04 - val_loss: 9.3386e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.4614e-04 - val_loss: 8.8670e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.4334e-04 - val_loss: 8.7201e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.4146e-04 - val_loss: 8.5897e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.3959e-04 - val_loss: 8.9492e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.3820e-04 - val_loss: 9.2814e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3651e-04 - val_loss: 9.2053e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3531e-04 - val_loss: 9.3577e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3410e-04 - val_loss: 9.5296e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3292e-04 - val_loss: 9.8128e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3168e-04 - val_loss: 1.0025e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2992e-04 - val_loss: 9.8575e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2701e-04 - val_loss: 9.5610e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2412e-04 - val_loss: 9.5840e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2509e-04 - val_loss: 9.2731e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2415e-04 - val_loss: 9.2630e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2269e-04 - val_loss: 9.2787e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2128e-04 - val_loss: 9.0147e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2075e-04 - val_loss: 9.2449e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1833e-04 - val_loss: 8.9711e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.1757e-04 - val_loss: 9.3866e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1700e-04 - val_loss: 9.2481e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.1632e-04 - val_loss: 9.3116e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.1649e-04 - val_loss: 9.1004e-05\n",
      "Validation RMSE:  0.009539625 \n",
      "\n",
      "Model took 1223.75 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 49s 1ms/step - loss: 0.0058 - val_loss: 0.0034\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 0.0012 - val_loss: 0.0022\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 6.4260e-04 - val_loss: 5.3353e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 5.1214e-04 - val_loss: 3.6839e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 4.4184e-04 - val_loss: 3.3275e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.9487e-04 - val_loss: 2.7133e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.6554e-04 - val_loss: 2.8433e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.4776e-04 - val_loss: 2.7246e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.3033e-04 - val_loss: 2.7770e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.1382e-04 - val_loss: 2.7376e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 3.0706e-04 - val_loss: 2.6950e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.9862e-04 - val_loss: 2.5519e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.9039e-04 - val_loss: 2.4822e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.8531e-04 - val_loss: 2.3695e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.7911e-04 - val_loss: 2.2244e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.7326e-04 - val_loss: 2.1715e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.6993e-04 - val_loss: 2.1691e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.6256e-04 - val_loss: 2.2164e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.5531e-04 - val_loss: 2.3351e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.4787e-04 - val_loss: 2.4918e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.4073e-04 - val_loss: 2.3297e-04\n",
      "Epoch 22/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.3594e-04 - val_loss: 2.4413e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.2868e-04 - val_loss: 2.3840e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.2148e-04 - val_loss: 2.2296e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.0801e-04 - val_loss: 2.1056e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.9768e-04 - val_loss: 1.8239e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.9218e-04 - val_loss: 1.6854e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.8496e-04 - val_loss: 1.6568e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.7630e-04 - val_loss: 1.5146e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.7031e-04 - val_loss: 1.4210e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.6399e-04 - val_loss: 1.2318e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.5998e-04 - val_loss: 1.1799e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.5563e-04 - val_loss: 1.1700e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.5062e-04 - val_loss: 1.1920e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.4721e-04 - val_loss: 1.1976e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4494e-04 - val_loss: 1.1548e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.4122e-04 - val_loss: 1.1605e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3923e-04 - val_loss: 1.2118e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3702e-04 - val_loss: 1.1860e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3487e-04 - val_loss: 1.1381e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3490e-04 - val_loss: 1.1244e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.3403e-04 - val_loss: 1.0993e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3269e-04 - val_loss: 1.0920e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3208e-04 - val_loss: 1.0910e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2974e-04 - val_loss: 1.1288e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2890e-04 - val_loss: 1.1419e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2797e-04 - val_loss: 1.1496e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2596e-04 - val_loss: 1.1459e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2435e-04 - val_loss: 1.1770e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2392e-04 - val_loss: 1.1963e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2353e-04 - val_loss: 1.1857e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.2206e-04 - val_loss: 1.2216e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2007e-04 - val_loss: 1.2821e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2055e-04 - val_loss: 1.1966e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2052e-04 - val_loss: 1.2642e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1847e-04 - val_loss: 1.3197e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1800e-04 - val_loss: 1.2158e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1781e-04 - val_loss: 1.1610e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.1733e-04 - val_loss: 1.2307e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1653e-04 - val_loss: 1.1419e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1364e-04 - val_loss: 1.4147e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.1433e-04 - val_loss: 1.2511e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.1230e-04 - val_loss: 1.1790e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1048e-04 - val_loss: 1.3047e-04\n",
      "Validation RMSE:  0.011422436 \n",
      "\n",
      "Model took 1372.18 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 49s 1ms/step - loss: 0.0057 - val_loss: 0.0047\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 0.0017 - val_loss: 0.0019\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 7.5117e-04 - val_loss: 0.0011\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 5.5710e-04 - val_loss: 4.7986e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 4.8578e-04 - val_loss: 4.1908e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 4.3553e-04 - val_loss: 3.7560e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.9916e-04 - val_loss: 3.5088e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.6995e-04 - val_loss: 2.8076e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 3.5523e-04 - val_loss: 2.5709e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.4218e-04 - val_loss: 2.4202e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 3.3027e-04 - val_loss: 2.4036e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 3.2034e-04 - val_loss: 2.5843e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.0839e-04 - val_loss: 2.5150e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 3.0002e-04 - val_loss: 2.4513e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.8813e-04 - val_loss: 2.5123e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.7817e-04 - val_loss: 2.2894e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.6969e-04 - val_loss: 2.2552e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.6363e-04 - val_loss: 2.1755e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.5554e-04 - val_loss: 2.1500e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.4660e-04 - val_loss: 2.0452e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.3955e-04 - val_loss: 1.7096e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.3034e-04 - val_loss: 1.5813e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.2470e-04 - val_loss: 1.3490e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.1288e-04 - val_loss: 1.2751e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.0089e-04 - val_loss: 1.2300e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.9186e-04 - val_loss: 1.0836e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.8306e-04 - val_loss: 1.0061e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.7617e-04 - val_loss: 9.6990e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.7170e-04 - val_loss: 9.1852e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.6708e-04 - val_loss: 9.8171e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.6224e-04 - val_loss: 9.2938e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.5837e-04 - val_loss: 9.1951e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.5485e-04 - val_loss: 1.0979e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.5170e-04 - val_loss: 1.1518e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.4831e-04 - val_loss: 1.1484e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.4645e-04 - val_loss: 1.1685e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.4454e-04 - val_loss: 1.1178e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.4245e-04 - val_loss: 1.1352e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.4077e-04 - val_loss: 1.1942e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.3894e-04 - val_loss: 1.2300e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3700e-04 - val_loss: 1.2216e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.3532e-04 - val_loss: 1.2197e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.3386e-04 - val_loss: 1.1735e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3190e-04 - val_loss: 1.2159e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2835e-04 - val_loss: 1.2357e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.2841e-04 - val_loss: 1.2251e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.2705e-04 - val_loss: 1.2149e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2720e-04 - val_loss: 1.2539e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.2601e-04 - val_loss: 1.1894e-04\n",
      "Validation RMSE:  0.010905759 \n",
      "\n",
      "Model took 1038.22 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "Overlap 2 and 3\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 50s 1ms/step - loss: 0.0050 - val_loss: 0.0036\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 6.4754e-04 - val_loss: 5.6674e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 5.2830e-04 - val_loss: 3.6505e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 4.6160e-04 - val_loss: 3.5807e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 4.1818e-04 - val_loss: 2.8658e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 3.9405e-04 - val_loss: 3.0661e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 3.7289e-04 - val_loss: 2.6491e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 3.5730e-04 - val_loss: 2.6612e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 3.4475e-04 - val_loss: 2.7059e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 3.2946e-04 - val_loss: 2.6008e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.1947e-04 - val_loss: 2.5932e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.0695e-04 - val_loss: 2.5696e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 2.9945e-04 - val_loss: 2.2974e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 2.8482e-04 - val_loss: 2.3067e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.7453e-04 - val_loss: 2.2723e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 2.6492e-04 - val_loss: 2.2071e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 2.5836e-04 - val_loss: 2.1829e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 2.5196e-04 - val_loss: 2.0159e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 2.3536e-04 - val_loss: 1.7782e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.2330e-04 - val_loss: 1.7041e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 2.1188e-04 - val_loss: 1.3968e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.0656e-04 - val_loss: 1.3178e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.9798e-04 - val_loss: 1.3134e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.8913e-04 - val_loss: 1.1341e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.8275e-04 - val_loss: 1.0962e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.7492e-04 - val_loss: 1.0471e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.7196e-04 - val_loss: 9.9667e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.6714e-04 - val_loss: 9.7850e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.6288e-04 - val_loss: 9.4623e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.5877e-04 - val_loss: 9.5586e-05\n",
      "Epoch 32/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.5587e-04 - val_loss: 9.6035e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.5253e-04 - val_loss: 9.7305e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.4981e-04 - val_loss: 9.7709e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.4809e-04 - val_loss: 9.5970e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.4685e-04 - val_loss: 9.4170e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.4482e-04 - val_loss: 9.4045e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.4313e-04 - val_loss: 9.4277e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.4120e-04 - val_loss: 9.2872e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.4024e-04 - val_loss: 9.1662e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.3868e-04 - val_loss: 9.3295e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.3703e-04 - val_loss: 9.4663e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3630e-04 - val_loss: 9.4072e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3580e-04 - val_loss: 9.3539e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.3287e-04 - val_loss: 9.8523e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.3155e-04 - val_loss: 9.4822e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.3093e-04 - val_loss: 9.3136e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.3017e-04 - val_loss: 9.7135e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2910e-04 - val_loss: 9.3094e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2995e-04 - val_loss: 1.0019e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2930e-04 - val_loss: 9.6634e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2884e-04 - val_loss: 9.4084e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.2714e-04 - val_loss: 1.0314e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2644e-04 - val_loss: 9.8000e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.2491e-04 - val_loss: 1.0571e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.2319e-04 - val_loss: 1.1767e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.2206e-04 - val_loss: 1.2671e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.2119e-04 - val_loss: 1.3046e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2155e-04 - val_loss: 1.0424e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.2072e-04 - val_loss: 1.0117e-04\n",
      "Validation RMSE:  0.010058259 \n",
      "\n",
      "Model took 1273.99 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 50s 1ms/step - loss: 0.0066 - val_loss: 0.0041\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 6.4876e-04 - val_loss: 6.3985e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 4.8192e-04 - val_loss: 4.0822e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 4.1633e-04 - val_loss: 3.5108e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 3.7142e-04 - val_loss: 3.0976e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 3.4436e-04 - val_loss: 2.9686e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 3.2485e-04 - val_loss: 2.6727e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 3.1168e-04 - val_loss: 2.5668e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 2.9922e-04 - val_loss: 2.2887e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.8946e-04 - val_loss: 2.2967e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.7688e-04 - val_loss: 2.4361e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.6333e-04 - val_loss: 2.4885e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 2.5237e-04 - val_loss: 2.6083e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.4203e-04 - val_loss: 2.3985e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 2.2809e-04 - val_loss: 1.9553e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 2.2198e-04 - val_loss: 1.9051e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 2.1560e-04 - val_loss: 1.6270e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 2.0260e-04 - val_loss: 1.4453e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.9406e-04 - val_loss: 1.3842e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.8371e-04 - val_loss: 1.2603e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.7539e-04 - val_loss: 1.1262e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.6867e-04 - val_loss: 1.1142e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.6443e-04 - val_loss: 9.8429e-05\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.6091e-04 - val_loss: 1.2989e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.5697e-04 - val_loss: 1.2584e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.5289e-04 - val_loss: 1.2541e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.5105e-04 - val_loss: 1.2970e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.4866e-04 - val_loss: 1.2290e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.4487e-04 - val_loss: 1.1487e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.4234e-04 - val_loss: 1.0861e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.4053e-04 - val_loss: 1.2393e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 516us/step - loss: 1.3952e-04 - val_loss: 1.1902e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3745e-04 - val_loss: 1.1232e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.3603e-04 - val_loss: 1.1135e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 517us/step - loss: 1.3461e-04 - val_loss: 1.0827e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.3303e-04 - val_loss: 1.0967e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.3167e-04 - val_loss: 1.0897e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.3141e-04 - val_loss: 1.0862e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.3124e-04 - val_loss: 1.0399e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 513us/step - loss: 1.2965e-04 - val_loss: 1.0689e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 20s 514us/step - loss: 1.2916e-04 - val_loss: 1.0244e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.2861e-04 - val_loss: 1.0660e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 515us/step - loss: 1.2674e-04 - val_loss: 1.0274e-04\n",
      "Validation RMSE:  0.010136132 \n",
      "\n",
      "Model took 933.40 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 51s 1ms/step - loss: 0.0060 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 7.4592e-04 - val_loss: 0.0011\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 5.3985e-04 - val_loss: 5.6709e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 4.6206e-04 - val_loss: 4.6438e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 4.1672e-04 - val_loss: 3.9593e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.8418e-04 - val_loss: 3.1811e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.5544e-04 - val_loss: 3.0109e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.3843e-04 - val_loss: 3.0873e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 3.2558e-04 - val_loss: 3.2963e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.1413e-04 - val_loss: 2.7144e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.0443e-04 - val_loss: 2.5926e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.9509e-04 - val_loss: 2.5200e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.8672e-04 - val_loss: 2.3721e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.7785e-04 - val_loss: 2.2769e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.6410e-04 - val_loss: 2.3545e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.5436e-04 - val_loss: 2.2674e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.3994e-04 - val_loss: 2.4075e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.2794e-04 - val_loss: 2.3034e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.2556e-04 - val_loss: 1.8601e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.1371e-04 - val_loss: 1.9478e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.0444e-04 - val_loss: 1.7795e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.9459e-04 - val_loss: 1.6563e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.8684e-04 - val_loss: 1.4051e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.7798e-04 - val_loss: 1.0444e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.7311e-04 - val_loss: 1.2265e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.6949e-04 - val_loss: 1.2523e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.6549e-04 - val_loss: 1.2030e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.6231e-04 - val_loss: 1.1400e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.6000e-04 - val_loss: 1.1544e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.5710e-04 - val_loss: 1.0321e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.5335e-04 - val_loss: 1.0039e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.5189e-04 - val_loss: 1.0403e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.4868e-04 - val_loss: 1.0329e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4679e-04 - val_loss: 1.0244e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.4346e-04 - val_loss: 1.0290e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4167e-04 - val_loss: 1.0371e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3963e-04 - val_loss: 1.0185e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3840e-04 - val_loss: 1.0940e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.3682e-04 - val_loss: 1.0508e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3460e-04 - val_loss: 1.0127e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.3306e-04 - val_loss: 9.4936e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.3167e-04 - val_loss: 9.5147e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.3093e-04 - val_loss: 1.0447e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2964e-04 - val_loss: 1.0378e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.2794e-04 - val_loss: 9.8781e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.2743e-04 - val_loss: 1.0182e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2449e-04 - val_loss: 1.1053e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2474e-04 - val_loss: 9.3406e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2280e-04 - val_loss: 9.8540e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2256e-04 - val_loss: 9.5670e-05\n",
      "Epoch 52/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2163e-04 - val_loss: 9.7398e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2100e-04 - val_loss: 9.6446e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2013e-04 - val_loss: 9.2898e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1958e-04 - val_loss: 9.1099e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1862e-04 - val_loss: 8.9032e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1766e-04 - val_loss: 8.8043e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1621e-04 - val_loss: 8.6908e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.1521e-04 - val_loss: 8.7854e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1460e-04 - val_loss: 8.6894e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1402e-04 - val_loss: 8.6159e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.1280e-04 - val_loss: 8.8500e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1259e-04 - val_loss: 8.5996e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1203e-04 - val_loss: 8.9916e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1136e-04 - val_loss: 8.1657e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1026e-04 - val_loss: 8.5624e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.0947e-04 - val_loss: 8.6087e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.0862e-04 - val_loss: 8.6526e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.0782e-04 - val_loss: 8.4191e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.0827e-04 - val_loss: 8.2142e-05\n",
      "Validation RMSE:  0.009063226 \n",
      "\n",
      "Model took 1490.37 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 51s 1ms/step - loss: 0.0056 - val_loss: 0.0038\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 7.0161e-04 - val_loss: 5.7810e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 5.5180e-04 - val_loss: 4.6581e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 4.7741e-04 - val_loss: 3.7229e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 4.2477e-04 - val_loss: 3.6929e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 3.9142e-04 - val_loss: 3.2106e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.6852e-04 - val_loss: 3.1291e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 3.5231e-04 - val_loss: 2.8260e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 3.3624e-04 - val_loss: 2.7146e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 3.2238e-04 - val_loss: 2.5721e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 3.1539e-04 - val_loss: 2.4157e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 3.0713e-04 - val_loss: 2.3287e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 3.0097e-04 - val_loss: 2.4037e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.9432e-04 - val_loss: 2.5654e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 2.8619e-04 - val_loss: 2.5065e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.7907e-04 - val_loss: 2.7179e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 2.7205e-04 - val_loss: 2.5676e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.6514e-04 - val_loss: 2.4047e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 2.5762e-04 - val_loss: 2.3880e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 2.5581e-04 - val_loss: 2.4629e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 2.4957e-04 - val_loss: 2.3260e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.4280e-04 - val_loss: 2.1293e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.3322e-04 - val_loss: 1.8161e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 2.1570e-04 - val_loss: 1.6138e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 2.0613e-04 - val_loss: 1.4884e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.9560e-04 - val_loss: 1.3244e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.8602e-04 - val_loss: 1.1611e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.7552e-04 - val_loss: 9.9246e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.7278e-04 - val_loss: 9.0166e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.6759e-04 - val_loss: 8.7739e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.6393e-04 - val_loss: 8.9047e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.6006e-04 - val_loss: 8.8521e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.5700e-04 - val_loss: 8.6545e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.5352e-04 - val_loss: 8.5927e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.5088e-04 - val_loss: 8.6872e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 20s 518us/step - loss: 1.4911e-04 - val_loss: 8.4894e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.4725e-04 - val_loss: 8.6031e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.4571e-04 - val_loss: 8.4947e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.4438e-04 - val_loss: 8.4858e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.4288e-04 - val_loss: 8.5531e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.4113e-04 - val_loss: 8.7098e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.3949e-04 - val_loss: 8.6722e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.3795e-04 - val_loss: 8.9903e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.3624e-04 - val_loss: 9.1525e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.3501e-04 - val_loss: 9.2814e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.3379e-04 - val_loss: 9.2005e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.3277e-04 - val_loss: 9.3190e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.3224e-04 - val_loss: 9.1235e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.3132e-04 - val_loss: 8.9733e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.3027e-04 - val_loss: 8.8395e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2973e-04 - val_loss: 9.0296e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2891e-04 - val_loss: 9.2412e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2827e-04 - val_loss: 9.2679e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2751e-04 - val_loss: 9.2570e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.2668e-04 - val_loss: 8.7417e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2621e-04 - val_loss: 8.6786e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2498e-04 - val_loss: 8.4293e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.2347e-04 - val_loss: 8.6801e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 20s 519us/step - loss: 1.2310e-04 - val_loss: 8.7179e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.2227e-04 - val_loss: 8.8050e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 20s 522us/step - loss: 1.2135e-04 - val_loss: 8.8063e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 20s 520us/step - loss: 1.2108e-04 - val_loss: 8.5948e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2048e-04 - val_loss: 8.8464e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1934e-04 - val_loss: 9.0199e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1957e-04 - val_loss: 8.8038e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1814e-04 - val_loss: 8.7693e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1800e-04 - val_loss: 8.5343e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1772e-04 - val_loss: 8.3343e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 20s 521us/step - loss: 1.1665e-04 - val_loss: 8.2051e-05\n",
      "Validation RMSE:  0.009058222 \n",
      "\n",
      "Model took 1475.63 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 51s 1ms/step - loss: 0.0062 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 6.9639e-04 - val_loss: 6.1826e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 5.3442e-04 - val_loss: 3.8552e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 4.6070e-04 - val_loss: 3.4886e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 4.0976e-04 - val_loss: 3.1697e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.7461e-04 - val_loss: 3.1333e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 3.4833e-04 - val_loss: 2.8632e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.3616e-04 - val_loss: 2.6745e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.2533e-04 - val_loss: 2.1851e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.1357e-04 - val_loss: 2.1387e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.0593e-04 - val_loss: 2.1866e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.9706e-04 - val_loss: 2.2237e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.8256e-04 - val_loss: 2.1873e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.7228e-04 - val_loss: 2.0882e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.6309e-04 - val_loss: 2.1758e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.5492e-04 - val_loss: 2.0476e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.4990e-04 - val_loss: 2.0943e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.3870e-04 - val_loss: 2.4938e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.3005e-04 - val_loss: 2.1891e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.2304e-04 - val_loss: 2.0752e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.1712e-04 - val_loss: 2.1277e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.0486e-04 - val_loss: 1.8435e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.9699e-04 - val_loss: 1.3276e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.8690e-04 - val_loss: 1.3013e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.7822e-04 - val_loss: 1.0823e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.7130e-04 - val_loss: 1.0441e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.6741e-04 - val_loss: 1.0541e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.6424e-04 - val_loss: 1.0932e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.6134e-04 - val_loss: 1.0874e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.5855e-04 - val_loss: 1.0911e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.5553e-04 - val_loss: 1.0622e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.5262e-04 - val_loss: 1.0287e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.5033e-04 - val_loss: 1.0080e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4850e-04 - val_loss: 1.0158e-04\n",
      "Epoch 36/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.4633e-04 - val_loss: 1.0023e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.4433e-04 - val_loss: 9.8788e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.4328e-04 - val_loss: 9.6954e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.4205e-04 - val_loss: 9.5988e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.4117e-04 - val_loss: 9.3158e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3997e-04 - val_loss: 9.2006e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.3961e-04 - val_loss: 8.9978e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3826e-04 - val_loss: 8.9690e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3722e-04 - val_loss: 8.7310e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.3620e-04 - val_loss: 8.6815e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3525e-04 - val_loss: 8.6650e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3403e-04 - val_loss: 8.7105e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3296e-04 - val_loss: 8.7640e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.3098e-04 - val_loss: 8.1732e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3036e-04 - val_loss: 8.2478e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2946e-04 - val_loss: 8.7441e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2810e-04 - val_loss: 8.8473e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2615e-04 - val_loss: 8.4913e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2577e-04 - val_loss: 8.7867e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2505e-04 - val_loss: 8.3551e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2473e-04 - val_loss: 8.1699e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2382e-04 - val_loss: 8.4383e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2286e-04 - val_loss: 8.5344e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2166e-04 - val_loss: 8.5471e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2074e-04 - val_loss: 8.2535e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1971e-04 - val_loss: 8.0626e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1864e-04 - val_loss: 8.2215e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1718e-04 - val_loss: 8.3200e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1777e-04 - val_loss: 8.1479e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.1691e-04 - val_loss: 8.2912e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1542e-04 - val_loss: 8.1008e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1527e-04 - val_loss: 8.3073e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.1403e-04 - val_loss: 8.0000e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1441e-04 - val_loss: 7.9791e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1399e-04 - val_loss: 7.8505e-05\n",
      "Validation RMSE:  0.008860327 \n",
      "\n",
      "Model took 1498.14 seconds to train\n",
      "26 \t10    \t0.0888023 \t0.237067   \t0.00886033\t0.8       \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 52s 1ms/step - loss: 0.0053 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 6.5221e-04 - val_loss: 4.5594e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 5.3259e-04 - val_loss: 3.7386e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 4.6330e-04 - val_loss: 3.4376e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 4.1078e-04 - val_loss: 3.1287e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 3.7904e-04 - val_loss: 2.8532e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.5783e-04 - val_loss: 2.9204e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.4126e-04 - val_loss: 2.6505e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.2669e-04 - val_loss: 2.6964e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.1147e-04 - val_loss: 2.8140e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.9805e-04 - val_loss: 2.5888e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.8346e-04 - val_loss: 2.5063e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.7283e-04 - val_loss: 2.3504e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.6003e-04 - val_loss: 2.2306e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.4683e-04 - val_loss: 1.9660e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.4118e-04 - val_loss: 1.6699e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.3365e-04 - val_loss: 1.9742e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.2165e-04 - val_loss: 2.1281e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.1443e-04 - val_loss: 1.7214e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.0549e-04 - val_loss: 1.5220e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.9933e-04 - val_loss: 1.4150e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.8915e-04 - val_loss: 1.3749e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.8637e-04 - val_loss: 1.2241e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.7914e-04 - val_loss: 1.2150e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.7425e-04 - val_loss: 1.1183e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.6833e-04 - val_loss: 1.0734e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.6528e-04 - val_loss: 1.0175e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.6246e-04 - val_loss: 9.2207e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.5994e-04 - val_loss: 8.9535e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.5598e-04 - val_loss: 8.9896e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.5295e-04 - val_loss: 9.0588e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.4985e-04 - val_loss: 9.0862e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.4720e-04 - val_loss: 9.8561e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.4502e-04 - val_loss: 9.6161e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4420e-04 - val_loss: 9.2349e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4262e-04 - val_loss: 9.4265e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4043e-04 - val_loss: 9.1880e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.3906e-04 - val_loss: 8.9643e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.3811e-04 - val_loss: 8.7990e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3719e-04 - val_loss: 9.1753e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.3564e-04 - val_loss: 8.9690e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3438e-04 - val_loss: 8.6847e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.3355e-04 - val_loss: 8.5656e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.3173e-04 - val_loss: 9.2663e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.3204e-04 - val_loss: 9.0547e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3129e-04 - val_loss: 9.0525e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2965e-04 - val_loss: 8.8200e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2843e-04 - val_loss: 8.7294e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2881e-04 - val_loss: 8.7370e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2700e-04 - val_loss: 9.2490e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2674e-04 - val_loss: 9.3719e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.2577e-04 - val_loss: 9.6500e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.2526e-04 - val_loss: 9.5960e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.2400e-04 - val_loss: 9.6384e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2262e-04 - val_loss: 9.8774e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2176e-04 - val_loss: 9.8620e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2171e-04 - val_loss: 1.0346e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2029e-04 - val_loss: 1.0383e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 523us/step - loss: 1.1917e-04 - val_loss: 1.1321e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1985e-04 - val_loss: 1.0468e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1941e-04 - val_loss: 1.2235e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1724e-04 - val_loss: 1.1963e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1699e-04 - val_loss: 1.2411e-04\n",
      "Validation RMSE:  0.011140291 \n",
      "\n",
      "Model took 1366.28 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 53s 1ms/step - loss: 0.0057 - val_loss: 0.0037\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 6.7173e-04 - val_loss: 7.0949e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 5.4673e-04 - val_loss: 3.8991e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 4.7384e-04 - val_loss: 3.7716e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 4.2909e-04 - val_loss: 3.2180e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 3.9745e-04 - val_loss: 3.0198e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 3.7791e-04 - val_loss: 2.6949e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 3.5840e-04 - val_loss: 2.6759e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 3.4444e-04 - val_loss: 2.6124e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 3.3076e-04 - val_loss: 2.4535e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 3.1937e-04 - val_loss: 2.6296e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.0913e-04 - val_loss: 2.5256e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 3.0097e-04 - val_loss: 2.5935e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.9157e-04 - val_loss: 2.2725e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.8077e-04 - val_loss: 2.1910e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.6915e-04 - val_loss: 2.0516e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 2.5763e-04 - val_loss: 2.1254e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.4464e-04 - val_loss: 1.8735e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.3561e-04 - val_loss: 1.8000e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.3291e-04 - val_loss: 1.5630e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.2602e-04 - val_loss: 1.4505e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.1729e-04 - val_loss: 1.4183e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 2.0398e-04 - val_loss: 1.2740e-04\n",
      "Epoch 25/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.9667e-04 - val_loss: 1.1069e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.9091e-04 - val_loss: 1.0289e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.8422e-04 - val_loss: 1.0158e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.7811e-04 - val_loss: 9.9452e-05\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.7295e-04 - val_loss: 9.8536e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.6823e-04 - val_loss: 9.5205e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.6414e-04 - val_loss: 9.7475e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.6060e-04 - val_loss: 9.7668e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5655e-04 - val_loss: 1.0064e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.5198e-04 - val_loss: 1.0487e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.4851e-04 - val_loss: 1.0323e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.4386e-04 - val_loss: 9.3284e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.4195e-04 - val_loss: 9.7870e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3901e-04 - val_loss: 9.9002e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3864e-04 - val_loss: 9.3334e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3718e-04 - val_loss: 9.4026e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.3530e-04 - val_loss: 9.1248e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.3254e-04 - val_loss: 9.3152e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3087e-04 - val_loss: 8.4742e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2821e-04 - val_loss: 8.3018e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2781e-04 - val_loss: 8.1818e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2706e-04 - val_loss: 8.3908e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2505e-04 - val_loss: 7.8503e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.2273e-04 - val_loss: 8.7665e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.2194e-04 - val_loss: 8.4302e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.2023e-04 - val_loss: 8.3626e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.1995e-04 - val_loss: 8.4344e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.1920e-04 - val_loss: 8.3244e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1890e-04 - val_loss: 8.9829e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.1806e-04 - val_loss: 8.7914e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.1671e-04 - val_loss: 8.9382e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.1711e-04 - val_loss: 9.3215e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1527e-04 - val_loss: 9.0634e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.1564e-04 - val_loss: 9.1609e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.1473e-04 - val_loss: 9.3867e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1352e-04 - val_loss: 1.0329e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.1207e-04 - val_loss: 9.8177e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.1163e-04 - val_loss: 9.6588e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.1072e-04 - val_loss: 9.7121e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.0871e-04 - val_loss: 9.4915e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.0813e-04 - val_loss: 9.6659e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.0792e-04 - val_loss: 9.7769e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.0733e-04 - val_loss: 9.6291e-05\n",
      "Validation RMSE:  0.009812795 \n",
      "\n",
      "Model took 1456.35 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 22 19 1\n",
      "1st Window Start: 10\n",
      "2nd Window Start: 511\n",
      "3rd Window Start: 1000\n",
      "4th Window Start: 1133\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 1, 1, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1, 1, 1]\n",
      "Lags length:\n",
      " 66\n",
      "Length Used:\n",
      " 22\n",
      "New Dataset shape after Lags: (48982, 67)\n",
      "Training and test data length 39185 9797\n",
      "trainX.shape[0]:  39185\n",
      "window_length:  22\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39185, 66)\n",
      "Train X shape after np.reshape (39185, 22, 3)\n",
      "Test X shape after np.reshape (9797, 22, 3)\n",
      "Train Y Shape (39185,)\n",
      "Test Y Shape (9797,)\n",
      "Train on 39185 samples, validate on 9797 samples\n",
      "Epoch 1/70\n",
      "39185/39185 [==============================] - 54s 1ms/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 2/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 0.0016 - val_loss: 8.5254e-04\n",
      "Epoch 3/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 5.9939e-04 - val_loss: 5.0104e-04\n",
      "Epoch 4/70\n",
      "39185/39185 [==============================] - 23s 574us/step - loss: 4.4581e-04 - val_loss: 2.9804e-04\n",
      "Epoch 5/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 3.8209e-04 - val_loss: 2.8919e-04\n",
      "Epoch 6/70\n",
      "39185/39185 [==============================] - 23s 577us/step - loss: 3.5055e-04 - val_loss: 2.5550e-04\n",
      "Epoch 7/70\n",
      "39185/39185 [==============================] - 23s 574us/step - loss: 3.0459e-04 - val_loss: 2.1247e-04\n",
      "Epoch 8/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 2.7218e-04 - val_loss: 2.3337e-04\n",
      "Epoch 9/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 2.5604e-04 - val_loss: 2.2279e-04\n",
      "Epoch 10/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 2.4338e-04 - val_loss: 2.0568e-04\n",
      "Epoch 11/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 2.3403e-04 - val_loss: 1.8762e-04\n",
      "Epoch 12/70\n",
      "39185/39185 [==============================] - 23s 577us/step - loss: 2.2527e-04 - val_loss: 1.9773e-04\n",
      "Epoch 13/70\n",
      "39185/39185 [==============================] - 22s 573us/step - loss: 2.1634e-04 - val_loss: 2.0181e-04\n",
      "Epoch 14/70\n",
      "39185/39185 [==============================] - 22s 573us/step - loss: 2.0677e-04 - val_loss: 2.4466e-04\n",
      "Epoch 15/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 2.0224e-04 - val_loss: 1.9015e-04\n",
      "Epoch 16/70\n",
      "39185/39185 [==============================] - 23s 577us/step - loss: 1.9680e-04 - val_loss: 1.9010e-04\n",
      "Epoch 17/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.9283e-04 - val_loss: 1.9324e-04\n",
      "Epoch 18/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.8979e-04 - val_loss: 1.5122e-04\n",
      "Epoch 19/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.8464e-04 - val_loss: 1.4363e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.8214e-04 - val_loss: 1.3728e-04\n",
      "Epoch 21/70\n",
      "39185/39185 [==============================] - 23s 577us/step - loss: 1.7855e-04 - val_loss: 1.3325e-04\n",
      "Epoch 22/70\n",
      "39185/39185 [==============================] - 23s 579us/step - loss: 1.7566e-04 - val_loss: 1.3354e-04\n",
      "Epoch 23/70\n",
      "39185/39185 [==============================] - 23s 578us/step - loss: 1.7419e-04 - val_loss: 1.3891e-04\n",
      "Epoch 24/70\n",
      "39185/39185 [==============================] - 22s 573us/step - loss: 1.7234e-04 - val_loss: 1.4253e-04\n",
      "Epoch 25/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.7068e-04 - val_loss: 1.4232e-04\n",
      "Epoch 26/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.6850e-04 - val_loss: 1.4027e-04\n",
      "Epoch 27/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.6573e-04 - val_loss: 1.3663e-04\n",
      "Epoch 28/70\n",
      "39185/39185 [==============================] - 23s 574us/step - loss: 1.6480e-04 - val_loss: 1.3820e-04\n",
      "Epoch 29/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.6429e-04 - val_loss: 1.3384e-04\n",
      "Epoch 30/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.6263e-04 - val_loss: 1.3105e-04\n",
      "Epoch 31/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.6119e-04 - val_loss: 1.2989e-04\n",
      "Epoch 32/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.5903e-04 - val_loss: 1.2578e-04\n",
      "Epoch 33/70\n",
      "39185/39185 [==============================] - 23s 577us/step - loss: 1.5859e-04 - val_loss: 1.2295e-04\n",
      "Epoch 34/70\n",
      "39185/39185 [==============================] - 23s 577us/step - loss: 1.5660e-04 - val_loss: 1.1923e-04\n",
      "Epoch 35/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.5591e-04 - val_loss: 1.1981e-04\n",
      "Epoch 36/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.5390e-04 - val_loss: 1.1662e-04\n",
      "Epoch 37/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.5239e-04 - val_loss: 1.1300e-04\n",
      "Epoch 38/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.5070e-04 - val_loss: 1.1189e-04\n",
      "Epoch 39/70\n",
      "39185/39185 [==============================] - 23s 577us/step - loss: 1.4924e-04 - val_loss: 1.0840e-04\n",
      "Epoch 40/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.4730e-04 - val_loss: 1.1048e-04\n",
      "Epoch 41/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.4623e-04 - val_loss: 1.1512e-04\n",
      "Epoch 42/70\n",
      "39185/39185 [==============================] - 22s 573us/step - loss: 1.4536e-04 - val_loss: 1.1717e-04\n",
      "Epoch 43/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.4387e-04 - val_loss: 1.1996e-04\n",
      "Epoch 44/70\n",
      "39185/39185 [==============================] - 23s 578us/step - loss: 1.4170e-04 - val_loss: 1.1354e-04\n",
      "Epoch 45/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.4122e-04 - val_loss: 1.1924e-04\n",
      "Epoch 46/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.3896e-04 - val_loss: 1.1567e-04\n",
      "Epoch 47/70\n",
      "39185/39185 [==============================] - 23s 578us/step - loss: 1.3833e-04 - val_loss: 1.1283e-04\n",
      "Epoch 48/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.3668e-04 - val_loss: 1.1220e-04\n",
      "Epoch 49/70\n",
      "39185/39185 [==============================] - 22s 573us/step - loss: 1.3542e-04 - val_loss: 1.1738e-04\n",
      "Epoch 50/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.3393e-04 - val_loss: 1.1255e-04\n",
      "Epoch 51/70\n",
      "39185/39185 [==============================] - 22s 572us/step - loss: 1.3324e-04 - val_loss: 1.0897e-04\n",
      "Epoch 52/70\n",
      "39185/39185 [==============================] - 23s 579us/step - loss: 1.3136e-04 - val_loss: 1.1004e-04\n",
      "Epoch 53/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.3042e-04 - val_loss: 1.0976e-04\n",
      "Epoch 54/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.2934e-04 - val_loss: 1.0640e-04\n",
      "Epoch 55/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.2778e-04 - val_loss: 1.0739e-04\n",
      "Epoch 56/70\n",
      "39185/39185 [==============================] - 23s 576us/step - loss: 1.2577e-04 - val_loss: 1.0630e-04\n",
      "Epoch 57/70\n",
      "39185/39185 [==============================] - 22s 573us/step - loss: 1.2522e-04 - val_loss: 1.0331e-04\n",
      "Epoch 58/70\n",
      "39185/39185 [==============================] - 23s 578us/step - loss: 1.2311e-04 - val_loss: 8.8396e-05\n",
      "Epoch 59/70\n",
      "39185/39185 [==============================] - 23s 577us/step - loss: 1.2324e-04 - val_loss: 9.2824e-05\n",
      "Epoch 60/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.2071e-04 - val_loss: 9.7473e-05\n",
      "Epoch 61/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.2175e-04 - val_loss: 1.0189e-04\n",
      "Epoch 62/70\n",
      "39185/39185 [==============================] - 23s 578us/step - loss: 1.2080e-04 - val_loss: 1.0302e-04\n",
      "Epoch 63/70\n",
      "39185/39185 [==============================] - 22s 572us/step - loss: 1.1919e-04 - val_loss: 9.4281e-05\n",
      "Epoch 64/70\n",
      "39185/39185 [==============================] - 22s 573us/step - loss: 1.1774e-04 - val_loss: 8.9838e-05\n",
      "Epoch 65/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.1757e-04 - val_loss: 9.6821e-05\n",
      "Epoch 66/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.1752e-04 - val_loss: 9.9430e-05\n",
      "Epoch 67/70\n",
      "39185/39185 [==============================] - 22s 573us/step - loss: 1.1641e-04 - val_loss: 1.0047e-04\n",
      "Epoch 68/70\n",
      "39185/39185 [==============================] - 22s 574us/step - loss: 1.1577e-04 - val_loss: 9.7015e-05\n",
      "Epoch 69/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.1532e-04 - val_loss: 9.6419e-05\n",
      "Epoch 70/70\n",
      "39185/39185 [==============================] - 23s 575us/step - loss: 1.1526e-04 - val_loss: 9.6550e-05\n",
      "Validation RMSE:  0.009825994 \n",
      "\n",
      "Model took 1625.58 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 53s 1ms/step - loss: 0.0056 - val_loss: 0.0038\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 7.2077e-04 - val_loss: 8.1384e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 5.5790e-04 - val_loss: 5.4603e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 4.8144e-04 - val_loss: 4.4103e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 4.3333e-04 - val_loss: 3.6666e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 4.0365e-04 - val_loss: 3.1167e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.8048e-04 - val_loss: 2.5632e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.6357e-04 - val_loss: 2.5814e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 3.4443e-04 - val_loss: 2.2026e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 3.3101e-04 - val_loss: 2.0608e-04\n",
      "Epoch 12/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.1885e-04 - val_loss: 2.0645e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.1052e-04 - val_loss: 2.0167e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.0394e-04 - val_loss: 1.9890e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.9476e-04 - val_loss: 2.0244e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.8276e-04 - val_loss: 1.9727e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.7185e-04 - val_loss: 1.9435e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.5873e-04 - val_loss: 1.7780e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.4992e-04 - val_loss: 1.7883e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.4575e-04 - val_loss: 1.7115e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.3290e-04 - val_loss: 1.7664e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.1790e-04 - val_loss: 2.2404e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.1375e-04 - val_loss: 1.5257e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.0229e-04 - val_loss: 1.3184e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.9401e-04 - val_loss: 1.2456e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.8633e-04 - val_loss: 1.2671e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.8046e-04 - val_loss: 1.2480e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.7536e-04 - val_loss: 1.2412e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.7117e-04 - val_loss: 1.2424e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.6809e-04 - val_loss: 1.1967e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.6507e-04 - val_loss: 1.1927e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.6184e-04 - val_loss: 1.1775e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.5897e-04 - val_loss: 1.1497e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.5622e-04 - val_loss: 1.1556e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.5351e-04 - val_loss: 1.1737e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5113e-04 - val_loss: 1.1201e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.4910e-04 - val_loss: 1.1105e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.4673e-04 - val_loss: 1.0910e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.4501e-04 - val_loss: 1.1244e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.4306e-04 - val_loss: 1.1298e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4109e-04 - val_loss: 1.1153e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3955e-04 - val_loss: 1.1298e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3784e-04 - val_loss: 1.1381e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3635e-04 - val_loss: 1.1092e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3500e-04 - val_loss: 1.0650e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3375e-04 - val_loss: 1.0090e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3287e-04 - val_loss: 1.0059e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3207e-04 - val_loss: 9.9236e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3110e-04 - val_loss: 9.7424e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3051e-04 - val_loss: 9.7352e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2881e-04 - val_loss: 9.5376e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2823e-04 - val_loss: 9.5454e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2779e-04 - val_loss: 9.5406e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2690e-04 - val_loss: 9.5746e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2578e-04 - val_loss: 9.2431e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2501e-04 - val_loss: 9.1508e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2449e-04 - val_loss: 8.8079e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2325e-04 - val_loss: 8.7283e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2244e-04 - val_loss: 8.5985e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.2192e-04 - val_loss: 8.2779e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2007e-04 - val_loss: 8.0741e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.1881e-04 - val_loss: 8.0441e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1675e-04 - val_loss: 8.0420e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1704e-04 - val_loss: 7.9059e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1448e-04 - val_loss: 7.8082e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1537e-04 - val_loss: 7.8681e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1425e-04 - val_loss: 7.9933e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.1411e-04 - val_loss: 7.8732e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1292e-04 - val_loss: 8.0715e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1291e-04 - val_loss: 7.9961e-05\n",
      "Validation RMSE:  0.008942089 \n",
      "\n",
      "Model took 1497.93 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 54s 1ms/step - loss: 0.0061 - val_loss: 0.0049\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 7.0805e-04 - val_loss: 5.9796e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 5.3353e-04 - val_loss: 4.6621e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 4.6794e-04 - val_loss: 3.8437e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 4.1944e-04 - val_loss: 3.6707e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 3.7940e-04 - val_loss: 3.2234e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 3.5649e-04 - val_loss: 3.2638e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.3901e-04 - val_loss: 3.1983e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.2939e-04 - val_loss: 3.0610e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.1707e-04 - val_loss: 2.8840e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 3.0828e-04 - val_loss: 2.7753e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.0091e-04 - val_loss: 2.7969e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.9343e-04 - val_loss: 2.8212e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.8480e-04 - val_loss: 2.7343e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.7741e-04 - val_loss: 2.7464e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.6763e-04 - val_loss: 2.8480e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.6076e-04 - val_loss: 2.8542e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.5111e-04 - val_loss: 2.6953e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.4679e-04 - val_loss: 2.7760e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.4016e-04 - val_loss: 2.4796e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.3063e-04 - val_loss: 2.3075e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.2265e-04 - val_loss: 2.1999e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.1472e-04 - val_loss: 2.2927e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.0898e-04 - val_loss: 2.0110e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.9912e-04 - val_loss: 1.5253e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.8705e-04 - val_loss: 1.2929e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.8043e-04 - val_loss: 1.2806e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.7371e-04 - val_loss: 1.2507e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.6860e-04 - val_loss: 1.1825e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.6471e-04 - val_loss: 1.2374e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.6166e-04 - val_loss: 1.4022e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5855e-04 - val_loss: 1.5519e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.5607e-04 - val_loss: 1.4019e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.5389e-04 - val_loss: 1.5538e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5126e-04 - val_loss: 1.3894e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.4844e-04 - val_loss: 1.4445e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4565e-04 - val_loss: 1.2971e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4416e-04 - val_loss: 1.1259e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.4167e-04 - val_loss: 1.1252e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.3907e-04 - val_loss: 1.0104e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3766e-04 - val_loss: 1.0507e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3601e-04 - val_loss: 1.0086e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3474e-04 - val_loss: 1.0286e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3494e-04 - val_loss: 1.0555e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.3264e-04 - val_loss: 1.0286e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3218e-04 - val_loss: 1.0109e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3051e-04 - val_loss: 9.6501e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3010e-04 - val_loss: 9.8152e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2929e-04 - val_loss: 1.0310e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.2726e-04 - val_loss: 9.9325e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2667e-04 - val_loss: 1.0397e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2613e-04 - val_loss: 9.8938e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.2448e-04 - val_loss: 9.9036e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.2417e-04 - val_loss: 9.4959e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.2335e-04 - val_loss: 9.1218e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.2181e-04 - val_loss: 9.0063e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2093e-04 - val_loss: 9.0457e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2001e-04 - val_loss: 8.8356e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1856e-04 - val_loss: 8.9216e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1763e-04 - val_loss: 8.3634e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1743e-04 - val_loss: 8.6286e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1647e-04 - val_loss: 8.2184e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1527e-04 - val_loss: 8.7141e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1424e-04 - val_loss: 9.0874e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1399e-04 - val_loss: 9.1755e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1328e-04 - val_loss: 9.1879e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1226e-04 - val_loss: 8.6947e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1143e-04 - val_loss: 1.0038e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1177e-04 - val_loss: 9.5313e-05\n",
      "Validation RMSE:  0.00976286 \n",
      "\n",
      "Model took 1495.31 seconds to train\n",
      "27 \t5     \t0.00949056\t0.00066469 \t0.00886033\t0.0111403 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 54s 1ms/step - loss: 0.0054 - val_loss: 0.0045\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 0.0017 - val_loss: 0.0021\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 8.2470e-04 - val_loss: 7.4713e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 5.7872e-04 - val_loss: 6.3972e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 4.8183e-04 - val_loss: 4.1605e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 4.3300e-04 - val_loss: 3.5328e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.9780e-04 - val_loss: 3.1662e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.7138e-04 - val_loss: 2.7594e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.5279e-04 - val_loss: 2.4920e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.3929e-04 - val_loss: 2.4293e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.2929e-04 - val_loss: 2.3556e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.1883e-04 - val_loss: 2.2691e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.0800e-04 - val_loss: 2.2159e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.9751e-04 - val_loss: 2.1807e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.8916e-04 - val_loss: 2.2473e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.7910e-04 - val_loss: 2.1493e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.6965e-04 - val_loss: 2.1176e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.6230e-04 - val_loss: 2.2400e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.4690e-04 - val_loss: 2.4426e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.4064e-04 - val_loss: 2.4562e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.3588e-04 - val_loss: 2.1510e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.3430e-04 - val_loss: 1.6892e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.2758e-04 - val_loss: 1.7811e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.2071e-04 - val_loss: 1.5423e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.0542e-04 - val_loss: 1.7180e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.9777e-04 - val_loss: 1.6670e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.9029e-04 - val_loss: 1.3191e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.8270e-04 - val_loss: 1.3683e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.7742e-04 - val_loss: 1.3041e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.7218e-04 - val_loss: 1.4018e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.6987e-04 - val_loss: 1.2488e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.6582e-04 - val_loss: 1.2391e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.6239e-04 - val_loss: 1.2084e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.5945e-04 - val_loss: 1.3605e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.5915e-04 - val_loss: 1.2698e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.5646e-04 - val_loss: 1.2663e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.5501e-04 - val_loss: 1.4664e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.5246e-04 - val_loss: 1.3704e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.4984e-04 - val_loss: 1.4207e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.4896e-04 - val_loss: 1.4583e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.4593e-04 - val_loss: 1.3288e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.4319e-04 - val_loss: 1.3779e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.4144e-04 - val_loss: 1.2324e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3993e-04 - val_loss: 1.2227e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.3762e-04 - val_loss: 1.1452e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3653e-04 - val_loss: 1.1094e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.3462e-04 - val_loss: 1.0741e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.3350e-04 - val_loss: 1.0535e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2924e-04 - val_loss: 1.0264e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2915e-04 - val_loss: 1.0328e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2772e-04 - val_loss: 9.5034e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2636e-04 - val_loss: 9.4589e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2656e-04 - val_loss: 9.1552e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2573e-04 - val_loss: 1.0284e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2355e-04 - val_loss: 9.6880e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2339e-04 - val_loss: 9.7379e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2164e-04 - val_loss: 9.6568e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.1921e-04 - val_loss: 9.7644e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1833e-04 - val_loss: 9.4164e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1628e-04 - val_loss: 1.0372e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.1813e-04 - val_loss: 9.3059e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1706e-04 - val_loss: 1.0230e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1623e-04 - val_loss: 9.8475e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.1501e-04 - val_loss: 9.8811e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.1428e-04 - val_loss: 9.8482e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1120e-04 - val_loss: 1.0605e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1187e-04 - val_loss: 9.6487e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.1195e-04 - val_loss: 9.6262e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1155e-04 - val_loss: 9.5337e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.0984e-04 - val_loss: 9.2991e-05\n",
      "Validation RMSE:  0.009643195 \n",
      "\n",
      "Model took 1502.34 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 55s 1ms/step - loss: 0.0071 - val_loss: 0.0048\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 7.2952e-04 - val_loss: 9.0466e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 5.0237e-04 - val_loss: 4.6552e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 4.2356e-04 - val_loss: 4.1593e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 3.7750e-04 - val_loss: 3.9057e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.4687e-04 - val_loss: 3.6726e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 3.2363e-04 - val_loss: 3.3916e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 3.0633e-04 - val_loss: 3.1049e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.9199e-04 - val_loss: 2.8797e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.8251e-04 - val_loss: 2.5987e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.7351e-04 - val_loss: 2.4925e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.6391e-04 - val_loss: 2.4246e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.5673e-04 - val_loss: 2.1944e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.4893e-04 - val_loss: 2.0882e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.4266e-04 - val_loss: 1.9265e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.3229e-04 - val_loss: 1.8932e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.2396e-04 - val_loss: 2.3081e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.1918e-04 - val_loss: 2.2179e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.1258e-04 - val_loss: 2.2371e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.0685e-04 - val_loss: 1.9722e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.0391e-04 - val_loss: 2.2536e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.9924e-04 - val_loss: 2.0841e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.9544e-04 - val_loss: 2.0068e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.9082e-04 - val_loss: 1.9900e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.8583e-04 - val_loss: 2.0614e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.7995e-04 - val_loss: 1.9966e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.7721e-04 - val_loss: 2.0511e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.7409e-04 - val_loss: 1.9048e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.6847e-04 - val_loss: 1.5004e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.6052e-04 - val_loss: 1.4578e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.5358e-04 - val_loss: 1.2057e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4636e-04 - val_loss: 1.1307e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.4165e-04 - val_loss: 1.0944e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.3682e-04 - val_loss: 1.1167e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3556e-04 - val_loss: 1.0066e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3428e-04 - val_loss: 9.5692e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3336e-04 - val_loss: 9.3006e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3210e-04 - val_loss: 9.3159e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3010e-04 - val_loss: 9.3541e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2827e-04 - val_loss: 9.4977e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2663e-04 - val_loss: 9.3516e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2645e-04 - val_loss: 9.3444e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2376e-04 - val_loss: 9.4432e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.2439e-04 - val_loss: 9.6617e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2359e-04 - val_loss: 1.1283e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2190e-04 - val_loss: 1.0142e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2078e-04 - val_loss: 1.0730e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2043e-04 - val_loss: 1.0730e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1822e-04 - val_loss: 1.0245e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1717e-04 - val_loss: 9.1851e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1633e-04 - val_loss: 9.9081e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1646e-04 - val_loss: 9.4944e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1518e-04 - val_loss: 8.8597e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.1482e-04 - val_loss: 9.2619e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1456e-04 - val_loss: 8.4710e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1397e-04 - val_loss: 8.3631e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.1323e-04 - val_loss: 8.3199e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1235e-04 - val_loss: 8.3810e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1097e-04 - val_loss: 7.5440e-05\n",
      "Epoch 61/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1011e-04 - val_loss: 7.3406e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.1012e-04 - val_loss: 7.9403e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.0917e-04 - val_loss: 8.1568e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.0939e-04 - val_loss: 8.6357e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.0743e-04 - val_loss: 8.3123e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.0694e-04 - val_loss: 8.3529e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.0599e-04 - val_loss: 8.3153e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.0484e-04 - val_loss: 8.3868e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.0504e-04 - val_loss: 8.1503e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.0377e-04 - val_loss: 8.4128e-05\n",
      "Validation RMSE:  0.00917211 \n",
      "\n",
      "Model took 1507.73 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 55s 1ms/step - loss: 0.0074 - val_loss: 0.0049\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 8.2424e-04 - val_loss: 9.1156e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 5.5567e-04 - val_loss: 5.2944e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 4.6466e-04 - val_loss: 3.9821e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 4.0525e-04 - val_loss: 3.7791e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 3.6624e-04 - val_loss: 3.0769e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 3.4271e-04 - val_loss: 2.5886e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 3.2526e-04 - val_loss: 2.6322e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 3.1019e-04 - val_loss: 2.4355e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.9818e-04 - val_loss: 2.4052e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.8708e-04 - val_loss: 2.3491e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.7341e-04 - val_loss: 2.3723e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.6511e-04 - val_loss: 2.1763e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.5347e-04 - val_loss: 2.1142e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.4521e-04 - val_loss: 1.9935e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.4097e-04 - val_loss: 2.1228e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.3412e-04 - val_loss: 1.5471e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 2.2135e-04 - val_loss: 1.5187e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 2.0898e-04 - val_loss: 1.8720e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.0008e-04 - val_loss: 1.7419e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.9079e-04 - val_loss: 1.5913e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.8395e-04 - val_loss: 1.4170e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.7833e-04 - val_loss: 1.2475e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.7376e-04 - val_loss: 1.2869e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.7049e-04 - val_loss: 1.2456e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.6599e-04 - val_loss: 1.2284e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.6206e-04 - val_loss: 1.1046e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5925e-04 - val_loss: 1.1126e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.5669e-04 - val_loss: 1.0479e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.5548e-04 - val_loss: 1.1620e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.5316e-04 - val_loss: 1.1291e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.5210e-04 - val_loss: 1.0972e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.4891e-04 - val_loss: 1.0179e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4626e-04 - val_loss: 1.0651e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.4530e-04 - val_loss: 1.0084e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.4154e-04 - val_loss: 9.6459e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4056e-04 - val_loss: 1.0054e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.3966e-04 - val_loss: 9.8994e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3840e-04 - val_loss: 1.0424e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3585e-04 - val_loss: 9.7899e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3493e-04 - val_loss: 9.5656e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.3386e-04 - val_loss: 9.9010e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.3207e-04 - val_loss: 9.4729e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3028e-04 - val_loss: 9.3203e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2778e-04 - val_loss: 9.4511e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.2742e-04 - val_loss: 9.3535e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2614e-04 - val_loss: 8.4380e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2478e-04 - val_loss: 8.8554e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2371e-04 - val_loss: 8.3125e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2318e-04 - val_loss: 8.8752e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2206e-04 - val_loss: 8.8473e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2117e-04 - val_loss: 8.6656e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.2064e-04 - val_loss: 8.2259e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1894e-04 - val_loss: 8.4389e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.1847e-04 - val_loss: 8.1655e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1780e-04 - val_loss: 8.5269e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.1651e-04 - val_loss: 8.3985e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1557e-04 - val_loss: 8.4086e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1449e-04 - val_loss: 8.3882e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1203e-04 - val_loss: 8.4028e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1069e-04 - val_loss: 8.4304e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.0912e-04 - val_loss: 8.4957e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.0937e-04 - val_loss: 8.5242e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.0836e-04 - val_loss: 8.7607e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.0861e-04 - val_loss: 8.6575e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.0741e-04 - val_loss: 8.4507e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.0678e-04 - val_loss: 8.5136e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.0619e-04 - val_loss: 8.5756e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.0575e-04 - val_loss: 8.5250e-05\n",
      "Validation RMSE:  0.009233091 \n",
      "\n",
      "Model took 1519.66 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 55s 1ms/step - loss: 0.0062 - val_loss: 0.0051\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 8.2029e-04 - val_loss: 8.6268e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 5.7886e-04 - val_loss: 4.7494e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 4.9598e-04 - val_loss: 4.1358e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 4.4422e-04 - val_loss: 4.3184e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 4.0736e-04 - val_loss: 3.8259e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.8189e-04 - val_loss: 3.3931e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 3.5844e-04 - val_loss: 3.1144e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 3.4185e-04 - val_loss: 3.0361e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.2730e-04 - val_loss: 2.8365e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 3.1715e-04 - val_loss: 2.7428e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 3.0696e-04 - val_loss: 2.5662e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.9649e-04 - val_loss: 2.6782e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.8727e-04 - val_loss: 2.6052e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.7827e-04 - val_loss: 2.4695e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.6694e-04 - val_loss: 2.3262e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.5957e-04 - val_loss: 2.3990e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 2.5359e-04 - val_loss: 2.3294e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.4842e-04 - val_loss: 2.3902e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 2.4203e-04 - val_loss: 2.6024e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.3187e-04 - val_loss: 2.6001e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 2.1893e-04 - val_loss: 2.0501e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 2.1239e-04 - val_loss: 1.9313e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.9854e-04 - val_loss: 1.8236e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.9106e-04 - val_loss: 1.4342e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.8381e-04 - val_loss: 1.3785e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.7679e-04 - val_loss: 1.2655e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.7192e-04 - val_loss: 1.1685e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.6725e-04 - val_loss: 1.1754e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 20s 523us/step - loss: 1.6357e-04 - val_loss: 1.1564e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.6091e-04 - val_loss: 1.1323e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.5869e-04 - val_loss: 1.1664e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.5724e-04 - val_loss: 1.1109e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.5482e-04 - val_loss: 1.1092e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.5240e-04 - val_loss: 1.1186e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.5177e-04 - val_loss: 1.0883e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.5017e-04 - val_loss: 1.0496e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.4879e-04 - val_loss: 9.6927e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.4626e-04 - val_loss: 9.2543e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.4527e-04 - val_loss: 8.9018e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.4402e-04 - val_loss: 8.8117e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.4243e-04 - val_loss: 8.7554e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.4062e-04 - val_loss: 8.6074e-05\n",
      "Epoch 45/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3850e-04 - val_loss: 8.5277e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3661e-04 - val_loss: 8.5691e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.3528e-04 - val_loss: 8.5493e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.3362e-04 - val_loss: 8.4862e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.3164e-04 - val_loss: 8.3737e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 525us/step - loss: 1.2987e-04 - val_loss: 8.3056e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.2766e-04 - val_loss: 8.2302e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.2704e-04 - val_loss: 8.0077e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2584e-04 - val_loss: 7.8420e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2386e-04 - val_loss: 7.7255e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2390e-04 - val_loss: 7.6084e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2242e-04 - val_loss: 7.4268e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2151e-04 - val_loss: 7.3003e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.2134e-04 - val_loss: 7.4272e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.2012e-04 - val_loss: 7.2599e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.1923e-04 - val_loss: 7.6134e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1821e-04 - val_loss: 7.5397e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1708e-04 - val_loss: 7.4485e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1640e-04 - val_loss: 7.3536e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 526us/step - loss: 1.1573e-04 - val_loss: 7.4548e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1464e-04 - val_loss: 7.2517e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1391e-04 - val_loss: 7.3258e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1297e-04 - val_loss: 7.3863e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 528us/step - loss: 1.1185e-04 - val_loss: 7.1724e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 527us/step - loss: 1.1133e-04 - val_loss: 7.2629e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 524us/step - loss: 1.1024e-04 - val_loss: 7.3451e-05\n",
      "Validation RMSE:  0.008570368 \n",
      "\n",
      "Model took 1495.52 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 56s 1ms/step - loss: 0.0060 - val_loss: 0.0033\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 6.8373e-04 - val_loss: 4.9377e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 5.1383e-04 - val_loss: 4.4737e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 4.5047e-04 - val_loss: 3.2517e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 4.0634e-04 - val_loss: 3.3429e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 3.7265e-04 - val_loss: 2.9788e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 3.4773e-04 - val_loss: 2.7204e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 3.3123e-04 - val_loss: 2.6346e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 3.1822e-04 - val_loss: 2.7935e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 3.0639e-04 - val_loss: 2.7796e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.9522e-04 - val_loss: 2.8280e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 2.8422e-04 - val_loss: 2.7917e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.7279e-04 - val_loss: 2.7300e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 2.6391e-04 - val_loss: 2.8871e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.5529e-04 - val_loss: 2.8822e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.4667e-04 - val_loss: 2.7911e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.4076e-04 - val_loss: 2.7719e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.3325e-04 - val_loss: 2.6720e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 2.2357e-04 - val_loss: 2.4524e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.1414e-04 - val_loss: 3.3604e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.1094e-04 - val_loss: 2.4468e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.0539e-04 - val_loss: 1.8356e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.9491e-04 - val_loss: 1.5443e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.8373e-04 - val_loss: 1.3267e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.7640e-04 - val_loss: 1.2516e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.6726e-04 - val_loss: 1.1682e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.6113e-04 - val_loss: 1.1560e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.5653e-04 - val_loss: 1.0758e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.5250e-04 - val_loss: 1.0357e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.4965e-04 - val_loss: 1.1006e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.4612e-04 - val_loss: 1.0445e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4384e-04 - val_loss: 1.0276e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.4042e-04 - val_loss: 9.8931e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3759e-04 - val_loss: 9.6954e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3592e-04 - val_loss: 9.6534e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.3422e-04 - val_loss: 9.4934e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3255e-04 - val_loss: 9.3388e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.3116e-04 - val_loss: 9.2167e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2929e-04 - val_loss: 9.1905e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2762e-04 - val_loss: 9.1866e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2563e-04 - val_loss: 9.3263e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2469e-04 - val_loss: 9.2608e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2324e-04 - val_loss: 9.3612e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.2178e-04 - val_loss: 9.3512e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.2049e-04 - val_loss: 9.1504e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.1890e-04 - val_loss: 9.0487e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.1702e-04 - val_loss: 9.4623e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 1.1664e-04 - val_loss: 8.8149e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 1.1531e-04 - val_loss: 8.5546e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 1.1434e-04 - val_loss: 8.4401e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.1357e-04 - val_loss: 8.4751e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.1268e-04 - val_loss: 8.0992e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.1155e-04 - val_loss: 8.0156e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.1113e-04 - val_loss: 8.1312e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 22s 551us/step - loss: 1.1010e-04 - val_loss: 7.9829e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.0946e-04 - val_loss: 8.2353e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.0842e-04 - val_loss: 8.4103e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 22s 551us/step - loss: 1.0753e-04 - val_loss: 7.8936e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.0711e-04 - val_loss: 7.7142e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 1.0566e-04 - val_loss: 7.9884e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.0582e-04 - val_loss: 8.0865e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.0440e-04 - val_loss: 8.0698e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.0471e-04 - val_loss: 7.7641e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.0353e-04 - val_loss: 7.9747e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 22s 551us/step - loss: 1.0319e-04 - val_loss: 7.8634e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 1.0240e-04 - val_loss: 7.6915e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.0215e-04 - val_loss: 7.5679e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 1.0127e-04 - val_loss: 7.5265e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.0038e-04 - val_loss: 7.4242e-05\n",
      "Validation RMSE:  0.008616362 \n",
      "\n",
      "Model took 1535.42 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 57s 1ms/step - loss: 0.0041 - val_loss: 0.0035\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 6.8589e-04 - val_loss: 8.1919e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 22s 551us/step - loss: 5.7498e-04 - val_loss: 5.6242e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 5.0526e-04 - val_loss: 4.3496e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 4.5390e-04 - val_loss: 4.0685e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 4.1797e-04 - val_loss: 3.6920e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 3.9022e-04 - val_loss: 3.2161e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 3.6788e-04 - val_loss: 3.2927e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 3.5537e-04 - val_loss: 2.9564e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 3.4014e-04 - val_loss: 2.6967e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 3.3225e-04 - val_loss: 2.6865e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 3.2081e-04 - val_loss: 2.7359e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 3.0995e-04 - val_loss: 2.6424e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 3.0046e-04 - val_loss: 2.7119e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 2.9182e-04 - val_loss: 2.4488e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 2.8372e-04 - val_loss: 2.2570e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 2.7620e-04 - val_loss: 2.4358e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 2.6299e-04 - val_loss: 1.9079e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 2.4519e-04 - val_loss: 2.4219e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 2.4097e-04 - val_loss: 1.8530e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.3157e-04 - val_loss: 1.4392e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.1561e-04 - val_loss: 1.4903e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.0373e-04 - val_loss: 1.8809e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.9922e-04 - val_loss: 1.2398e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.8765e-04 - val_loss: 1.6845e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.8347e-04 - val_loss: 1.3312e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.7675e-04 - val_loss: 1.2177e-04\n",
      "Epoch 29/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 529us/step - loss: 1.7159e-04 - val_loss: 1.2877e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.6810e-04 - val_loss: 1.2461e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.6469e-04 - val_loss: 1.2444e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.6219e-04 - val_loss: 1.1888e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.5955e-04 - val_loss: 1.1876e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.5707e-04 - val_loss: 1.1370e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5556e-04 - val_loss: 1.0748e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.5325e-04 - val_loss: 1.0180e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5100e-04 - val_loss: 1.0000e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.4852e-04 - val_loss: 1.0311e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.4759e-04 - val_loss: 1.0498e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.4737e-04 - val_loss: 1.0092e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4529e-04 - val_loss: 9.8032e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.4322e-04 - val_loss: 1.0733e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.4327e-04 - val_loss: 1.0986e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.4190e-04 - val_loss: 1.1227e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3905e-04 - val_loss: 1.1668e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3855e-04 - val_loss: 1.2080e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3666e-04 - val_loss: 1.2622e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3563e-04 - val_loss: 1.2060e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.3400e-04 - val_loss: 1.1935e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3366e-04 - val_loss: 1.1736e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.3225e-04 - val_loss: 1.1856e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3135e-04 - val_loss: 1.2189e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2978e-04 - val_loss: 1.1834e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2820e-04 - val_loss: 1.1197e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.2658e-04 - val_loss: 1.1241e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2528e-04 - val_loss: 1.1027e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.2448e-04 - val_loss: 1.0637e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2374e-04 - val_loss: 1.0392e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2206e-04 - val_loss: 1.0441e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2220e-04 - val_loss: 1.0366e-04\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2163e-04 - val_loss: 1.0192e-04\n",
      "Validation RMSE:  0.010095304 \n",
      "\n",
      "Model took 1339.00 seconds to train\n",
      "28 \t6     \t0.00915699\t0.000459705\t0.00857037\t0.0100953 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 73\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 748\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: Adam\n",
      "Three Lags are\n",
      " [73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (49234, 61)\n",
      "Training and test data length 39387 9847\n",
      "trainX.shape[0]:  39387\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39387, 60)\n",
      "Train X shape after np.reshape (39387, 20, 3)\n",
      "Test X shape after np.reshape (9847, 20, 3)\n",
      "Train Y Shape (39387,)\n",
      "Test Y Shape (9847,)\n",
      "Train on 39387 samples, validate on 9847 samples\n",
      "Epoch 1/70\n",
      "39387/39387 [==============================] - 56s 1ms/step - loss: 0.0178 - val_loss: 0.0118\n",
      "Epoch 2/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 3/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 7.0078e-04 - val_loss: 0.0011\n",
      "Epoch 4/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 4.6335e-04 - val_loss: 7.6346e-04\n",
      "Epoch 5/70\n",
      "39387/39387 [==============================] - 21s 537us/step - loss: 4.4439e-04 - val_loss: 4.6229e-04\n",
      "Epoch 6/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 4.4025e-04 - val_loss: 4.4181e-04\n",
      "Epoch 7/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 4.4020e-04 - val_loss: 4.4691e-04\n",
      "Epoch 8/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 4.4186e-04 - val_loss: 4.9548e-04\n",
      "Epoch 9/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 4.4179e-04 - val_loss: 5.1152e-04\n",
      "Epoch 10/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 4.3281e-04 - val_loss: 5.7229e-04\n",
      "Epoch 11/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 4.3054e-04 - val_loss: 7.1126e-04\n",
      "Epoch 12/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 4.4058e-04 - val_loss: 0.0014\n",
      "Epoch 13/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 5.6145e-04 - val_loss: 0.0061\n",
      "Epoch 14/70\n",
      "39387/39387 [==============================] - 21s 538us/step - loss: 6.5542e-04 - val_loss: 9.9612e-04\n",
      "Epoch 15/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 5.2233e-04 - val_loss: 3.6154e-04\n",
      "Epoch 16/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 4.7768e-04 - val_loss: 3.8121e-04\n",
      "Epoch 17/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 4.5700e-04 - val_loss: 7.4543e-04\n",
      "Epoch 18/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 4.6455e-04 - val_loss: 4.1715e-04\n",
      "Epoch 19/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 4.4061e-04 - val_loss: 7.4199e-04\n",
      "Epoch 20/70\n",
      "39387/39387 [==============================] - 21s 536us/step - loss: 4.3986e-04 - val_loss: 4.0491e-04\n",
      "Epoch 21/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 4.2437e-04 - val_loss: 6.3570e-04\n",
      "Epoch 22/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 4.1204e-04 - val_loss: 3.0572e-04\n",
      "Epoch 23/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 4.1400e-04 - val_loss: 4.0884e-04\n",
      "Epoch 24/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 3.9708e-04 - val_loss: 6.9949e-04\n",
      "Epoch 25/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 3.6168e-04 - val_loss: 4.6601e-04\n",
      "Epoch 26/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 3.5048e-04 - val_loss: 4.3788e-04\n",
      "Epoch 27/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 3.4373e-04 - val_loss: 4.7900e-04\n",
      "Epoch 28/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 3.4717e-04 - val_loss: 4.9397e-04\n",
      "Epoch 29/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 3.2803e-04 - val_loss: 4.1064e-04\n",
      "Epoch 30/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39387/39387 [==============================] - 21s 542us/step - loss: 3.1769e-04 - val_loss: 3.6681e-04\n",
      "Epoch 31/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 3.1453e-04 - val_loss: 3.6288e-04\n",
      "Epoch 32/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 3.0918e-04 - val_loss: 4.2675e-04\n",
      "Epoch 33/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 3.0518e-04 - val_loss: 8.8023e-04\n",
      "Epoch 34/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 3.1230e-04 - val_loss: 3.0879e-04\n",
      "Epoch 35/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 2.8778e-04 - val_loss: 4.8978e-04\n",
      "Epoch 36/70\n",
      "39387/39387 [==============================] - 21s 537us/step - loss: 2.8554e-04 - val_loss: 5.5451e-04\n",
      "Epoch 37/70\n",
      "39387/39387 [==============================] - 21s 538us/step - loss: 2.8826e-04 - val_loss: 7.1442e-04\n",
      "Epoch 38/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 2.8152e-04 - val_loss: 3.1694e-04\n",
      "Epoch 39/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 2.7116e-04 - val_loss: 4.1050e-04\n",
      "Epoch 40/70\n",
      "39387/39387 [==============================] - 21s 538us/step - loss: 2.6151e-04 - val_loss: 2.9876e-04\n",
      "Epoch 41/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 2.6439e-04 - val_loss: 4.7766e-04\n",
      "Epoch 42/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 2.5540e-04 - val_loss: 2.1806e-04\n",
      "Epoch 43/70\n",
      "39387/39387 [==============================] - 21s 537us/step - loss: 2.4329e-04 - val_loss: 2.8611e-04\n",
      "Epoch 44/70\n",
      "39387/39387 [==============================] - 21s 542us/step - loss: 2.2495e-04 - val_loss: 3.1783e-04\n",
      "Epoch 45/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 2.2195e-04 - val_loss: 8.5503e-04\n",
      "Epoch 46/70\n",
      "39387/39387 [==============================] - 21s 536us/step - loss: 2.2581e-04 - val_loss: 5.3428e-04\n",
      "Epoch 47/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 2.1735e-04 - val_loss: 4.1351e-04\n",
      "Epoch 48/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 2.1433e-04 - val_loss: 4.5649e-04\n",
      "Epoch 49/70\n",
      "39387/39387 [==============================] - 21s 542us/step - loss: 2.1246e-04 - val_loss: 4.0413e-04\n",
      "Epoch 50/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 2.0762e-04 - val_loss: 3.5970e-04\n",
      "Epoch 51/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 2.0295e-04 - val_loss: 3.2260e-04\n",
      "Epoch 52/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 1.9930e-04 - val_loss: 3.1106e-04\n",
      "Epoch 53/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 1.9766e-04 - val_loss: 2.9077e-04\n",
      "Epoch 54/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 1.9261e-04 - val_loss: 1.8776e-04\n",
      "Epoch 55/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 1.8085e-04 - val_loss: 1.6563e-04\n",
      "Epoch 56/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 1.8447e-04 - val_loss: 2.4621e-04\n",
      "Epoch 57/70\n",
      "39387/39387 [==============================] - 21s 536us/step - loss: 1.8178e-04 - val_loss: 1.6259e-04\n",
      "Epoch 58/70\n",
      "39387/39387 [==============================] - 21s 541us/step - loss: 1.6751e-04 - val_loss: 1.8113e-04\n",
      "Epoch 59/70\n",
      "39387/39387 [==============================] - 21s 538us/step - loss: 1.7715e-04 - val_loss: 3.0237e-04\n",
      "Epoch 60/70\n",
      "39387/39387 [==============================] - 21s 538us/step - loss: 1.7400e-04 - val_loss: 2.0351e-04\n",
      "Epoch 61/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 1.6677e-04 - val_loss: 2.3769e-04\n",
      "Epoch 62/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 1.7192e-04 - val_loss: 1.9403e-04\n",
      "Epoch 63/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 1.6640e-04 - val_loss: 2.0148e-04\n",
      "Epoch 64/70\n",
      "39387/39387 [==============================] - 21s 538us/step - loss: 1.6524e-04 - val_loss: 2.0384e-04\n",
      "Epoch 65/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 1.6353e-04 - val_loss: 2.0769e-04\n",
      "Epoch 66/70\n",
      "39387/39387 [==============================] - 22s 549us/step - loss: 1.6217e-04 - val_loss: 2.1980e-04\n",
      "Epoch 67/70\n",
      "39387/39387 [==============================] - 22s 547us/step - loss: 1.6301e-04 - val_loss: 2.1633e-04\n",
      "Epoch 68/70\n",
      "39387/39387 [==============================] - 21s 540us/step - loss: 1.5969e-04 - val_loss: 2.3011e-04\n",
      "Epoch 69/70\n",
      "39387/39387 [==============================] - 21s 538us/step - loss: 1.6053e-04 - val_loss: 2.2484e-04\n",
      "Epoch 70/70\n",
      "39387/39387 [==============================] - 21s 539us/step - loss: 1.5787e-04 - val_loss: 2.0992e-04\n",
      "Validation RMSE:  0.014488461 \n",
      "\n",
      "Model took 1539.88 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 57s 1ms/step - loss: 0.0050 - val_loss: 0.0039\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 0.0013 - val_loss: 0.0016\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 6.6045e-04 - val_loss: 5.2313e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 5.1729e-04 - val_loss: 3.8274e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 4.4657e-04 - val_loss: 3.5471e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 4.0606e-04 - val_loss: 3.1088e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 3.7081e-04 - val_loss: 2.8391e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 3.5082e-04 - val_loss: 2.7373e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 3.3468e-04 - val_loss: 2.6037e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.2018e-04 - val_loss: 2.7840e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 3.0722e-04 - val_loss: 2.8424e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.9453e-04 - val_loss: 2.7992e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.8612e-04 - val_loss: 2.7291e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.7406e-04 - val_loss: 2.8550e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 529us/step - loss: 2.6260e-04 - val_loss: 2.6464e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.5178e-04 - val_loss: 2.6079e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.4641e-04 - val_loss: 2.7784e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.3817e-04 - val_loss: 2.7060e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.3083e-04 - val_loss: 2.3422e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.1907e-04 - val_loss: 2.2976e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.1068e-04 - val_loss: 2.4762e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.0484e-04 - val_loss: 2.2583e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.9424e-04 - val_loss: 2.0162e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.8911e-04 - val_loss: 1.6868e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.8215e-04 - val_loss: 1.5421e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.7639e-04 - val_loss: 1.4678e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.7133e-04 - val_loss: 1.3623e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.6745e-04 - val_loss: 1.3295e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.6437e-04 - val_loss: 1.4035e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.6221e-04 - val_loss: 1.2554e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 22s 551us/step - loss: 1.5958e-04 - val_loss: 1.1628e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5674e-04 - val_loss: 1.1057e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.5299e-04 - val_loss: 1.1641e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5040e-04 - val_loss: 1.1872e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.4888e-04 - val_loss: 1.2307e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.4728e-04 - val_loss: 1.2469e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4593e-04 - val_loss: 1.2157e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.4555e-04 - val_loss: 1.1838e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.4435e-04 - val_loss: 1.2215e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.4217e-04 - val_loss: 1.1989e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.4070e-04 - val_loss: 1.1939e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.3858e-04 - val_loss: 1.2776e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.3907e-04 - val_loss: 1.2051e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3738e-04 - val_loss: 1.2260e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.3633e-04 - val_loss: 1.1852e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.3531e-04 - val_loss: 1.2375e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.3453e-04 - val_loss: 1.2006e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.3371e-04 - val_loss: 1.1693e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.3292e-04 - val_loss: 1.2141e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.3221e-04 - val_loss: 1.1960e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3176e-04 - val_loss: 1.1224e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3123e-04 - val_loss: 1.0796e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3035e-04 - val_loss: 1.0907e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.2938e-04 - val_loss: 1.1250e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2855e-04 - val_loss: 1.1336e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2747e-04 - val_loss: 9.4254e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2643e-04 - val_loss: 1.0825e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2548e-04 - val_loss: 1.1001e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.2306e-04 - val_loss: 1.0776e-04\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.2334e-04 - val_loss: 8.8666e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2171e-04 - val_loss: 1.0198e-04\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2041e-04 - val_loss: 1.0193e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2020e-04 - val_loss: 1.0823e-04\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.1886e-04 - val_loss: 1.0793e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1758e-04 - val_loss: 9.9559e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1728e-04 - val_loss: 1.0188e-04\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1624e-04 - val_loss: 1.0118e-04\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1559e-04 - val_loss: 1.0129e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1468e-04 - val_loss: 1.0306e-04\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1420e-04 - val_loss: 1.0090e-04\n",
      "Validation RMSE:  0.010045085 \n",
      "\n",
      "Model took 1518.18 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 59s 1ms/step - loss: 0.0071 - val_loss: 0.0047\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 0.0019 - val_loss: 0.0023\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 8.7078e-04 - val_loss: 9.6964e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 5.5610e-04 - val_loss: 4.5563e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 4.6355e-04 - val_loss: 3.8569e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 4.0849e-04 - val_loss: 3.9933e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 3.7563e-04 - val_loss: 3.9863e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 3.5233e-04 - val_loss: 3.1984e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 3.3874e-04 - val_loss: 2.8896e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 3.2416e-04 - val_loss: 2.5270e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 3.1119e-04 - val_loss: 2.4191e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 2.9965e-04 - val_loss: 2.2529e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 2.9120e-04 - val_loss: 2.2550e-04\n",
      "Epoch 14/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 544us/step - loss: 2.8149e-04 - val_loss: 2.1708e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.7288e-04 - val_loss: 2.1440e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 2.6503e-04 - val_loss: 2.1736e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 2.5696e-04 - val_loss: 2.3410e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 2.4865e-04 - val_loss: 2.3708e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 2.3960e-04 - val_loss: 2.3179e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 2.3319e-04 - val_loss: 2.3559e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 2.2822e-04 - val_loss: 2.2969e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 2.2513e-04 - val_loss: 2.3221e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.1954e-04 - val_loss: 2.3678e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 2.1334e-04 - val_loss: 2.3018e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 2.0751e-04 - val_loss: 2.2964e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 2.0116e-04 - val_loss: 2.3285e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.9413e-04 - val_loss: 2.1495e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.8843e-04 - val_loss: 2.0448e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.8171e-04 - val_loss: 1.6753e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.7578e-04 - val_loss: 1.5790e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.6998e-04 - val_loss: 1.5078e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.6509e-04 - val_loss: 1.5070e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.5938e-04 - val_loss: 1.3986e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.5427e-04 - val_loss: 1.2715e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.4890e-04 - val_loss: 1.1820e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.4421e-04 - val_loss: 1.1631e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.4080e-04 - val_loss: 1.2061e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.3840e-04 - val_loss: 1.1731e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.3715e-04 - val_loss: 1.1533e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.3551e-04 - val_loss: 1.1162e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3404e-04 - val_loss: 1.0800e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.3248e-04 - val_loss: 1.0951e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3088e-04 - val_loss: 1.0413e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.2944e-04 - val_loss: 1.0549e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.2840e-04 - val_loss: 9.8843e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.2716e-04 - val_loss: 9.9204e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.2589e-04 - val_loss: 1.0028e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.2479e-04 - val_loss: 1.0074e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.2292e-04 - val_loss: 9.9825e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.2186e-04 - val_loss: 9.9100e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.2091e-04 - val_loss: 1.0130e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.1977e-04 - val_loss: 9.4961e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.1857e-04 - val_loss: 9.1319e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.1676e-04 - val_loss: 8.6426e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.1649e-04 - val_loss: 8.8035e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.1510e-04 - val_loss: 8.5369e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.1381e-04 - val_loss: 8.6550e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.1396e-04 - val_loss: 8.6917e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.1219e-04 - val_loss: 8.4720e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.1228e-04 - val_loss: 7.8807e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.1026e-04 - val_loss: 7.9706e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.0875e-04 - val_loss: 7.8527e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.0838e-04 - val_loss: 7.7378e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.0784e-04 - val_loss: 7.5954e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.0742e-04 - val_loss: 7.6284e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.0649e-04 - val_loss: 7.6317e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.0558e-04 - val_loss: 7.8660e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.0552e-04 - val_loss: 7.1786e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.0510e-04 - val_loss: 7.5215e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.0447e-04 - val_loss: 7.4748e-05\n",
      "Validation RMSE:  0.008645689 \n",
      "\n",
      "Model took 1542.98 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 58s 1ms/step - loss: 0.0057 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 6.8066e-04 - val_loss: 7.4076e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 5.1877e-04 - val_loss: 4.9113e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 4.6030e-04 - val_loss: 3.6730e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 4.1153e-04 - val_loss: 3.9059e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 3.7048e-04 - val_loss: 3.6337e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 3.4728e-04 - val_loss: 3.8198e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 3.3477e-04 - val_loss: 2.8684e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 3.2186e-04 - val_loss: 2.9643e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 3.0961e-04 - val_loss: 2.8765e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.9884e-04 - val_loss: 2.8933e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.8516e-04 - val_loss: 2.6714e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 2.7392e-04 - val_loss: 3.0637e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.6359e-04 - val_loss: 2.9510e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.5534e-04 - val_loss: 2.7081e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.4766e-04 - val_loss: 2.4576e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 2.3620e-04 - val_loss: 2.3816e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.3028e-04 - val_loss: 1.8937e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 2.2504e-04 - val_loss: 1.8396e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.1678e-04 - val_loss: 1.6626e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.0826e-04 - val_loss: 1.6672e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 2.0233e-04 - val_loss: 1.5386e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.9675e-04 - val_loss: 1.3257e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.8957e-04 - val_loss: 1.1707e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.8452e-04 - val_loss: 1.2046e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.7987e-04 - val_loss: 1.0120e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.7635e-04 - val_loss: 1.0893e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.7283e-04 - val_loss: 1.0153e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.7008e-04 - val_loss: 1.0346e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.6673e-04 - val_loss: 1.0044e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.6323e-04 - val_loss: 9.5851e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.6057e-04 - val_loss: 9.8695e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.5727e-04 - val_loss: 9.7895e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.5473e-04 - val_loss: 9.8980e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.5278e-04 - val_loss: 9.7909e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5089e-04 - val_loss: 1.0229e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.4899e-04 - val_loss: 9.7318e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4749e-04 - val_loss: 9.7786e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.4556e-04 - val_loss: 1.0052e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.4425e-04 - val_loss: 9.1613e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4223e-04 - val_loss: 9.3030e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.4018e-04 - val_loss: 9.1112e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3880e-04 - val_loss: 8.7521e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3662e-04 - val_loss: 8.6179e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3487e-04 - val_loss: 8.6575e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.3329e-04 - val_loss: 8.4380e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.3173e-04 - val_loss: 8.2442e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.3021e-04 - val_loss: 8.2594e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.2884e-04 - val_loss: 8.1615e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.2749e-04 - val_loss: 8.0516e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2648e-04 - val_loss: 8.0390e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.2549e-04 - val_loss: 8.0563e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2400e-04 - val_loss: 8.1525e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 531us/step - loss: 1.2267e-04 - val_loss: 8.1218e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.2132e-04 - val_loss: 7.6963e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1969e-04 - val_loss: 7.6395e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1907e-04 - val_loss: 7.8615e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1766e-04 - val_loss: 7.7865e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1645e-04 - val_loss: 7.7914e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1511e-04 - val_loss: 7.7519e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1447e-04 - val_loss: 7.8745e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1367e-04 - val_loss: 8.0088e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1303e-04 - val_loss: 7.9896e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.1116e-04 - val_loss: 8.0381e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.1024e-04 - val_loss: 8.0687e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.0944e-04 - val_loss: 8.2073e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 532us/step - loss: 1.0812e-04 - val_loss: 8.0630e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 530us/step - loss: 1.0775e-04 - val_loss: 7.9969e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 533us/step - loss: 1.0673e-04 - val_loss: 8.1103e-05\n",
      "Validation RMSE:  0.009005702 \n",
      "\n",
      "Model took 1517.36 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 58s 1ms/step - loss: 0.0065 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 7.3807e-04 - val_loss: 0.0011\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 5.6585e-04 - val_loss: 4.2147e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 4.8031e-04 - val_loss: 3.4921e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 4.2918e-04 - val_loss: 2.8261e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.8928e-04 - val_loss: 2.6668e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 3.6540e-04 - val_loss: 2.4616e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.4763e-04 - val_loss: 2.2892e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 3.3367e-04 - val_loss: 2.2985e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 3.1890e-04 - val_loss: 2.4131e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 3.1033e-04 - val_loss: 2.3681e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.0145e-04 - val_loss: 2.2951e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 2.9163e-04 - val_loss: 2.2485e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.8117e-04 - val_loss: 2.2564e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.7165e-04 - val_loss: 2.4230e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.6293e-04 - val_loss: 2.2819e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.5409e-04 - val_loss: 2.2184e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.4781e-04 - val_loss: 2.2334e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.4054e-04 - val_loss: 2.1966e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 2.2861e-04 - val_loss: 1.7849e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 2.1673e-04 - val_loss: 1.7192e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.1133e-04 - val_loss: 1.3235e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.0168e-04 - val_loss: 1.5507e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.9495e-04 - val_loss: 1.2513e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.8929e-04 - val_loss: 1.1517e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 22s 559us/step - loss: 1.8338e-04 - val_loss: 1.0451e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.7645e-04 - val_loss: 1.0424e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.7240e-04 - val_loss: 1.0492e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 22s 558us/step - loss: 1.6882e-04 - val_loss: 1.0338e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.6598e-04 - val_loss: 1.0496e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 22s 558us/step - loss: 1.6300e-04 - val_loss: 9.6916e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.5894e-04 - val_loss: 1.0767e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.5874e-04 - val_loss: 1.0570e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.5545e-04 - val_loss: 1.0757e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.5192e-04 - val_loss: 1.0867e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.5027e-04 - val_loss: 1.0002e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.4838e-04 - val_loss: 9.6997e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.4677e-04 - val_loss: 9.1524e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.4493e-04 - val_loss: 9.0697e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.4297e-04 - val_loss: 9.1132e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.4120e-04 - val_loss: 9.2242e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 22s 558us/step - loss: 1.3904e-04 - val_loss: 9.3601e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.3706e-04 - val_loss: 9.7935e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.3476e-04 - val_loss: 9.3566e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.3176e-04 - val_loss: 1.0442e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 1.2989e-04 - val_loss: 9.5017e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.2877e-04 - val_loss: 9.1206e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.2787e-04 - val_loss: 8.3608e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 1.2591e-04 - val_loss: 8.3290e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.2363e-04 - val_loss: 8.2397e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 22s 559us/step - loss: 1.2267e-04 - val_loss: 8.0654e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 1.2188e-04 - val_loss: 8.1954e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.1976e-04 - val_loss: 7.9663e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.1765e-04 - val_loss: 8.1002e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.1697e-04 - val_loss: 9.2346e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 22s 558us/step - loss: 1.1541e-04 - val_loss: 8.4712e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.1408e-04 - val_loss: 8.5680e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.1309e-04 - val_loss: 8.4712e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.1199e-04 - val_loss: 8.4708e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.1025e-04 - val_loss: 8.6372e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.0922e-04 - val_loss: 8.3660e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.0749e-04 - val_loss: 8.3793e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.0541e-04 - val_loss: 8.3765e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.0644e-04 - val_loss: 8.2353e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.0567e-04 - val_loss: 8.3466e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 1.0527e-04 - val_loss: 8.3795e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.0455e-04 - val_loss: 8.7615e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.0442e-04 - val_loss: 9.0520e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.0374e-04 - val_loss: 9.2122e-05\n",
      "Validation RMSE:  0.009598017 \n",
      "\n",
      "Model took 1560.81 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 4\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 16 20 19 8\n",
      "1st Window Start: 15\n",
      "2nd Window Start: 502\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Four Lags are\n",
      " [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 1, 1, 1, 1, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Lags length:\n",
      " 80\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48861, 81)\n",
      "Training and test data length 39088 9773\n",
      "trainX.shape[0]:  39088\n",
      "window_length:  20\n",
      "no_wins:  4\n",
      "trainX shape:\n",
      " (39088, 80)\n",
      "Train X shape after np.reshape (39088, 20, 4)\n",
      "Test X shape after np.reshape (9773, 20, 4)\n",
      "Train Y Shape (39088,)\n",
      "Test Y Shape (9773,)\n",
      "Train on 39088 samples, validate on 9773 samples\n",
      "Epoch 1/70\n",
      "39088/39088 [==============================] - 60s 2ms/step - loss: 0.0028 - val_loss: 3.8686e-04\n",
      "Epoch 2/70\n",
      "39088/39088 [==============================] - 22s 561us/step - loss: 7.0601e-04 - val_loss: 5.2215e-04\n",
      "Epoch 3/70\n",
      "39088/39088 [==============================] - 22s 561us/step - loss: 6.1676e-04 - val_loss: 5.1544e-04\n",
      "Epoch 4/70\n",
      "39088/39088 [==============================] - 22s 562us/step - loss: 5.8036e-04 - val_loss: 5.1877e-04\n",
      "Epoch 5/70\n",
      "39088/39088 [==============================] - 22s 557us/step - loss: 5.4163e-04 - val_loss: 5.0070e-04\n",
      "Epoch 6/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 5.1041e-04 - val_loss: 4.7895e-04\n",
      "Epoch 7/70\n",
      "39088/39088 [==============================] - 22s 561us/step - loss: 4.7629e-04 - val_loss: 4.2596e-04\n",
      "Epoch 8/70\n",
      "39088/39088 [==============================] - 22s 556us/step - loss: 4.5873e-04 - val_loss: 3.8905e-04\n",
      "Epoch 9/70\n",
      "39088/39088 [==============================] - 22s 555us/step - loss: 4.4361e-04 - val_loss: 3.4582e-04\n",
      "Epoch 10/70\n",
      "39088/39088 [==============================] - 22s 558us/step - loss: 4.2531e-04 - val_loss: 3.8855e-04\n",
      "Epoch 11/70\n",
      "39088/39088 [==============================] - 22s 558us/step - loss: 4.0585e-04 - val_loss: 4.0555e-04\n",
      "Epoch 12/70\n",
      "39088/39088 [==============================] - 22s 556us/step - loss: 3.8764e-04 - val_loss: 3.3243e-04\n",
      "Epoch 13/70\n",
      "39088/39088 [==============================] - 22s 558us/step - loss: 3.7857e-04 - val_loss: 2.4939e-04\n",
      "Epoch 14/70\n",
      "39088/39088 [==============================] - 22s 558us/step - loss: 3.5439e-04 - val_loss: 3.3737e-04\n",
      "Epoch 15/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 3.4931e-04 - val_loss: 3.2175e-04\n",
      "Epoch 16/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 3.3332e-04 - val_loss: 3.2263e-04\n",
      "Epoch 17/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 3.1954e-04 - val_loss: 2.4439e-04\n",
      "Epoch 18/70\n",
      "39088/39088 [==============================] - 22s 560us/step - loss: 3.0476e-04 - val_loss: 2.6731e-04\n",
      "Epoch 19/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 3.0095e-04 - val_loss: 2.3254e-04\n",
      "Epoch 20/70\n",
      "39088/39088 [==============================] - 22s 560us/step - loss: 2.9315e-04 - val_loss: 2.2821e-04\n",
      "Epoch 21/70\n",
      "39088/39088 [==============================] - 22s 558us/step - loss: 2.9027e-04 - val_loss: 2.1585e-04\n",
      "Epoch 22/70\n",
      "39088/39088 [==============================] - 22s 558us/step - loss: 2.8365e-04 - val_loss: 2.3142e-04\n",
      "Epoch 23/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 2.7961e-04 - val_loss: 2.1707e-04\n",
      "Epoch 24/70\n",
      "39088/39088 [==============================] - 22s 560us/step - loss: 2.7274e-04 - val_loss: 2.3611e-04\n",
      "Epoch 25/70\n",
      "39088/39088 [==============================] - 22s 561us/step - loss: 2.6954e-04 - val_loss: 2.2920e-04\n",
      "Epoch 26/70\n",
      "39088/39088 [==============================] - 22s 556us/step - loss: 2.6462e-04 - val_loss: 2.3233e-04\n",
      "Epoch 27/70\n",
      "39088/39088 [==============================] - 22s 558us/step - loss: 2.6124e-04 - val_loss: 2.5902e-04\n",
      "Epoch 28/70\n",
      "39088/39088 [==============================] - 22s 560us/step - loss: 2.5630e-04 - val_loss: 2.3868e-04\n",
      "Epoch 29/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 2.5277e-04 - val_loss: 2.2423e-04\n",
      "Epoch 30/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 2.5015e-04 - val_loss: 2.3347e-04\n",
      "Epoch 31/70\n",
      "39088/39088 [==============================] - 22s 562us/step - loss: 2.4355e-04 - val_loss: 2.3816e-04\n",
      "Epoch 32/70\n",
      "39088/39088 [==============================] - 22s 556us/step - loss: 2.4182e-04 - val_loss: 2.5780e-04\n",
      "Epoch 33/70\n",
      "39088/39088 [==============================] - 22s 558us/step - loss: 2.3794e-04 - val_loss: 2.6513e-04\n",
      "Epoch 34/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 2.3667e-04 - val_loss: 2.6311e-04\n",
      "Epoch 35/70\n",
      "39088/39088 [==============================] - 22s 557us/step - loss: 2.3283e-04 - val_loss: 2.4567e-04\n",
      "Epoch 36/70\n",
      "39088/39088 [==============================] - 22s 556us/step - loss: 2.2876e-04 - val_loss: 2.4758e-04\n",
      "Epoch 37/70\n",
      "39088/39088 [==============================] - 22s 557us/step - loss: 2.2476e-04 - val_loss: 2.3243e-04\n",
      "Epoch 38/70\n",
      "39088/39088 [==============================] - 22s 560us/step - loss: 2.2317e-04 - val_loss: 2.4243e-04\n",
      "Epoch 39/70\n",
      "39088/39088 [==============================] - 22s 555us/step - loss: 2.1904e-04 - val_loss: 2.3066e-04\n",
      "Epoch 40/70\n",
      "39088/39088 [==============================] - 22s 558us/step - loss: 2.1729e-04 - val_loss: 2.3866e-04\n",
      "Epoch 41/70\n",
      "39088/39088 [==============================] - 22s 559us/step - loss: 2.1461e-04 - val_loss: 2.4818e-04\n",
      "Validation RMSE:  0.015753627 \n",
      "\n",
      "Model took 950.35 seconds to train\n",
      "29 \t6     \t0.0102573 \t0.0024907  \t0.00857037\t0.0157536 \n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 61s 2ms/step - loss: 0.0074 - val_loss: 0.0046\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 7.8009e-04 - val_loss: 8.8947e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 5.5483e-04 - val_loss: 4.9326e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 4.8107e-04 - val_loss: 4.1913e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 4.2871e-04 - val_loss: 3.4061e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 3.9488e-04 - val_loss: 3.0466e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 3.7368e-04 - val_loss: 2.9040e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 3.5411e-04 - val_loss: 2.7490e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 3.3815e-04 - val_loss: 2.6488e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 3.2386e-04 - val_loss: 2.4311e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 3.1378e-04 - val_loss: 2.3726e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 3.0262e-04 - val_loss: 2.3004e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 2.9248e-04 - val_loss: 2.3080e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 2.8818e-04 - val_loss: 2.2890e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 2.7571e-04 - val_loss: 2.1905e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 2.7405e-04 - val_loss: 2.2241e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 2.6459e-04 - val_loss: 2.0900e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 2.5938e-04 - val_loss: 1.9666e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 2.5366e-04 - val_loss: 1.8911e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 22s 559us/step - loss: 2.4785e-04 - val_loss: 1.9150e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 2.4045e-04 - val_loss: 1.7237e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 2.3475e-04 - val_loss: 1.8874e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 2.2999e-04 - val_loss: 1.9812e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 22s 558us/step - loss: 2.2593e-04 - val_loss: 1.8697e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 2.1917e-04 - val_loss: 1.8161e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 2.0941e-04 - val_loss: 1.7433e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 2.0276e-04 - val_loss: 1.7125e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 1.9820e-04 - val_loss: 1.8646e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.9390e-04 - val_loss: 1.7591e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.8616e-04 - val_loss: 1.8985e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.8139e-04 - val_loss: 1.6201e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.7505e-04 - val_loss: 1.5211e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.6540e-04 - val_loss: 1.5470e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 22s 551us/step - loss: 1.5967e-04 - val_loss: 1.1346e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.5298e-04 - val_loss: 1.0076e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.5019e-04 - val_loss: 9.7046e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.4724e-04 - val_loss: 9.9754e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.4461e-04 - val_loss: 1.0056e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.4238e-04 - val_loss: 9.0095e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.4029e-04 - val_loss: 8.6672e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.3510e-04 - val_loss: 1.1381e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.3461e-04 - val_loss: 1.0471e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 1.3475e-04 - val_loss: 9.3271e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.3112e-04 - val_loss: 9.8660e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.3084e-04 - val_loss: 9.1442e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.2851e-04 - val_loss: 1.0845e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 22s 557us/step - loss: 1.2808e-04 - val_loss: 1.1275e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 22s 560us/step - loss: 1.2853e-04 - val_loss: 1.0802e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.2832e-04 - val_loss: 1.0388e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.2460e-04 - val_loss: 1.1087e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.2414e-04 - val_loss: 1.1132e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.2241e-04 - val_loss: 1.1948e-04\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.2146e-04 - val_loss: 1.0598e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 1.2012e-04 - val_loss: 1.1178e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 22s 558us/step - loss: 1.1824e-04 - val_loss: 1.0408e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.1701e-04 - val_loss: 9.8378e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 22s 555us/step - loss: 1.1520e-04 - val_loss: 1.0392e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.1424e-04 - val_loss: 9.9305e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 22s 554us/step - loss: 1.1321e-04 - val_loss: 9.7791e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 1.1169e-04 - val_loss: 9.7428e-05\n",
      "Validation RMSE:  0.009870557 \n",
      "\n",
      "Model took 1381.82 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 61s 2ms/step - loss: 0.0059 - val_loss: 0.0042\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 22s 551us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 6.7448e-04 - val_loss: 5.8367e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 5.0880e-04 - val_loss: 4.2938e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 22s 553us/step - loss: 4.4616e-04 - val_loss: 3.7633e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 4.0072e-04 - val_loss: 3.4145e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 22s 556us/step - loss: 3.7503e-04 - val_loss: 3.1267e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 3.5146e-04 - val_loss: 2.9451e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 22s 552us/step - loss: 3.3445e-04 - val_loss: 2.8046e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 3.1844e-04 - val_loss: 2.8041e-04\n",
      "Epoch 11/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 546us/step - loss: 3.0579e-04 - val_loss: 2.7082e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.9739e-04 - val_loss: 2.7336e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.8819e-04 - val_loss: 2.6696e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 2.8037e-04 - val_loss: 2.4569e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.7028e-04 - val_loss: 2.4783e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 2.6295e-04 - val_loss: 2.2041e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 2.5551e-04 - val_loss: 2.1710e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 2.4887e-04 - val_loss: 1.9874e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.4065e-04 - val_loss: 2.2068e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.3405e-04 - val_loss: 2.0790e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.2626e-04 - val_loss: 2.0982e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 2.1916e-04 - val_loss: 1.8282e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.1444e-04 - val_loss: 1.8199e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 2.0231e-04 - val_loss: 1.6439e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.9212e-04 - val_loss: 1.5976e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.8479e-04 - val_loss: 1.5965e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.7873e-04 - val_loss: 1.3757e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.7204e-04 - val_loss: 1.3461e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.6774e-04 - val_loss: 1.2742e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.6295e-04 - val_loss: 1.1504e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5915e-04 - val_loss: 1.1719e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.5638e-04 - val_loss: 1.1304e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5443e-04 - val_loss: 1.0601e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.5246e-04 - val_loss: 1.0452e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.5128e-04 - val_loss: 1.0322e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.4918e-04 - val_loss: 1.0284e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.4730e-04 - val_loss: 1.0025e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.4533e-04 - val_loss: 9.9053e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.4377e-04 - val_loss: 9.8432e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.4212e-04 - val_loss: 9.8372e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.4049e-04 - val_loss: 9.8865e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3896e-04 - val_loss: 1.0021e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3716e-04 - val_loss: 9.7490e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.3587e-04 - val_loss: 9.6758e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.3419e-04 - val_loss: 9.6223e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.3241e-04 - val_loss: 9.5487e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3116e-04 - val_loss: 9.4396e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.2961e-04 - val_loss: 9.6467e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.2818e-04 - val_loss: 9.6242e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2681e-04 - val_loss: 9.6048e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.2514e-04 - val_loss: 9.6311e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.2377e-04 - val_loss: 9.7341e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.2260e-04 - val_loss: 9.7487e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.2142e-04 - val_loss: 9.7998e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.2005e-04 - val_loss: 9.9017e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1898e-04 - val_loss: 9.5868e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1789e-04 - val_loss: 9.4911e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.1625e-04 - val_loss: 9.3797e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1630e-04 - val_loss: 9.7052e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.1546e-04 - val_loss: 9.7750e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1396e-04 - val_loss: 9.7652e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1435e-04 - val_loss: 9.7784e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.1191e-04 - val_loss: 9.3120e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.1209e-04 - val_loss: 9.6030e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.1056e-04 - val_loss: 9.5566e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.1066e-04 - val_loss: 9.6125e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.0972e-04 - val_loss: 9.5399e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.0949e-04 - val_loss: 1.0032e-04\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.0792e-04 - val_loss: 9.7429e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 534us/step - loss: 1.0645e-04 - val_loss: 9.7919e-05\n",
      "Validation RMSE:  0.00989539 \n",
      "\n",
      "Model took 1534.78 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 60s 2ms/step - loss: 0.0042 - val_loss: 0.0029\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 6.8135e-04 - val_loss: 5.8876e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 5.5837e-04 - val_loss: 4.9148e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 4.8392e-04 - val_loss: 3.8437e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 4.2718e-04 - val_loss: 3.9563e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 3.9159e-04 - val_loss: 3.1882e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 3.6630e-04 - val_loss: 2.8116e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 3.4837e-04 - val_loss: 2.8031e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 3.3307e-04 - val_loss: 2.8614e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 3.1926e-04 - val_loss: 2.8416e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 3.0886e-04 - val_loss: 2.6468e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 2.9753e-04 - val_loss: 2.5062e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 2.8637e-04 - val_loss: 2.5994e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 2.7666e-04 - val_loss: 2.4036e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 2.6841e-04 - val_loss: 2.3371e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 2.5964e-04 - val_loss: 2.3579e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 2.5208e-04 - val_loss: 2.1728e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 2.4260e-04 - val_loss: 2.2251e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 2.3507e-04 - val_loss: 2.0607e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 2.2282e-04 - val_loss: 1.8288e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 2.2427e-04 - val_loss: 1.9101e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 2.1470e-04 - val_loss: 1.8397e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 2.0438e-04 - val_loss: 1.9738e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.9321e-04 - val_loss: 1.7400e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.8337e-04 - val_loss: 1.7377e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.7947e-04 - val_loss: 1.6993e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.7372e-04 - val_loss: 1.6733e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.6871e-04 - val_loss: 1.6156e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.6748e-04 - val_loss: 1.5400e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.6246e-04 - val_loss: 1.4213e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.5971e-04 - val_loss: 1.3451e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.5918e-04 - val_loss: 1.3016e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 549us/step - loss: 1.5553e-04 - val_loss: 1.2922e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.5296e-04 - val_loss: 1.2426e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.4995e-04 - val_loss: 1.2372e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.4692e-04 - val_loss: 1.2637e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.4516e-04 - val_loss: 1.2400e-04\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.4177e-04 - val_loss: 1.2019e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.3936e-04 - val_loss: 1.2201e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.3823e-04 - val_loss: 1.1711e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.3714e-04 - val_loss: 1.1918e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.3572e-04 - val_loss: 1.2301e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.3441e-04 - val_loss: 1.2296e-04\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.3271e-04 - val_loss: 1.2067e-04\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.3173e-04 - val_loss: 1.1582e-04\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.3052e-04 - val_loss: 1.1106e-04\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.2915e-04 - val_loss: 1.1100e-04\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.2722e-04 - val_loss: 1.0584e-04\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.2577e-04 - val_loss: 1.0527e-04\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.2472e-04 - val_loss: 1.0332e-04\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.2376e-04 - val_loss: 1.0127e-04\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.2304e-04 - val_loss: 9.9210e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.2190e-04 - val_loss: 1.0462e-04\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.2080e-04 - val_loss: 1.0011e-04\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.1963e-04 - val_loss: 1.0213e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.1869e-04 - val_loss: 9.7297e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.1754e-04 - val_loss: 9.4103e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 22s 550us/step - loss: 1.1647e-04 - val_loss: 9.4663e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.1588e-04 - val_loss: 9.5612e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.1556e-04 - val_loss: 9.6257e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.1422e-04 - val_loss: 9.5978e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.1326e-04 - val_loss: 9.3893e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 548us/step - loss: 1.1230e-04 - val_loss: 9.2446e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 1.1126e-04 - val_loss: 9.1807e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.1069e-04 - val_loss: 8.4595e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 22s 549us/step - loss: 1.0994e-04 - val_loss: 9.1322e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.0885e-04 - val_loss: 9.4551e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.0781e-04 - val_loss: 9.8050e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 547us/step - loss: 1.0691e-04 - val_loss: 9.9113e-05\n",
      "Validation RMSE:  0.009955529 \n",
      "\n",
      "Model took 1557.07 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 60s 2ms/step - loss: 0.0048 - val_loss: 0.0040\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 7.5984e-04 - val_loss: 7.5055e-04\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 5.8604e-04 - val_loss: 4.1268e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 5.0676e-04 - val_loss: 3.2502e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 4.5528e-04 - val_loss: 3.2241e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 4.1761e-04 - val_loss: 2.8809e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 3.9113e-04 - val_loss: 2.6934e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 3.7236e-04 - val_loss: 2.7850e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 3.5493e-04 - val_loss: 2.8598e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 3.4262e-04 - val_loss: 2.7952e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.2806e-04 - val_loss: 2.8806e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 3.1252e-04 - val_loss: 2.9987e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 3.0105e-04 - val_loss: 2.8025e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.9032e-04 - val_loss: 2.6972e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.8065e-04 - val_loss: 2.6957e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 2.7203e-04 - val_loss: 2.8615e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 2.6373e-04 - val_loss: 2.5868e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.5315e-04 - val_loss: 2.6926e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.4420e-04 - val_loss: 2.0267e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 2.3406e-04 - val_loss: 2.0077e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.2143e-04 - val_loss: 1.3923e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 2.1261e-04 - val_loss: 1.4327e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.0192e-04 - val_loss: 1.2370e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.9236e-04 - val_loss: 1.1545e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.8928e-04 - val_loss: 1.0742e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.8434e-04 - val_loss: 1.0677e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.8068e-04 - val_loss: 1.0696e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.7677e-04 - val_loss: 1.0542e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.7376e-04 - val_loss: 1.0223e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.6946e-04 - val_loss: 1.0115e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.6878e-04 - val_loss: 1.0463e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.6443e-04 - val_loss: 1.0392e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.6123e-04 - val_loss: 1.0370e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.5962e-04 - val_loss: 1.0471e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.5691e-04 - val_loss: 1.0199e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.5217e-04 - val_loss: 1.0279e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.5278e-04 - val_loss: 9.8274e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.5065e-04 - val_loss: 9.3464e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.4933e-04 - val_loss: 9.1795e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.4785e-04 - val_loss: 9.1434e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.4649e-04 - val_loss: 8.9865e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.4489e-04 - val_loss: 8.8195e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.4334e-04 - val_loss: 8.6821e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.4146e-04 - val_loss: 8.6244e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.4029e-04 - val_loss: 8.3523e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.3893e-04 - val_loss: 8.6062e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.3737e-04 - val_loss: 8.4377e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3616e-04 - val_loss: 8.5126e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.3452e-04 - val_loss: 8.2954e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.3252e-04 - val_loss: 8.0502e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.3211e-04 - val_loss: 8.0414e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.3161e-04 - val_loss: 8.0123e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 549us/step - loss: 1.3117e-04 - val_loss: 8.0665e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3031e-04 - val_loss: 8.0148e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.2889e-04 - val_loss: 7.7924e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2861e-04 - val_loss: 7.8197e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.2758e-04 - val_loss: 7.6892e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.2710e-04 - val_loss: 8.0217e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.2536e-04 - val_loss: 7.9158e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.2488e-04 - val_loss: 8.0021e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.2300e-04 - val_loss: 7.8515e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.2265e-04 - val_loss: 8.3081e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.2247e-04 - val_loss: 8.2013e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.2105e-04 - val_loss: 8.4468e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.1965e-04 - val_loss: 8.3677e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.2016e-04 - val_loss: 8.3794e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.1784e-04 - val_loss: 8.2743e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.1719e-04 - val_loss: 8.1145e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.1730e-04 - val_loss: 7.9841e-05\n",
      "Validation RMSE:  0.008935389 \n",
      "\n",
      "Model took 1541.91 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 70\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 28 20 3 8\n",
      "1st Window Start: 15\n",
      "2nd Window Start: 502\n",
      "3rd Window Start: 940\n",
      "4th Window Start: 1388\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 1, 1, 1, 1, 1, 1, 1, 1, 940, 941, 942, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Lags length:\n",
      " 84\n",
      "Length Used:\n",
      " 28\n",
      "New Dataset shape after Lags: (49058, 85)\n",
      "Training and test data length 39246 9812\n",
      "trainX.shape[0]:  39246\n",
      "window_length:  28\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39246, 84)\n",
      "Train X shape after np.reshape (39246, 28, 3)\n",
      "Test X shape after np.reshape (9812, 28, 3)\n",
      "Train Y Shape (39246,)\n",
      "Test Y Shape (9812,)\n",
      "Train on 39246 samples, validate on 9812 samples\n",
      "Epoch 1/70\n",
      "39246/39246 [==============================] - 69s 2ms/step - loss: 0.0035 - val_loss: 4.5965e-04\n",
      "Epoch 2/70\n",
      "39246/39246 [==============================] - 28s 710us/step - loss: 8.4895e-04 - val_loss: 4.2715e-04\n",
      "Epoch 3/70\n",
      "39246/39246 [==============================] - 28s 711us/step - loss: 7.3647e-04 - val_loss: 4.2136e-04\n",
      "Epoch 4/70\n",
      "39246/39246 [==============================] - 28s 709us/step - loss: 6.7387e-04 - val_loss: 4.0937e-04\n",
      "Epoch 5/70\n",
      "39246/39246 [==============================] - 28s 713us/step - loss: 6.3865e-04 - val_loss: 3.9893e-04\n",
      "Epoch 6/70\n",
      "39246/39246 [==============================] - 28s 711us/step - loss: 6.1286e-04 - val_loss: 3.8536e-04\n",
      "Epoch 7/70\n",
      "39246/39246 [==============================] - 28s 710us/step - loss: 5.8596e-04 - val_loss: 3.5064e-04\n",
      "Epoch 8/70\n",
      "39246/39246 [==============================] - 28s 709us/step - loss: 5.4781e-04 - val_loss: 3.2419e-04\n",
      "Epoch 9/70\n",
      "39246/39246 [==============================] - 28s 708us/step - loss: 5.2657e-04 - val_loss: 3.1438e-04\n",
      "Epoch 10/70\n",
      "39246/39246 [==============================] - 28s 714us/step - loss: 5.0868e-04 - val_loss: 3.1180e-04\n",
      "Epoch 11/70\n",
      "39246/39246 [==============================] - 28s 712us/step - loss: 4.9680e-04 - val_loss: 3.1147e-04\n",
      "Epoch 12/70\n",
      "39246/39246 [==============================] - 28s 713us/step - loss: 4.8624e-04 - val_loss: 3.2301e-04\n",
      "Epoch 13/70\n",
      "39246/39246 [==============================] - 28s 711us/step - loss: 4.7632e-04 - val_loss: 3.3921e-04\n",
      "Epoch 14/70\n",
      "39246/39246 [==============================] - 28s 708us/step - loss: 4.6667e-04 - val_loss: 3.3577e-04\n",
      "Epoch 15/70\n",
      "39246/39246 [==============================] - 28s 710us/step - loss: 4.5921e-04 - val_loss: 3.2402e-04\n",
      "Epoch 16/70\n",
      "39246/39246 [==============================] - 28s 711us/step - loss: 4.4462e-04 - val_loss: 3.4512e-04\n",
      "Epoch 17/70\n",
      "39246/39246 [==============================] - 28s 713us/step - loss: 4.2524e-04 - val_loss: 4.1094e-04\n",
      "Epoch 18/70\n",
      "39246/39246 [==============================] - 28s 712us/step - loss: 4.0521e-04 - val_loss: 5.5534e-04\n",
      "Epoch 19/70\n",
      "39246/39246 [==============================] - 28s 711us/step - loss: 3.9085e-04 - val_loss: 6.0777e-04\n",
      "Epoch 20/70\n",
      "39246/39246 [==============================] - 28s 716us/step - loss: 3.7689e-04 - val_loss: 6.8150e-04\n",
      "Epoch 21/70\n",
      "39246/39246 [==============================] - 28s 712us/step - loss: 3.6111e-04 - val_loss: 5.2833e-04\n",
      "Epoch 22/70\n",
      "39246/39246 [==============================] - 28s 713us/step - loss: 3.4935e-04 - val_loss: 5.2474e-04\n",
      "Epoch 23/70\n",
      "39246/39246 [==============================] - 28s 712us/step - loss: 3.3754e-04 - val_loss: 5.4458e-04\n",
      "Epoch 24/70\n",
      "39246/39246 [==============================] - 28s 710us/step - loss: 3.2896e-04 - val_loss: 5.4384e-04\n",
      "Epoch 25/70\n",
      "39246/39246 [==============================] - 28s 708us/step - loss: 3.2008e-04 - val_loss: 5.4535e-04\n",
      "Epoch 26/70\n",
      "39246/39246 [==============================] - 28s 715us/step - loss: 3.1592e-04 - val_loss: 5.5692e-04\n",
      "Epoch 27/70\n",
      "39246/39246 [==============================] - 28s 713us/step - loss: 3.1274e-04 - val_loss: 5.4524e-04\n",
      "Epoch 28/70\n",
      "39246/39246 [==============================] - 28s 708us/step - loss: 3.0328e-04 - val_loss: 5.5336e-04\n",
      "Epoch 29/70\n",
      "39246/39246 [==============================] - 28s 711us/step - loss: 3.0016e-04 - val_loss: 6.3361e-04\n",
      "Epoch 30/70\n",
      "39246/39246 [==============================] - 28s 714us/step - loss: 2.9653e-04 - val_loss: 6.7784e-04\n",
      "Epoch 31/70\n",
      "39246/39246 [==============================] - 28s 713us/step - loss: 2.9233e-04 - val_loss: 7.0183e-04\n",
      "Validation RMSE:  0.026492141 \n",
      "\n",
      "Model took 925.16 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 62s 2ms/step - loss: 0.0058 - val_loss: 0.0048\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 0.0018 - val_loss: 0.0021\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 8.3997e-04 - val_loss: 0.0011\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 5.9705e-04 - val_loss: 5.9957e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 5.0842e-04 - val_loss: 4.4410e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 4.5141e-04 - val_loss: 3.9960e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 4.0962e-04 - val_loss: 3.2946e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.7547e-04 - val_loss: 3.5442e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 3.5311e-04 - val_loss: 3.3111e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.3352e-04 - val_loss: 3.1102e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.1723e-04 - val_loss: 2.8930e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.0350e-04 - val_loss: 2.6680e-04\n",
      "Epoch 13/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.8732e-04 - val_loss: 2.7745e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.7139e-04 - val_loss: 2.3916e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.5565e-04 - val_loss: 2.2677e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.4408e-04 - val_loss: 2.6215e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.3820e-04 - val_loss: 2.2307e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 2.2914e-04 - val_loss: 1.7796e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.2371e-04 - val_loss: 1.6675e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 2.1462e-04 - val_loss: 1.5473e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 2.0698e-04 - val_loss: 1.4362e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.0051e-04 - val_loss: 1.2999e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.9300e-04 - val_loss: 1.1396e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.8642e-04 - val_loss: 1.0933e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.8143e-04 - val_loss: 1.1202e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.7683e-04 - val_loss: 1.0455e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.7300e-04 - val_loss: 1.0331e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.6922e-04 - val_loss: 1.0080e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.6576e-04 - val_loss: 9.3204e-05\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.6315e-04 - val_loss: 9.2755e-05\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.6006e-04 - val_loss: 9.1429e-05\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.5656e-04 - val_loss: 9.3650e-05\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.5405e-04 - val_loss: 9.0949e-05\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.5165e-04 - val_loss: 8.9022e-05\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.4894e-04 - val_loss: 8.8511e-05\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.4818e-04 - val_loss: 9.5461e-05\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 535us/step - loss: 1.4668e-04 - val_loss: 9.1932e-05\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.4478e-04 - val_loss: 9.2516e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.4355e-04 - val_loss: 8.8045e-05\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.4244e-04 - val_loss: 8.9942e-05\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.4067e-04 - val_loss: 8.8076e-05\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.3988e-04 - val_loss: 9.0519e-05\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.3864e-04 - val_loss: 8.8036e-05\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.3751e-04 - val_loss: 8.7219e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.3686e-04 - val_loss: 8.5782e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.3600e-04 - val_loss: 8.5136e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.3468e-04 - val_loss: 8.4628e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.3335e-04 - val_loss: 8.1679e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3167e-04 - val_loss: 8.1745e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.3088e-04 - val_loss: 8.1564e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2980e-04 - val_loss: 8.0315e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.2931e-04 - val_loss: 8.0274e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.2745e-04 - val_loss: 7.9990e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2696e-04 - val_loss: 8.0038e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.2639e-04 - val_loss: 8.1634e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.2493e-04 - val_loss: 8.0095e-05\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.2435e-04 - val_loss: 8.3302e-05\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.2317e-04 - val_loss: 8.4277e-05\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.2155e-04 - val_loss: 8.3458e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.2048e-04 - val_loss: 8.5244e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1917e-04 - val_loss: 8.8271e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.1793e-04 - val_loss: 8.7282e-05\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.1676e-04 - val_loss: 8.6978e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.1577e-04 - val_loss: 8.5327e-05\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 538us/step - loss: 1.1488e-04 - val_loss: 8.5092e-05\n",
      "Epoch 66/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.1373e-04 - val_loss: 8.4301e-05\n",
      "Epoch 67/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.1269e-04 - val_loss: 8.5583e-05\n",
      "Epoch 68/70\n",
      "39182/39182 [==============================] - 21s 536us/step - loss: 1.1144e-04 - val_loss: 8.0964e-05\n",
      "Epoch 69/70\n",
      "39182/39182 [==============================] - 21s 537us/step - loss: 1.1067e-04 - val_loss: 8.1829e-05\n",
      "Epoch 70/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.0988e-04 - val_loss: 8.0765e-05\n",
      "Validation RMSE:  0.008986946 \n",
      "\n",
      "Model took 1535.96 seconds to train\n",
      "Dataset Shape (50000, 1)\n",
      "Number of Windows are:\n",
      " 3\n",
      "Batch Size is: 68\n",
      "No Overlap\n",
      "Windows length are:\n",
      " 20 20 19 8\n",
      "1st Window Start: 11\n",
      "2nd Window Start: 510\n",
      "3rd Window Start: 1004\n",
      "4th Window Start: 1132\n",
      "Activation Function is: relu\n",
      "Optimizer is: RMSprop\n",
      "Three Lags are\n",
      " [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1]\n",
      "Lags length:\n",
      " 60\n",
      "Length Used:\n",
      " 20\n",
      "New Dataset shape after Lags: (48978, 61)\n",
      "Training and test data length 39182 9796\n",
      "trainX.shape[0]:  39182\n",
      "window_length:  20\n",
      "no_wins:  3\n",
      "trainX shape:\n",
      " (39182, 60)\n",
      "Train X shape after np.reshape (39182, 20, 3)\n",
      "Test X shape after np.reshape (9796, 20, 3)\n",
      "Train Y Shape (39182,)\n",
      "Test Y Shape (9796,)\n",
      "Train on 39182 samples, validate on 9796 samples\n",
      "Epoch 1/70\n",
      "39182/39182 [==============================] - 61s 2ms/step - loss: 0.0064 - val_loss: 0.0046\n",
      "Epoch 2/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 3/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 8.0887e-04 - val_loss: 0.0011\n",
      "Epoch 4/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 5.4826e-04 - val_loss: 4.9965e-04\n",
      "Epoch 5/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 4.6075e-04 - val_loss: 3.8577e-04\n",
      "Epoch 6/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 4.0866e-04 - val_loss: 3.5567e-04\n",
      "Epoch 7/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 3.6924e-04 - val_loss: 3.0361e-04\n",
      "Epoch 8/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 3.4797e-04 - val_loss: 2.6521e-04\n",
      "Epoch 9/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 3.2951e-04 - val_loss: 2.5484e-04\n",
      "Epoch 10/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 3.1899e-04 - val_loss: 2.3409e-04\n",
      "Epoch 11/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 3.0947e-04 - val_loss: 2.3159e-04\n",
      "Epoch 12/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 2.9941e-04 - val_loss: 2.2100e-04\n",
      "Epoch 13/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39182/39182 [==============================] - 21s 540us/step - loss: 2.9125e-04 - val_loss: 2.1978e-04\n",
      "Epoch 14/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.8006e-04 - val_loss: 1.9766e-04\n",
      "Epoch 15/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 2.7005e-04 - val_loss: 1.7661e-04\n",
      "Epoch 16/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 2.6341e-04 - val_loss: 1.7359e-04\n",
      "Epoch 17/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.5759e-04 - val_loss: 1.7877e-04\n",
      "Epoch 18/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 2.5194e-04 - val_loss: 1.9116e-04\n",
      "Epoch 19/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 2.4264e-04 - val_loss: 1.8380e-04\n",
      "Epoch 20/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.3151e-04 - val_loss: 1.6362e-04\n",
      "Epoch 21/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 2.2437e-04 - val_loss: 2.3698e-04\n",
      "Epoch 22/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 2.2371e-04 - val_loss: 2.0810e-04\n",
      "Epoch 23/70\n",
      "39182/39182 [==============================] - 21s 546us/step - loss: 2.1734e-04 - val_loss: 1.5050e-04\n",
      "Epoch 24/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 2.0802e-04 - val_loss: 1.4639e-04\n",
      "Epoch 25/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.9963e-04 - val_loss: 1.2975e-04\n",
      "Epoch 26/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.9275e-04 - val_loss: 1.0840e-04\n",
      "Epoch 27/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.8504e-04 - val_loss: 1.0921e-04\n",
      "Epoch 28/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.7793e-04 - val_loss: 1.0539e-04\n",
      "Epoch 29/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.7267e-04 - val_loss: 1.0569e-04\n",
      "Epoch 30/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.6890e-04 - val_loss: 1.0438e-04\n",
      "Epoch 31/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.6530e-04 - val_loss: 1.0266e-04\n",
      "Epoch 32/70\n",
      "39182/39182 [==============================] - 21s 544us/step - loss: 1.6365e-04 - val_loss: 1.1110e-04\n",
      "Epoch 33/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.6091e-04 - val_loss: 1.1606e-04\n",
      "Epoch 34/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.5747e-04 - val_loss: 1.0563e-04\n",
      "Epoch 35/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.5556e-04 - val_loss: 1.0358e-04\n",
      "Epoch 36/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.5443e-04 - val_loss: 1.0959e-04\n",
      "Epoch 37/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.5289e-04 - val_loss: 1.0669e-04\n",
      "Epoch 38/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.5008e-04 - val_loss: 9.7577e-05\n",
      "Epoch 39/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.4831e-04 - val_loss: 1.0786e-04\n",
      "Epoch 40/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.4779e-04 - val_loss: 1.0538e-04\n",
      "Epoch 41/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.4630e-04 - val_loss: 1.0325e-04\n",
      "Epoch 42/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.4499e-04 - val_loss: 1.0928e-04\n",
      "Epoch 43/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.4318e-04 - val_loss: 1.0319e-04\n",
      "Epoch 44/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.4211e-04 - val_loss: 9.9125e-05\n",
      "Epoch 45/70\n",
      "39182/39182 [==============================] - 21s 539us/step - loss: 1.4105e-04 - val_loss: 9.4546e-05\n",
      "Epoch 46/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3974e-04 - val_loss: 9.8485e-05\n",
      "Epoch 47/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.3852e-04 - val_loss: 9.8442e-05\n",
      "Epoch 48/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.3774e-04 - val_loss: 9.7905e-05\n",
      "Epoch 49/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3599e-04 - val_loss: 9.6532e-05\n",
      "Epoch 50/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3520e-04 - val_loss: 9.7263e-05\n",
      "Epoch 51/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3393e-04 - val_loss: 9.8460e-05\n",
      "Epoch 52/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.3318e-04 - val_loss: 9.6735e-05\n",
      "Epoch 53/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.3280e-04 - val_loss: 9.5125e-05\n",
      "Epoch 54/70\n",
      "39182/39182 [==============================] - 21s 540us/step - loss: 1.3130e-04 - val_loss: 9.5322e-05\n",
      "Epoch 55/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.3021e-04 - val_loss: 9.8416e-05\n",
      "Epoch 56/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.2882e-04 - val_loss: 1.0163e-04\n",
      "Epoch 57/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.2824e-04 - val_loss: 1.0208e-04\n",
      "Epoch 58/70\n",
      "39182/39182 [==============================] - 21s 543us/step - loss: 1.2727e-04 - val_loss: 1.0372e-04\n",
      "Epoch 59/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.2740e-04 - val_loss: 9.9879e-05\n",
      "Epoch 60/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.2673e-04 - val_loss: 9.5800e-05\n",
      "Epoch 61/70\n",
      "39182/39182 [==============================] - 21s 545us/step - loss: 1.2647e-04 - val_loss: 9.5556e-05\n",
      "Epoch 62/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.2589e-04 - val_loss: 1.0492e-04\n",
      "Epoch 63/70\n",
      "39182/39182 [==============================] - 21s 541us/step - loss: 1.2470e-04 - val_loss: 9.9526e-05\n",
      "Epoch 64/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.2495e-04 - val_loss: 1.0071e-04\n",
      "Epoch 65/70\n",
      "39182/39182 [==============================] - 21s 542us/step - loss: 1.2368e-04 - val_loss: 1.0025e-04\n",
      "Validation RMSE:  0.010012531 \n",
      "\n",
      "Model took 1437.28 seconds to train\n",
      "30 \t7     \t0.011001  \t0.00519492 \t0.00857037\t0.0264921 \n"
     ]
    }
   ],
   "source": [
    "hof = tools.HallOfFame(1)\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"std\", np.std)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "\n",
    "pop, log = algorithms.eaSimple(population, toolbox, cxpb=0.7, mutpb=0.1, ngen = num_generations,\n",
    "                                       stats=stats, halloffame=hof, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print top N solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "        0, 1, 0, 1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=tools.selBest(population,k = 1)\n",
    "a=np.asarray(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
    "        1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
    "        1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0,\n",
    "        0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[68:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generation</th>\n",
       "      <th>Average</th>\n",
       "      <th>Minimum</th>\n",
       "      <th>Maximum</th>\n",
       "      <th>N_Evaluations</th>\n",
       "      <th>Standard Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.657151</td>\n",
       "      <td>0.021108</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.287157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.610301</td>\n",
       "      <td>0.134017</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.290175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.399558</td>\n",
       "      <td>0.089192</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.327712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.179253</td>\n",
       "      <td>0.089192</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.207731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.088026</td>\n",
       "      <td>0.011894</td>\n",
       "      <td>0.113052</td>\n",
       "      <td>5</td>\n",
       "      <td>0.026897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.068270</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>0.128148</td>\n",
       "      <td>6</td>\n",
       "      <td>0.045749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.033974</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.117924</td>\n",
       "      <td>8</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.015175</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.039375</td>\n",
       "      <td>6</td>\n",
       "      <td>0.008119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.011713</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>8</td>\n",
       "      <td>0.001868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>9</td>\n",
       "      <td>0.004730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.010366</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>0.008748</td>\n",
       "      <td>0.010175</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.009215</td>\n",
       "      <td>0.008640</td>\n",
       "      <td>0.009846</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.010484</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.021126</td>\n",
       "      <td>8</td>\n",
       "      <td>0.003588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.009956</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.018553</td>\n",
       "      <td>4</td>\n",
       "      <td>0.002905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.009889</td>\n",
       "      <td>0.008841</td>\n",
       "      <td>0.012664</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.009710</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.008936</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>0.008476</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.009941</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.010935</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.023927</td>\n",
       "      <td>6</td>\n",
       "      <td>0.004435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.011770</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.009622</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.011395</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.010337</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.019640</td>\n",
       "      <td>8</td>\n",
       "      <td>0.003143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.010833</td>\n",
       "      <td>0.009034</td>\n",
       "      <td>0.021017</td>\n",
       "      <td>10</td>\n",
       "      <td>0.003445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>0.008626</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.088802</td>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.237067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>0.008860</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.009157</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.010095</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.010257</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>6</td>\n",
       "      <td>0.002491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.008570</td>\n",
       "      <td>0.026492</td>\n",
       "      <td>7</td>\n",
       "      <td>0.005195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Generation   Average   Minimum   Maximum  N_Evaluations  Standard Dev\n",
       "0            0  0.657151  0.021108  0.800000             10      0.287157\n",
       "1            1  0.610301  0.134017  0.800000              8      0.290175\n",
       "2            2  0.399558  0.089192  0.800000              4      0.327712\n",
       "3            3  0.179253  0.089192  0.800000              4      0.207731\n",
       "4            4  0.088026  0.011894  0.113052              5      0.026897\n",
       "5            5  0.068270  0.011694  0.128148              6      0.045749\n",
       "6            6  0.033974  0.011376  0.117924              8      0.041500\n",
       "7            7  0.015175  0.011376  0.039375              6      0.008119\n",
       "8            8  0.011713  0.008900  0.014074              8      0.001868\n",
       "9            9  0.013215  0.008900  0.024492              9      0.004730\n",
       "10          10  0.009333  0.008900  0.010366              6      0.000443\n",
       "11          11  0.009132  0.008748  0.010175              4      0.000407\n",
       "12          12  0.009215  0.008640  0.009846              8      0.000429\n",
       "13          13  0.010484  0.008682  0.021126              8      0.003588\n",
       "14          14  0.009956  0.008682  0.018553              4      0.002905\n",
       "15          15  0.009889  0.008841  0.012664             10      0.001051\n",
       "16          16  0.009046  0.008570  0.009710              9      0.000371\n",
       "17          17  0.008936  0.008570  0.009843              6      0.000416\n",
       "18          18  0.009057  0.008476  0.010113              6      0.000578\n",
       "19          19  0.009126  0.008570  0.009941              8      0.000419\n",
       "20          20  0.010935  0.008570  0.023927              6      0.004435\n",
       "21          21  0.009733  0.008570  0.011770              8      0.000925\n",
       "22          22  0.009622  0.008570  0.011395              8      0.000826\n",
       "23          23  0.010337  0.008570  0.019640              8      0.003143\n",
       "24          24  0.010833  0.009034  0.021017             10      0.003445\n",
       "25          25  0.009576  0.008626  0.010992              5      0.000752\n",
       "26          26  0.088802  0.008860  0.800000             10      0.237067\n",
       "27          27  0.009491  0.008860  0.011140              5      0.000665\n",
       "28          28  0.009157  0.008570  0.010095              6      0.000460\n",
       "29          29  0.010257  0.008570  0.015754              6      0.002491\n",
       "30          30  0.011001  0.008570  0.026492              7      0.005195"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save Results\n",
    "gen,avg,min1,max1,nevals, std = log.select('gen','avg','min','max','nevals','std')\n",
    "df=pd.DataFrame({'Generation':gen, 'Average':avg, 'Minimum':min1, 'Maximum':max,\n",
    "                 'N_Evaluations':nevals, 'Standard Dev':std})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('GA_Final_Res.xlsx')\n",
    "df.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('white')\n",
    "sns.set_context(\"paper\", font_scale=1.2)  \n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "#avg1,min1,max1=scaler.inverse_transform([avg1,min1,max1])\n",
    "\n",
    "plt.plot(gen1,avg1,marker='s',linewidth=1,markersize=6,label='Average')\n",
    "plt.plot(gen1,min1,marker='d',linewidth=1,markersize=6,label='Minimum')\n",
    "#plt.plot(gen1,max1,marker='*',linewidth=1,markersize=8,label='Maximum')\n",
    "#plt.tick_params(left=False, labelleft=True)\n",
    "#plt.box(False)\n",
    "plt.margins(0.2)\n",
    "\n",
    "\n",
    "plt.xlabel('Generation Number',labelpad=15)\n",
    "plt.ylabel('RMSE LSTM')\n",
    "#plt.xlim(-1, 26)\n",
    "plt.legend(loc='best');\n",
    "sns.despine(left=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fig1.png', bbox_inches='tight', dpi=400) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "a,b,c=log.select('min','max','std')\n",
    "\n",
    "#a,b,c=scaler.inverse_transform([a,b,c])\n",
    "\n",
    "df=pd.DataFrame({'Minimum':a,'Maximum':b, 'Std':c})\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.set_style('white')\n",
    "sns.set_context(\"paper\", font_scale=1.3)  \n",
    "sns.boxplot(data=df, width=0.6,color='#1f77b4')\n",
    "plt.xlabel('Statistics across Generations and Particles',labelpad=15)\n",
    "plt.ylabel('RMSE LSTM')\n",
    "sns.despine(left=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('Fig1.png', bbox_inches='tight', dpi=400) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e=log.select('gen','avg','min','max','std')\n",
    "\n",
    "#b,c,d,e=scaler.inverse_transform([b,c,d,e])\n",
    "df=pd.DataFrame({'Generation':a,'Average':b, 'Minimum':c,'Maximum':d,'Std':e})\n",
    "writer = pd.ExcelWriter('GA_Final__norm.xlsx')\n",
    "df.to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.479645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Consumption\n",
       "0     0.476205\n",
       "1     0.479645"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.copy(deep=True)\n",
    "df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lags are: (51,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n",
       "         12,   13,   14,   15,   16,   17,  509,  510,  511,  512,  513,\n",
       "        514,  515,  516,  517,  518,  519,  520,  521,  522,  523,  524,\n",
       "        525, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012,\n",
       "       1013, 1014, 1015, 1016, 1017, 1018, 1019])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lags LSTM\n",
    "lag1=np.arange(1,18,1)\n",
    "lag2=np.arange(509,526,1)\n",
    "lag3=np.arange(1003,1020,1)\n",
    "lag=np.concatenate([lag1,lag2,lag3])\n",
    "print('The number of lags are:',lag.shape)\n",
    "lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dataset shape after Lags: (156803, 52)\n"
     ]
    }
   ],
   "source": [
    "for col in df1.columns:\n",
    "        for idx, l in enumerate(lag):\n",
    "            df1.loc[:,col+\"_\"+str(idx)] = df1[col].shift(l)\n",
    "           \n",
    "df1.dropna(how='any',inplace=True)\n",
    "dataset = df1.values # Converted dataframe to numpy ndarray\n",
    "dataset = dataset.astype('float32')\n",
    "print('New Dataset shape after Lags:',dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test data length 125442 31361\n",
      "trainX.shape[0]:  125442\n",
      "trainX shape:\n",
      " (125442, 51)\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.80)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print('Training and test data length',len(train), len(test))\n",
    "    \n",
    "trainX=train[:,1:]\n",
    "trainY=train[:,0]\n",
    "testX=test[:,1:]\n",
    "testY=test[:,0]\n",
    "    \n",
    "print('trainX.shape[0]: ',trainX.shape[0])\n",
    "print('trainX shape:\\n',trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape after np.reshape (125442, 17, 3)\n",
      "Test X shape after np.reshape (31361, 17, 3)\n",
      "Train Y Shape (125442,)\n",
      "Test Y Shape (31361,)\n"
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "# reshape input to be [samples, time steps, features]\n",
    "#trainX = np.reshape(trainX, (trainX.shape[0],window_length,(no_wins)))\n",
    "#testX = np.reshape(testX, (testX.shape[0],window_length,(no_wins)))\n",
    "\n",
    "trainX = np.reshape(trainX, (trainX.shape[0],17,3))\n",
    "testX = np.reshape(testX, (testX.shape[0],17,3))\n",
    "\n",
    "\n",
    "print('Train X shape after np.reshape',trainX.shape)\n",
    "print('Test X shape after np.reshape',testX.shape)\n",
    "print('Train Y Shape',trainY.shape)\n",
    "print('Test Y Shape',testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential() # New Instance of Model Object\n",
    "model.add(LSTM(60, input_shape=(17,3)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(70, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mean_squared_error', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ali\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 125442 samples, validate on 31361 samples\n",
      "Epoch 1/120\n",
      "125442/125442 [==============================] - 31s 247us/step - loss: 0.0033 - val_loss: 3.0076e-04\n",
      "Epoch 2/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.9085e-04 - val_loss: 2.7266e-04\n",
      "Epoch 3/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.4971e-04 - val_loss: 2.7059e-04\n",
      "Epoch 4/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.2310e-04 - val_loss: 3.1914e-04\n",
      "Epoch 5/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.8972e-04 - val_loss: 2.3260e-04\n",
      "Epoch 6/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.8970e-04 - val_loss: 3.5295e-04\n",
      "Epoch 7/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.8629e-04 - val_loss: 2.5629e-04\n",
      "Epoch 8/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.5886e-04 - val_loss: 2.3119e-04\n",
      "Epoch 9/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.5061e-04 - val_loss: 2.4765e-04\n",
      "Epoch 10/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.3851e-04 - val_loss: 2.1526e-04\n",
      "Epoch 11/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.2221e-04 - val_loss: 1.7891e-04\n",
      "Epoch 12/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.0033e-04 - val_loss: 1.5674e-04\n",
      "Epoch 13/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 1.9671e-04 - val_loss: 1.6485e-04\n",
      "Epoch 14/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 1.7357e-04 - val_loss: 1.6283e-04\n",
      "Epoch 15/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 1.6200e-04 - val_loss: 1.4423e-04\n",
      "Epoch 16/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 1.5748e-04 - val_loss: 1.5789e-04\n",
      "Epoch 17/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 1.4411e-04 - val_loss: 1.2490e-04\n",
      "Epoch 18/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 1.3416e-04 - val_loss: 1.2268e-04\n",
      "Epoch 19/120\n",
      "125442/125442 [==============================] - 27s 218us/step - loss: 1.3285e-04 - val_loss: 1.1385e-04\n",
      "Epoch 20/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 1.2665e-04 - val_loss: 1.0008e-04\n",
      "Epoch 21/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 1.1319e-04 - val_loss: 1.0368e-04\n",
      "Epoch 22/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 1.0829e-04 - val_loss: 1.0047e-04\n",
      "Epoch 23/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 1.0365e-04 - val_loss: 1.0097e-04\n",
      "Epoch 24/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 9.4478e-05 - val_loss: 7.5449e-05\n",
      "Epoch 25/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 9.2299e-05 - val_loss: 7.6207e-05\n",
      "Epoch 26/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 8.7829e-05 - val_loss: 7.8571e-05\n",
      "Epoch 27/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 8.8911e-05 - val_loss: 1.0731e-04\n",
      "Epoch 28/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 8.6516e-05 - val_loss: 6.6654e-05\n",
      "Epoch 29/120\n",
      "125442/125442 [==============================] - 27s 218us/step - loss: 8.4619e-05 - val_loss: 6.3477e-05\n",
      "Epoch 30/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 8.0187e-05 - val_loss: 5.8471e-05\n",
      "Epoch 31/120\n",
      "125442/125442 [==============================] - 27s 218us/step - loss: 7.6877e-05 - val_loss: 7.0248e-05\n",
      "Epoch 32/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 7.8174e-05 - val_loss: 8.5110e-05\n",
      "Epoch 33/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 7.9578e-05 - val_loss: 6.6208e-05\n",
      "Epoch 34/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 7.5964e-05 - val_loss: 6.1986e-05\n",
      "Epoch 35/120\n",
      "125442/125442 [==============================] - 28s 219us/step - loss: 7.0519e-05 - val_loss: 5.5127e-05\n",
      "Epoch 36/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 7.0941e-05 - val_loss: 5.9736e-05\n",
      "Epoch 37/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 7.1089e-05 - val_loss: 6.9961e-05\n",
      "Epoch 38/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 6.7688e-05 - val_loss: 5.7451e-05\n",
      "Epoch 39/120\n",
      "125442/125442 [==============================] - 27s 218us/step - loss: 6.7286e-05 - val_loss: 8.9067e-05\n",
      "Epoch 40/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 6.9597e-05 - val_loss: 6.2815e-05\n",
      "Epoch 41/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 6.2842e-05 - val_loss: 5.3926e-05\n",
      "Epoch 42/120\n",
      "125442/125442 [==============================] - 27s 218us/step - loss: 6.6215e-05 - val_loss: 4.9379e-05\n",
      "Epoch 43/120\n",
      "125442/125442 [==============================] - 28s 219us/step - loss: 6.1224e-05 - val_loss: 5.1728e-05\n",
      "Epoch 44/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 6.1020e-05 - val_loss: 5.1204e-05\n",
      "Epoch 45/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 6.3227e-05 - val_loss: 5.3943e-05\n",
      "Epoch 46/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 6.0248e-05 - val_loss: 5.1374e-05\n",
      "Epoch 47/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 5.8201e-05 - val_loss: 4.6837e-05\n",
      "Epoch 48/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 5.8223e-05 - val_loss: 5.2241e-05\n",
      "Epoch 49/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 5.6539e-05 - val_loss: 5.2169e-05\n",
      "Epoch 50/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 5.7072e-05 - val_loss: 4.7554e-05\n",
      "Epoch 51/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 5.2860e-05 - val_loss: 4.6343e-05\n",
      "Epoch 52/120\n",
      "125442/125442 [==============================] - 28s 219us/step - loss: 5.3052e-05 - val_loss: 4.6167e-05\n",
      "Epoch 53/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 5.4847e-05 - val_loss: 5.0662e-05\n",
      "Epoch 54/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 5.0813e-05 - val_loss: 5.0049e-05\n",
      "Epoch 55/120\n",
      "125442/125442 [==============================] - 28s 219us/step - loss: 4.9873e-05 - val_loss: 4.0174e-05\n",
      "Epoch 56/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 4.8649e-05 - val_loss: 4.9941e-05\n",
      "Epoch 57/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 4.8936e-05 - val_loss: 3.9421e-05\n",
      "Epoch 58/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 4.9762e-05 - val_loss: 4.9270e-05\n",
      "Epoch 59/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 4.7746e-05 - val_loss: 3.5134e-05\n",
      "Epoch 60/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 4.6108e-05 - val_loss: 7.1026e-05\n",
      "Epoch 61/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 4.6410e-05 - val_loss: 3.4641e-05\n",
      "Epoch 62/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 4.4397e-05 - val_loss: 4.2986e-05\n",
      "Epoch 63/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 4.4265e-05 - val_loss: 5.1898e-05\n",
      "Epoch 64/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 4.3758e-05 - val_loss: 3.3441e-05\n",
      "Epoch 65/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 4.2754e-05 - val_loss: 3.7637e-05\n",
      "Epoch 66/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 4.1243e-05 - val_loss: 3.3541e-05\n",
      "Epoch 67/120\n",
      "125442/125442 [==============================] - 28s 219us/step - loss: 4.3736e-05 - val_loss: 3.7147e-05\n",
      "Epoch 68/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 4.0113e-05 - val_loss: 2.9382e-05\n",
      "Epoch 69/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 4.0735e-05 - val_loss: 3.4498e-05\n",
      "Epoch 70/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.9638e-05 - val_loss: 3.4987e-05\n",
      "Epoch 71/120\n",
      "125442/125442 [==============================] - 28s 219us/step - loss: 4.0182e-05 - val_loss: 3.6240e-05\n",
      "Epoch 72/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 3.9107e-05 - val_loss: 3.5057e-05\n",
      "Epoch 73/120\n",
      "125442/125442 [==============================] - 28s 219us/step - loss: 3.8772e-05 - val_loss: 3.1289e-05\n",
      "Epoch 74/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 3.9091e-05 - val_loss: 3.4895e-05\n",
      "Epoch 75/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 3.6018e-05 - val_loss: 8.3306e-05\n",
      "Epoch 76/120\n",
      "125442/125442 [==============================] - 27s 218us/step - loss: 3.5411e-05 - val_loss: 2.7119e-05\n",
      "Epoch 77/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.5213e-05 - val_loss: 3.2871e-05\n",
      "Epoch 78/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.5435e-05 - val_loss: 3.2457e-05\n",
      "Epoch 79/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 3.4347e-05 - val_loss: 3.4508e-05\n",
      "Epoch 80/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.5574e-05 - val_loss: 3.0933e-05\n",
      "Epoch 81/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.3049e-05 - val_loss: 3.3717e-05\n",
      "Epoch 82/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 3.3439e-05 - val_loss: 2.5498e-05\n",
      "Epoch 83/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 3.2947e-05 - val_loss: 3.3085e-05\n",
      "Epoch 84/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 3.3524e-05 - val_loss: 2.7433e-05\n",
      "Epoch 85/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.1973e-05 - val_loss: 2.9841e-05\n",
      "Epoch 86/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.3063e-05 - val_loss: 2.6033e-05\n",
      "Epoch 87/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 3.1249e-05 - val_loss: 2.7142e-05\n",
      "Epoch 88/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.2687e-05 - val_loss: 3.3046e-05\n",
      "Epoch 89/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 3.1358e-05 - val_loss: 2.9072e-05\n",
      "Epoch 90/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.0257e-05 - val_loss: 3.5212e-05\n",
      "Epoch 91/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 3.0838e-05 - val_loss: 2.4750e-05\n",
      "Epoch 92/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.9966e-05 - val_loss: 2.1475e-05\n",
      "Epoch 93/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 3.0841e-05 - val_loss: 3.1127e-05\n",
      "Epoch 94/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.8520e-05 - val_loss: 2.4149e-05\n",
      "Epoch 95/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.7632e-05 - val_loss: 4.0912e-05\n",
      "Epoch 96/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.7354e-05 - val_loss: 2.1567e-05\n",
      "Epoch 97/120\n",
      "125442/125442 [==============================] - 27s 218us/step - loss: 2.8907e-05 - val_loss: 2.6518e-05\n",
      "Epoch 98/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.8076e-05 - val_loss: 2.5636e-05\n",
      "Epoch 99/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.6388e-05 - val_loss: 2.4135e-05\n",
      "Epoch 100/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.6859e-05 - val_loss: 2.5894e-05\n",
      "Epoch 101/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.6603e-05 - val_loss: 2.6908e-05\n",
      "Epoch 102/120\n",
      "125442/125442 [==============================] - 28s 219us/step - loss: 2.6314e-05 - val_loss: 2.6759e-05\n",
      "Epoch 103/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 2.6020e-05 - val_loss: 2.0397e-05\n",
      "Epoch 104/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 2.6724e-05 - val_loss: 1.9954e-05\n",
      "Epoch 105/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.5057e-05 - val_loss: 2.1526e-05\n",
      "Epoch 106/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.5004e-05 - val_loss: 1.9718e-05\n",
      "Epoch 107/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.5540e-05 - val_loss: 2.1223e-05\n",
      "Epoch 108/120\n",
      "125442/125442 [==============================] - 28s 219us/step - loss: 2.5425e-05 - val_loss: 2.1887e-05\n",
      "Epoch 109/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.5038e-05 - val_loss: 3.1734e-05\n",
      "Epoch 110/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.4460e-05 - val_loss: 2.3424e-05\n",
      "Epoch 111/120\n",
      "125442/125442 [==============================] - 27s 219us/step - loss: 2.4524e-05 - val_loss: 1.9321e-05\n",
      "Epoch 112/120\n",
      "125442/125442 [==============================] - 27s 218us/step - loss: 2.4588e-05 - val_loss: 3.4785e-05\n",
      "Epoch 113/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.4105e-05 - val_loss: 2.8864e-05\n",
      "Epoch 114/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 2.4350e-05 - val_loss: 2.0856e-05\n",
      "Epoch 115/120\n",
      "125442/125442 [==============================] - 28s 222us/step - loss: 2.3824e-05 - val_loss: 1.9657e-05\n",
      "Epoch 116/120\n",
      "125442/125442 [==============================] - 28s 222us/step - loss: 2.2883e-05 - val_loss: 2.0265e-05\n",
      "Epoch 117/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.4257e-05 - val_loss: 1.7948e-05\n",
      "Epoch 118/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 2.4001e-05 - val_loss: 2.4066e-05\n",
      "Epoch 119/120\n",
      "125442/125442 [==============================] - 28s 220us/step - loss: 2.3621e-05 - val_loss: 3.0974e-05\n",
      "Epoch 120/120\n",
      "125442/125442 [==============================] - 28s 221us/step - loss: 2.2165e-05 - val_loss: 1.9697e-05\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 60)                15360     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               6100      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 70)                7070      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                3550      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 32,131\n",
      "Trainable params: 32,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "hist =model.fit(trainX, trainY, epochs=120, shuffle=True,batch_size=125, validation_data=(testX, testY), \n",
    "                callbacks=[EarlyStopping(monitor='val_loss', patience=20)], verbose=1)\n",
    "\n",
    "end = time.time()\n",
    "# Training Phase\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error Train: 242.16780533830598\n",
      "Train Score RMSE: 323.49 RMSE\n",
      "Mean Absolute Error Test: 240.9821425866052\n",
      "Mean Squared Error Test: 321.2727546409887\n"
     ]
    }
   ],
   "source": [
    "# calculate root mean squared error\n",
    "print('Mean Absolute Error Train:', mean_absolute_error(trainY[0], trainPredict[:,0]))\n",
    "\n",
    "testScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score RMSE: %.2f RMSE' % (testScore))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print('Mean Absolute Error Test:', mean_absolute_error(testY[0], testPredict[:,0]))\n",
    "print('Mean Squared Error Test:',np.sqrt(mean_squared_error(testY[0], testPredict[:,0])))\n",
    "#print('Mean Absolute Percentage Error:',MAPError(testY[0], testPredict[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 125442), (1, 31361))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.shape, testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6104836323543474"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(321.27/(np.mean(testY)))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
